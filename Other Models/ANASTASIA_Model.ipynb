{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2W5Mg3gbzf7k",
        "outputId": "8da36ac1-8f33-41a9-bf75-36908608ab31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "346FlnKfw2Ad"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "from ast import literal_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "iRBGAAx8ztnU",
        "outputId": "7866169e-2e08-4d7a-c37c-3b39b87354e9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a99692cf-39a6-4e18-a279-33752d879f3c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>UNINSTALL_SHORTCUT</th>\n",
              "      <th>MOUNT_UNMOUNT_FILESYSTEMS</th>\n",
              "      <th>DISABLE_KEYGUARD</th>\n",
              "      <th>VIBRATE</th>\n",
              "      <th>READ_SETTINGS</th>\n",
              "      <th>GET_TASKS</th>\n",
              "      <th>INSTALL_SHORTCUT</th>\n",
              "      <th>ACCESS_WIFI_STATE</th>\n",
              "      <th>...</th>\n",
              "      <th>WATCHDOX_API</th>\n",
              "      <th>CopyToClipboard</th>\n",
              "      <th>KP2aInternalFileBrowsing</th>\n",
              "      <th>BLACKLISTED_USB_DEVICE</th>\n",
              "      <th>READ_NOTE</th>\n",
              "      <th>RECEIVE_DATA</th>\n",
              "      <th>SEND_FEEDBACK</th>\n",
              "      <th>ACCESS_NETWORK</th>\n",
              "      <th>READ_ONLY</th>\n",
              "      <th>ACCESS_NETWORK_CHANGE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 627 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a99692cf-39a6-4e18-a279-33752d879f3c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a99692cf-39a6-4e18-a279-33752d879f3c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a99692cf-39a6-4e18-a279-33752d879f3c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  UNINSTALL_SHORTCUT  MOUNT_UNMOUNT_FILESYSTEMS  \\\n",
              "0           0             0                 1.0                        1.0   \n",
              "1           1             1                 1.0                        1.0   \n",
              "2           2             2                 1.0                        1.0   \n",
              "3           3             3                 1.0                        1.0   \n",
              "4           4             4                 1.0                        1.0   \n",
              "\n",
              "   DISABLE_KEYGUARD  VIBRATE  READ_SETTINGS  GET_TASKS  INSTALL_SHORTCUT  \\\n",
              "0               1.0      1.0            1.0        1.0               1.0   \n",
              "1               1.0      1.0            1.0        1.0               1.0   \n",
              "2               1.0      1.0            1.0        1.0               1.0   \n",
              "3               1.0      1.0            1.0        1.0               1.0   \n",
              "4               1.0      1.0            1.0        1.0               1.0   \n",
              "\n",
              "   ACCESS_WIFI_STATE  ...  WATCHDOX_API  CopyToClipboard  \\\n",
              "0                1.0  ...           0.0              0.0   \n",
              "1                1.0  ...           0.0              0.0   \n",
              "2                1.0  ...           0.0              0.0   \n",
              "3                1.0  ...           0.0              0.0   \n",
              "4                1.0  ...           0.0              0.0   \n",
              "\n",
              "   KP2aInternalFileBrowsing  BLACKLISTED_USB_DEVICE  READ_NOTE  RECEIVE_DATA  \\\n",
              "0                       0.0                     0.0        0.0           0.0   \n",
              "1                       0.0                     0.0        0.0           0.0   \n",
              "2                       0.0                     0.0        0.0           0.0   \n",
              "3                       0.0                     0.0        0.0           0.0   \n",
              "4                       0.0                     0.0        0.0           0.0   \n",
              "\n",
              "   SEND_FEEDBACK  ACCESS_NETWORK  READ_ONLY  ACCESS_NETWORK_CHANGE  \n",
              "0            0.0             0.0        0.0                    0.0  \n",
              "1            0.0             0.0        0.0                    0.0  \n",
              "2            0.0             0.0        0.0                    0.0  \n",
              "3            0.0             0.0        0.0                    0.0  \n",
              "4            0.0             0.0        0.0                    0.0  \n",
              "\n",
              "[5 rows x 627 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "permissions = pd.read_csv('/content/drive/MyDrive/apk-2020-zip/ultimate/Ultimate Permissions.csv')\n",
        "permissions.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "xswtGwiaURr_",
        "outputId": "442cf5a9-0438-405e-bae2-09d96fc953ae"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-adfcc2e3-5650-4e0b-bcbd-fe662f373b4b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UNINSTALL_SHORTCUT</th>\n",
              "      <th>MOUNT_UNMOUNT_FILESYSTEMS</th>\n",
              "      <th>DISABLE_KEYGUARD</th>\n",
              "      <th>VIBRATE</th>\n",
              "      <th>READ_SETTINGS</th>\n",
              "      <th>GET_TASKS</th>\n",
              "      <th>INSTALL_SHORTCUT</th>\n",
              "      <th>ACCESS_WIFI_STATE</th>\n",
              "      <th>SEND_SMS</th>\n",
              "      <th>RECORD_AUDIO</th>\n",
              "      <th>...</th>\n",
              "      <th>WATCHDOX_API</th>\n",
              "      <th>CopyToClipboard</th>\n",
              "      <th>KP2aInternalFileBrowsing</th>\n",
              "      <th>BLACKLISTED_USB_DEVICE</th>\n",
              "      <th>READ_NOTE</th>\n",
              "      <th>RECEIVE_DATA</th>\n",
              "      <th>SEND_FEEDBACK</th>\n",
              "      <th>ACCESS_NETWORK</th>\n",
              "      <th>READ_ONLY</th>\n",
              "      <th>ACCESS_NETWORK_CHANGE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14022</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14023</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14024</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14025</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14026</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14027 rows Ã— 625 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-adfcc2e3-5650-4e0b-bcbd-fe662f373b4b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-adfcc2e3-5650-4e0b-bcbd-fe662f373b4b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-adfcc2e3-5650-4e0b-bcbd-fe662f373b4b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       UNINSTALL_SHORTCUT  MOUNT_UNMOUNT_FILESYSTEMS  DISABLE_KEYGUARD  \\\n",
              "0                     1.0                        1.0               1.0   \n",
              "1                     1.0                        1.0               1.0   \n",
              "2                     1.0                        1.0               1.0   \n",
              "3                     1.0                        1.0               1.0   \n",
              "4                     1.0                        1.0               1.0   \n",
              "...                   ...                        ...               ...   \n",
              "14022                 1.0                        1.0               1.0   \n",
              "14023                 1.0                        1.0               1.0   \n",
              "14024                 1.0                        1.0               1.0   \n",
              "14025                 1.0                        1.0               1.0   \n",
              "14026                 1.0                        1.0               1.0   \n",
              "\n",
              "       VIBRATE  READ_SETTINGS  GET_TASKS  INSTALL_SHORTCUT  ACCESS_WIFI_STATE  \\\n",
              "0          1.0            1.0        1.0               1.0                1.0   \n",
              "1          1.0            1.0        1.0               1.0                1.0   \n",
              "2          1.0            1.0        1.0               1.0                1.0   \n",
              "3          1.0            1.0        1.0               1.0                1.0   \n",
              "4          1.0            1.0        1.0               1.0                1.0   \n",
              "...        ...            ...        ...               ...                ...   \n",
              "14022      1.0            1.0        1.0               1.0                1.0   \n",
              "14023      1.0            1.0        1.0               1.0                1.0   \n",
              "14024      1.0            1.0        1.0               1.0                1.0   \n",
              "14025      1.0            1.0        1.0               1.0                1.0   \n",
              "14026      1.0            1.0        1.0               1.0                1.0   \n",
              "\n",
              "       SEND_SMS  RECORD_AUDIO  ...  WATCHDOX_API  CopyToClipboard  \\\n",
              "0           1.0           1.0  ...           0.0              0.0   \n",
              "1           1.0           1.0  ...           0.0              0.0   \n",
              "2           1.0           1.0  ...           0.0              0.0   \n",
              "3           1.0           1.0  ...           0.0              0.0   \n",
              "4           1.0           1.0  ...           0.0              0.0   \n",
              "...         ...           ...  ...           ...              ...   \n",
              "14022       1.0           1.0  ...           1.0              1.0   \n",
              "14023       1.0           1.0  ...           1.0              1.0   \n",
              "14024       1.0           1.0  ...           1.0              1.0   \n",
              "14025       1.0           1.0  ...           1.0              1.0   \n",
              "14026       1.0           1.0  ...           1.0              1.0   \n",
              "\n",
              "       KP2aInternalFileBrowsing  BLACKLISTED_USB_DEVICE  READ_NOTE  \\\n",
              "0                           0.0                     0.0        0.0   \n",
              "1                           0.0                     0.0        0.0   \n",
              "2                           0.0                     0.0        0.0   \n",
              "3                           0.0                     0.0        0.0   \n",
              "4                           0.0                     0.0        0.0   \n",
              "...                         ...                     ...        ...   \n",
              "14022                       1.0                     1.0        1.0   \n",
              "14023                       1.0                     1.0        1.0   \n",
              "14024                       1.0                     1.0        1.0   \n",
              "14025                       1.0                     1.0        1.0   \n",
              "14026                       1.0                     1.0        1.0   \n",
              "\n",
              "       RECEIVE_DATA  SEND_FEEDBACK  ACCESS_NETWORK  READ_ONLY  \\\n",
              "0               0.0            0.0             0.0        0.0   \n",
              "1               0.0            0.0             0.0        0.0   \n",
              "2               0.0            0.0             0.0        0.0   \n",
              "3               0.0            0.0             0.0        0.0   \n",
              "4               0.0            0.0             0.0        0.0   \n",
              "...             ...            ...             ...        ...   \n",
              "14022           1.0            1.0             1.0        1.0   \n",
              "14023           1.0            1.0             1.0        1.0   \n",
              "14024           1.0            1.0             1.0        1.0   \n",
              "14025           1.0            1.0             1.0        1.0   \n",
              "14026           1.0            1.0             1.0        1.0   \n",
              "\n",
              "       ACCESS_NETWORK_CHANGE  \n",
              "0                        0.0  \n",
              "1                        0.0  \n",
              "2                        0.0  \n",
              "3                        0.0  \n",
              "4                        0.0  \n",
              "...                      ...  \n",
              "14022                    1.0  \n",
              "14023                    1.0  \n",
              "14024                    1.0  \n",
              "14025                    1.0  \n",
              "14026                    1.0  \n",
              "\n",
              "[14027 rows x 625 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "permissions = permissions.drop(\"Unnamed: 0\",axis=1)\n",
        "permissions = permissions.drop(\"Unnamed: 0.1\",axis=1)\n",
        "permissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "VgflyZaqUKDr",
        "outputId": "6742ea09-dfaa-4db3-ef43-1e6e57919e69"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-4281bc79-bad6-41cd-9b19-ca50a20a7114\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Unnamed: 0.1</th>\n",
              "      <th>(18, 'const/4')</th>\n",
              "      <th>(106, 'sput-boolean')</th>\n",
              "      <th>(103, 'sput')</th>\n",
              "      <th>(34, 'new-instance')</th>\n",
              "      <th>(112, 'invoke-direct')</th>\n",
              "      <th>(105, 'sput-object')</th>\n",
              "      <th>(26, 'const-string')</th>\n",
              "      <th>(22, 'const-wide/16')</th>\n",
              "      <th>...</th>\n",
              "      <th>(7423, 'sput-wide/jumbo')</th>\n",
              "      <th>(242, 'AG:invalid_instruction')</th>\n",
              "      <th>(233, 'AG:invalid_instruction')</th>\n",
              "      <th>(3839, 'iput-wide/jumbo')</th>\n",
              "      <th>(5375, 'sget/jumbo')</th>\n",
              "      <th>(7167, 'sput/jumbo')</th>\n",
              "      <th>(8703, 'sput-short/jumbo')</th>\n",
              "      <th>(9215, 'invoke-super/jumbo')</th>\n",
              "      <th>(9471, 'invoke-direct/jumbo')</th>\n",
              "      <th>(9983, 'invoke-interface/jumbo')</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5443.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>207.0</td>\n",
              "      <td>4145.0</td>\n",
              "      <td>5671.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>6923.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>9365.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>6090.0</td>\n",
              "      <td>8833.0</td>\n",
              "      <td>1358.0</td>\n",
              "      <td>11485.0</td>\n",
              "      <td>314.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>12041.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>5373.0</td>\n",
              "      <td>9984.0</td>\n",
              "      <td>725.0</td>\n",
              "      <td>8215.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 272 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4281bc79-bad6-41cd-9b19-ca50a20a7114')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4281bc79-bad6-41cd-9b19-ca50a20a7114 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4281bc79-bad6-41cd-9b19-ca50a20a7114');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   Unnamed: 0  Unnamed: 0.1  (18, 'const/4')  (106, 'sput-boolean')  \\\n",
              "0           0             0           5443.0                  152.0   \n",
              "1           1             1           9365.0                   55.0   \n",
              "2           2             2             29.0                    0.0   \n",
              "3           3             3          12041.0                   23.0   \n",
              "4           4             4             17.0                    0.0   \n",
              "\n",
              "   (103, 'sput')  (34, 'new-instance')  (112, 'invoke-direct')  \\\n",
              "0          207.0                4145.0                  5671.0   \n",
              "1           48.0                6090.0                  8833.0   \n",
              "2            0.0                  30.0                    37.0   \n",
              "3           24.0                5373.0                  9984.0   \n",
              "4            0.0                   9.0                    11.0   \n",
              "\n",
              "   (105, 'sput-object')  (26, 'const-string')  (22, 'const-wide/16')  ...  \\\n",
              "0                 565.0                6923.0                  241.0  ...   \n",
              "1                1358.0               11485.0                  314.0  ...   \n",
              "2                   4.0                  16.0                    1.0  ...   \n",
              "3                 725.0                8215.0                  161.0  ...   \n",
              "4                   0.0                  36.0                    0.0  ...   \n",
              "\n",
              "   (7423, 'sput-wide/jumbo')  (242, 'AG:invalid_instruction')  \\\n",
              "0                        0.0                              0.0   \n",
              "1                        0.0                              0.0   \n",
              "2                        0.0                              0.0   \n",
              "3                        0.0                              0.0   \n",
              "4                        0.0                              0.0   \n",
              "\n",
              "   (233, 'AG:invalid_instruction')  (3839, 'iput-wide/jumbo')  \\\n",
              "0                              0.0                        0.0   \n",
              "1                              0.0                        0.0   \n",
              "2                              0.0                        0.0   \n",
              "3                              0.0                        0.0   \n",
              "4                              0.0                        0.0   \n",
              "\n",
              "   (5375, 'sget/jumbo')  (7167, 'sput/jumbo')  (8703, 'sput-short/jumbo')  \\\n",
              "0                   0.0                   0.0                         0.0   \n",
              "1                   0.0                   0.0                         0.0   \n",
              "2                   0.0                   0.0                         0.0   \n",
              "3                   0.0                   0.0                         0.0   \n",
              "4                   0.0                   0.0                         0.0   \n",
              "\n",
              "   (9215, 'invoke-super/jumbo')  (9471, 'invoke-direct/jumbo')  \\\n",
              "0                           0.0                            0.0   \n",
              "1                           0.0                            0.0   \n",
              "2                           0.0                            0.0   \n",
              "3                           0.0                            0.0   \n",
              "4                           0.0                            0.0   \n",
              "\n",
              "   (9983, 'invoke-interface/jumbo')  \n",
              "0                               0.0  \n",
              "1                               0.0  \n",
              "2                               0.0  \n",
              "3                               0.0  \n",
              "4                               0.0  \n",
              "\n",
              "[5 rows x 272 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "opcodes = pd.read_csv('/content/drive/MyDrive/apk-2020-zip/ultimate/Ultimate opCodes.csv')\n",
        "opcodes.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "cjhpSNsdT-rZ",
        "outputId": "3763dce9-068d-4563-b3bc-d035dfd91324"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7749525c-059a-43f1-a4ab-72b14f8508f8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(18, 'const/4')</th>\n",
              "      <th>(106, 'sput-boolean')</th>\n",
              "      <th>(103, 'sput')</th>\n",
              "      <th>(34, 'new-instance')</th>\n",
              "      <th>(112, 'invoke-direct')</th>\n",
              "      <th>(105, 'sput-object')</th>\n",
              "      <th>(26, 'const-string')</th>\n",
              "      <th>(22, 'const-wide/16')</th>\n",
              "      <th>(104, 'sput-wide')</th>\n",
              "      <th>(14, 'return-void')</th>\n",
              "      <th>...</th>\n",
              "      <th>(7423, 'sput-wide/jumbo')</th>\n",
              "      <th>(242, 'AG:invalid_instruction')</th>\n",
              "      <th>(233, 'AG:invalid_instruction')</th>\n",
              "      <th>(3839, 'iput-wide/jumbo')</th>\n",
              "      <th>(5375, 'sget/jumbo')</th>\n",
              "      <th>(7167, 'sput/jumbo')</th>\n",
              "      <th>(8703, 'sput-short/jumbo')</th>\n",
              "      <th>(9215, 'invoke-super/jumbo')</th>\n",
              "      <th>(9471, 'invoke-direct/jumbo')</th>\n",
              "      <th>(9983, 'invoke-interface/jumbo')</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5443.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>207.0</td>\n",
              "      <td>4145.0</td>\n",
              "      <td>5671.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>6923.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>2393.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9365.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>6090.0</td>\n",
              "      <td>8833.0</td>\n",
              "      <td>1358.0</td>\n",
              "      <td>11485.0</td>\n",
              "      <td>314.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6125.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12041.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>5373.0</td>\n",
              "      <td>9984.0</td>\n",
              "      <td>725.0</td>\n",
              "      <td>8215.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>4842.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16188</th>\n",
              "      <td>18977.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>9564.0</td>\n",
              "      <td>17047.0</td>\n",
              "      <td>1706.0</td>\n",
              "      <td>12083.0</td>\n",
              "      <td>640.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>14023.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16189</th>\n",
              "      <td>16265.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>9179.0</td>\n",
              "      <td>15391.0</td>\n",
              "      <td>1826.0</td>\n",
              "      <td>12623.0</td>\n",
              "      <td>546.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>12631.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16190</th>\n",
              "      <td>9482.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>6201.0</td>\n",
              "      <td>10187.0</td>\n",
              "      <td>1400.0</td>\n",
              "      <td>9338.0</td>\n",
              "      <td>323.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7147.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16191</th>\n",
              "      <td>28676.0</td>\n",
              "      <td>190.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>19630.0</td>\n",
              "      <td>30661.0</td>\n",
              "      <td>4053.0</td>\n",
              "      <td>34995.0</td>\n",
              "      <td>1235.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>21598.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16192</th>\n",
              "      <td>19269.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>6617.0</td>\n",
              "      <td>9355.0</td>\n",
              "      <td>662.0</td>\n",
              "      <td>18948.0</td>\n",
              "      <td>187.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5875.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>16193 rows Ã— 269 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7749525c-059a-43f1-a4ab-72b14f8508f8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7749525c-059a-43f1-a4ab-72b14f8508f8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7749525c-059a-43f1-a4ab-72b14f8508f8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       (18, 'const/4')  (106, 'sput-boolean')  (103, 'sput')  \\\n",
              "0               5443.0                  152.0          207.0   \n",
              "1               9365.0                   55.0           48.0   \n",
              "2                 29.0                    0.0            0.0   \n",
              "3              12041.0                   23.0           24.0   \n",
              "4                 17.0                    0.0            0.0   \n",
              "...                ...                    ...            ...   \n",
              "16188          18977.0                   65.0           57.0   \n",
              "16189          16265.0                   71.0           42.0   \n",
              "16190           9482.0                   59.0           49.0   \n",
              "16191          28676.0                  190.0          157.0   \n",
              "16192          19269.0                   12.0           41.0   \n",
              "\n",
              "       (34, 'new-instance')  (112, 'invoke-direct')  (105, 'sput-object')  \\\n",
              "0                    4145.0                  5671.0                 565.0   \n",
              "1                    6090.0                  8833.0                1358.0   \n",
              "2                      30.0                    37.0                   4.0   \n",
              "3                    5373.0                  9984.0                 725.0   \n",
              "4                       9.0                    11.0                   0.0   \n",
              "...                     ...                     ...                   ...   \n",
              "16188                9564.0                 17047.0                1706.0   \n",
              "16189                9179.0                 15391.0                1826.0   \n",
              "16190                6201.0                 10187.0                1400.0   \n",
              "16191               19630.0                 30661.0                4053.0   \n",
              "16192                6617.0                  9355.0                 662.0   \n",
              "\n",
              "       (26, 'const-string')  (22, 'const-wide/16')  (104, 'sput-wide')  \\\n",
              "0                    6923.0                  241.0                31.0   \n",
              "1                   11485.0                  314.0                11.0   \n",
              "2                      16.0                    1.0                 4.0   \n",
              "3                    8215.0                  161.0                 2.0   \n",
              "4                      36.0                    0.0                 0.0   \n",
              "...                     ...                    ...                 ...   \n",
              "16188               12083.0                  640.0                11.0   \n",
              "16189               12623.0                  546.0                 7.0   \n",
              "16190                9338.0                  323.0                 2.0   \n",
              "16191               34995.0                 1235.0                42.0   \n",
              "16192               18948.0                  187.0                 0.0   \n",
              "\n",
              "       (14, 'return-void')  ...  (7423, 'sput-wide/jumbo')  \\\n",
              "0                   2393.0  ...                        0.0   \n",
              "1                   6125.0  ...                        0.0   \n",
              "2                     19.0  ...                        0.0   \n",
              "3                   4842.0  ...                        0.0   \n",
              "4                      8.0  ...                        0.0   \n",
              "...                    ...  ...                        ...   \n",
              "16188              14023.0  ...                        0.0   \n",
              "16189              12631.0  ...                        0.0   \n",
              "16190               7147.0  ...                        0.0   \n",
              "16191              21598.0  ...                        0.0   \n",
              "16192               5875.0  ...                        0.0   \n",
              "\n",
              "       (242, 'AG:invalid_instruction')  (233, 'AG:invalid_instruction')  \\\n",
              "0                                  0.0                              0.0   \n",
              "1                                  0.0                              0.0   \n",
              "2                                  0.0                              0.0   \n",
              "3                                  0.0                              0.0   \n",
              "4                                  0.0                              0.0   \n",
              "...                                ...                              ...   \n",
              "16188                              0.0                              0.0   \n",
              "16189                              0.0                              0.0   \n",
              "16190                              0.0                              0.0   \n",
              "16191                              0.0                              0.0   \n",
              "16192                              0.0                              0.0   \n",
              "\n",
              "       (3839, 'iput-wide/jumbo')  (5375, 'sget/jumbo')  (7167, 'sput/jumbo')  \\\n",
              "0                            0.0                   0.0                   0.0   \n",
              "1                            0.0                   0.0                   0.0   \n",
              "2                            0.0                   0.0                   0.0   \n",
              "3                            0.0                   0.0                   0.0   \n",
              "4                            0.0                   0.0                   0.0   \n",
              "...                          ...                   ...                   ...   \n",
              "16188                        0.0                   0.0                   0.0   \n",
              "16189                        0.0                   0.0                   0.0   \n",
              "16190                        0.0                   0.0                   0.0   \n",
              "16191                        0.0                   0.0                   0.0   \n",
              "16192                        0.0                   0.0                   0.0   \n",
              "\n",
              "       (8703, 'sput-short/jumbo')  (9215, 'invoke-super/jumbo')  \\\n",
              "0                             0.0                           0.0   \n",
              "1                             0.0                           0.0   \n",
              "2                             0.0                           0.0   \n",
              "3                             0.0                           0.0   \n",
              "4                             0.0                           0.0   \n",
              "...                           ...                           ...   \n",
              "16188                         0.0                           0.0   \n",
              "16189                         0.0                           0.0   \n",
              "16190                         0.0                           0.0   \n",
              "16191                         0.0                           0.0   \n",
              "16192                         0.0                           0.0   \n",
              "\n",
              "       (9471, 'invoke-direct/jumbo')  (9983, 'invoke-interface/jumbo')  \n",
              "0                                0.0                               0.0  \n",
              "1                                0.0                               0.0  \n",
              "2                                0.0                               0.0  \n",
              "3                                0.0                               0.0  \n",
              "4                                0.0                               0.0  \n",
              "...                              ...                               ...  \n",
              "16188                            0.0                               0.0  \n",
              "16189                            0.0                               0.0  \n",
              "16190                            0.0                               0.0  \n",
              "16191                            0.0                               0.0  \n",
              "16192                            0.0                               0.0  \n",
              "\n",
              "[16193 rows x 269 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "opcodes = opcodes.drop(\"Unnamed: 0\",axis=1)\n",
        "opcodes = opcodes.drop(\"Unnamed: 0.1\",axis=1)\n",
        "opcodes = opcodes.drop(\"TYPE\",axis=1)\n",
        "\n",
        "opcodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "gEzt6f31UanM"
      },
      "outputs": [],
      "source": [
        "dfpr = pd.merge(opcodes,permissions,on=(\"APP\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "id": "j-oKB7C3nEVA",
        "outputId": "0824016c-8148-456c-8375-aa6df89a3f75"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-75a50197-c5f4-42f5-ab08-36d19f743491\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(18, 'const/4')</th>\n",
              "      <th>(106, 'sput-boolean')</th>\n",
              "      <th>(103, 'sput')</th>\n",
              "      <th>(34, 'new-instance')</th>\n",
              "      <th>(112, 'invoke-direct')</th>\n",
              "      <th>(105, 'sput-object')</th>\n",
              "      <th>(26, 'const-string')</th>\n",
              "      <th>(22, 'const-wide/16')</th>\n",
              "      <th>(104, 'sput-wide')</th>\n",
              "      <th>(14, 'return-void')</th>\n",
              "      <th>...</th>\n",
              "      <th>WATCHDOX_API</th>\n",
              "      <th>CopyToClipboard</th>\n",
              "      <th>KP2aInternalFileBrowsing</th>\n",
              "      <th>BLACKLISTED_USB_DEVICE</th>\n",
              "      <th>READ_NOTE</th>\n",
              "      <th>RECEIVE_DATA</th>\n",
              "      <th>SEND_FEEDBACK</th>\n",
              "      <th>ACCESS_NETWORK</th>\n",
              "      <th>READ_ONLY</th>\n",
              "      <th>ACCESS_NETWORK_CHANGE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5443.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>207.0</td>\n",
              "      <td>4145.0</td>\n",
              "      <td>5671.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>6923.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>2393.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9365.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>6090.0</td>\n",
              "      <td>8833.0</td>\n",
              "      <td>1358.0</td>\n",
              "      <td>11485.0</td>\n",
              "      <td>314.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6125.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 893 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-75a50197-c5f4-42f5-ab08-36d19f743491')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-75a50197-c5f4-42f5-ab08-36d19f743491 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-75a50197-c5f4-42f5-ab08-36d19f743491');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   (18, 'const/4')  (106, 'sput-boolean')  (103, 'sput')  \\\n",
              "0           5443.0                  152.0          207.0   \n",
              "1           9365.0                   55.0           48.0   \n",
              "2             29.0                    0.0            0.0   \n",
              "3             17.0                    0.0            0.0   \n",
              "4              5.0                    0.0            0.0   \n",
              "\n",
              "   (34, 'new-instance')  (112, 'invoke-direct')  (105, 'sput-object')  \\\n",
              "0                4145.0                  5671.0                 565.0   \n",
              "1                6090.0                  8833.0                1358.0   \n",
              "2                  30.0                    37.0                   4.0   \n",
              "3                   9.0                    11.0                   0.0   \n",
              "4                   3.0                    15.0                   0.0   \n",
              "\n",
              "   (26, 'const-string')  (22, 'const-wide/16')  (104, 'sput-wide')  \\\n",
              "0                6923.0                  241.0                31.0   \n",
              "1               11485.0                  314.0                11.0   \n",
              "2                  16.0                    1.0                 4.0   \n",
              "3                  36.0                    0.0                 0.0   \n",
              "4                   3.0                    0.0                 0.0   \n",
              "\n",
              "   (14, 'return-void')  ...  WATCHDOX_API  CopyToClipboard  \\\n",
              "0               2393.0  ...           0.0              0.0   \n",
              "1               6125.0  ...           0.0              0.0   \n",
              "2                 19.0  ...           0.0              0.0   \n",
              "3                  8.0  ...           0.0              0.0   \n",
              "4                 14.0  ...           0.0              0.0   \n",
              "\n",
              "   KP2aInternalFileBrowsing  BLACKLISTED_USB_DEVICE  READ_NOTE  RECEIVE_DATA  \\\n",
              "0                       0.0                     0.0        0.0           0.0   \n",
              "1                       0.0                     0.0        0.0           0.0   \n",
              "2                       0.0                     0.0        0.0           0.0   \n",
              "3                       0.0                     0.0        0.0           0.0   \n",
              "4                       0.0                     0.0        0.0           0.0   \n",
              "\n",
              "   SEND_FEEDBACK  ACCESS_NETWORK  READ_ONLY  ACCESS_NETWORK_CHANGE  \n",
              "0            0.0             0.0        0.0                    0.0  \n",
              "1            0.0             0.0        0.0                    0.0  \n",
              "2            0.0             0.0        0.0                    0.0  \n",
              "3            0.0             0.0        0.0                    0.0  \n",
              "4            0.0             0.0        0.0                    0.0  \n",
              "\n",
              "[5 rows x 893 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfpr.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2F8HmhH1yB6",
        "outputId": "19720999-57d9-4c75-afab-4dfb0f72dfac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 14029 entries, 0 to 14028\n",
            "Columns: 893 entries, (18, 'const/4') to ACCESS_NETWORK_CHANGE\n",
            "dtypes: float64(891), object(2)\n",
            "memory usage: 95.7+ MB\n"
          ]
        }
      ],
      "source": [
        "dfpr.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiMQg4OY_7TB",
        "outputId": "818f9863-755a-4760-8d40-a86d1e0aae8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SMS         4758\n",
              "Riskware    3236\n",
              "Benign      2341\n",
              "Banking     2207\n",
              "Adware      1487\n",
              "Name: TYPE, dtype: int64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfpr['TYPE'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DvfIqHgTaLyD"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ar7MAqszhksM",
        "outputId": "f28db67b-80de-41e8-9bc5-ade505ebc9d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([3., 1., 0., 4., 2.])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ord_enc = OrdinalEncoder()\n",
        "dfpr['TYPE'] = ord_enc.fit_transform(dfpr[['TYPE']])\n",
        "dfpr['TYPE'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "4J1VVfDKLWTr"
      },
      "outputs": [],
      "source": [
        "dfpr['TYPE'] = dfpr['TYPE'].astype('int')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClhticPojw8t",
        "outputId": "3c679354-ec12-4696-972d-52afe2a9c129"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4    4758\n",
              "3    3236\n",
              "2    2341\n",
              "1    2207\n",
              "0    1487\n",
              "Name: TYPE, dtype: int64"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfpr['TYPE'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "U27xzherpH1q",
        "outputId": "e64ca9b2-f9b4-4375-a85a-48d8374c530b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b82855e2-9a63-4c9f-84c5-d0527f297de5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(18, 'const/4')</th>\n",
              "      <th>(106, 'sput-boolean')</th>\n",
              "      <th>(103, 'sput')</th>\n",
              "      <th>(34, 'new-instance')</th>\n",
              "      <th>(112, 'invoke-direct')</th>\n",
              "      <th>(105, 'sput-object')</th>\n",
              "      <th>(26, 'const-string')</th>\n",
              "      <th>(22, 'const-wide/16')</th>\n",
              "      <th>(104, 'sput-wide')</th>\n",
              "      <th>(14, 'return-void')</th>\n",
              "      <th>...</th>\n",
              "      <th>RECEIVE_DATA</th>\n",
              "      <th>SEND_FEEDBACK</th>\n",
              "      <th>ACCESS_NETWORK</th>\n",
              "      <th>READ_ONLY</th>\n",
              "      <th>ACCESS_NETWORK_CHANGE</th>\n",
              "      <th>TYPE_0</th>\n",
              "      <th>TYPE_1</th>\n",
              "      <th>TYPE_2</th>\n",
              "      <th>TYPE_3</th>\n",
              "      <th>TYPE_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5443.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>207.0</td>\n",
              "      <td>4145.0</td>\n",
              "      <td>5671.0</td>\n",
              "      <td>565.0</td>\n",
              "      <td>6923.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>2393.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>9365.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>48.0</td>\n",
              "      <td>6090.0</td>\n",
              "      <td>8833.0</td>\n",
              "      <td>1358.0</td>\n",
              "      <td>11485.0</td>\n",
              "      <td>314.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>6125.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>29.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14024</th>\n",
              "      <td>47953.0</td>\n",
              "      <td>189.0</td>\n",
              "      <td>133.0</td>\n",
              "      <td>31268.0</td>\n",
              "      <td>53597.0</td>\n",
              "      <td>6764.0</td>\n",
              "      <td>34554.0</td>\n",
              "      <td>2037.0</td>\n",
              "      <td>37.0</td>\n",
              "      <td>41025.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14025</th>\n",
              "      <td>35479.0</td>\n",
              "      <td>303.0</td>\n",
              "      <td>182.0</td>\n",
              "      <td>22388.0</td>\n",
              "      <td>38585.0</td>\n",
              "      <td>4133.0</td>\n",
              "      <td>29818.0</td>\n",
              "      <td>1466.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>28800.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14026</th>\n",
              "      <td>18977.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>9564.0</td>\n",
              "      <td>17047.0</td>\n",
              "      <td>1706.0</td>\n",
              "      <td>12083.0</td>\n",
              "      <td>640.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>14023.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14027</th>\n",
              "      <td>16265.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>42.0</td>\n",
              "      <td>9179.0</td>\n",
              "      <td>15391.0</td>\n",
              "      <td>1826.0</td>\n",
              "      <td>12623.0</td>\n",
              "      <td>546.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>12631.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14028</th>\n",
              "      <td>9482.0</td>\n",
              "      <td>59.0</td>\n",
              "      <td>49.0</td>\n",
              "      <td>6201.0</td>\n",
              "      <td>10187.0</td>\n",
              "      <td>1400.0</td>\n",
              "      <td>9338.0</td>\n",
              "      <td>323.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7147.0</td>\n",
              "      <td>...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14029 rows Ã— 897 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b82855e2-9a63-4c9f-84c5-d0527f297de5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b82855e2-9a63-4c9f-84c5-d0527f297de5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b82855e2-9a63-4c9f-84c5-d0527f297de5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       (18, 'const/4')  (106, 'sput-boolean')  (103, 'sput')  \\\n",
              "0               5443.0                  152.0          207.0   \n",
              "1               9365.0                   55.0           48.0   \n",
              "2                 29.0                    0.0            0.0   \n",
              "3                 17.0                    0.0            0.0   \n",
              "4                  5.0                    0.0            0.0   \n",
              "...                ...                    ...            ...   \n",
              "14024          47953.0                  189.0          133.0   \n",
              "14025          35479.0                  303.0          182.0   \n",
              "14026          18977.0                   65.0           57.0   \n",
              "14027          16265.0                   71.0           42.0   \n",
              "14028           9482.0                   59.0           49.0   \n",
              "\n",
              "       (34, 'new-instance')  (112, 'invoke-direct')  (105, 'sput-object')  \\\n",
              "0                    4145.0                  5671.0                 565.0   \n",
              "1                    6090.0                  8833.0                1358.0   \n",
              "2                      30.0                    37.0                   4.0   \n",
              "3                       9.0                    11.0                   0.0   \n",
              "4                       3.0                    15.0                   0.0   \n",
              "...                     ...                     ...                   ...   \n",
              "14024               31268.0                 53597.0                6764.0   \n",
              "14025               22388.0                 38585.0                4133.0   \n",
              "14026                9564.0                 17047.0                1706.0   \n",
              "14027                9179.0                 15391.0                1826.0   \n",
              "14028                6201.0                 10187.0                1400.0   \n",
              "\n",
              "       (26, 'const-string')  (22, 'const-wide/16')  (104, 'sput-wide')  \\\n",
              "0                    6923.0                  241.0                31.0   \n",
              "1                   11485.0                  314.0                11.0   \n",
              "2                      16.0                    1.0                 4.0   \n",
              "3                      36.0                    0.0                 0.0   \n",
              "4                       3.0                    0.0                 0.0   \n",
              "...                     ...                    ...                 ...   \n",
              "14024               34554.0                 2037.0                37.0   \n",
              "14025               29818.0                 1466.0                35.0   \n",
              "14026               12083.0                  640.0                11.0   \n",
              "14027               12623.0                  546.0                 7.0   \n",
              "14028                9338.0                  323.0                 2.0   \n",
              "\n",
              "       (14, 'return-void')  ...  RECEIVE_DATA  SEND_FEEDBACK  ACCESS_NETWORK  \\\n",
              "0                   2393.0  ...           0.0            0.0             0.0   \n",
              "1                   6125.0  ...           0.0            0.0             0.0   \n",
              "2                     19.0  ...           0.0            0.0             0.0   \n",
              "3                      8.0  ...           0.0            0.0             0.0   \n",
              "4                     14.0  ...           0.0            0.0             0.0   \n",
              "...                    ...  ...           ...            ...             ...   \n",
              "14024              41025.0  ...           1.0            1.0             1.0   \n",
              "14025              28800.0  ...           1.0            1.0             1.0   \n",
              "14026              14023.0  ...           1.0            1.0             1.0   \n",
              "14027              12631.0  ...           1.0            1.0             1.0   \n",
              "14028               7147.0  ...           1.0            1.0             1.0   \n",
              "\n",
              "       READ_ONLY  ACCESS_NETWORK_CHANGE  TYPE_0  TYPE_1  TYPE_2  TYPE_3  \\\n",
              "0            0.0                    0.0       0       0       0       1   \n",
              "1            0.0                    0.0       0       0       0       1   \n",
              "2            0.0                    0.0       0       0       0       1   \n",
              "3            0.0                    0.0       0       0       0       1   \n",
              "4            0.0                    0.0       0       0       0       1   \n",
              "...          ...                    ...     ...     ...     ...     ...   \n",
              "14024        1.0                    1.0       0       0       1       0   \n",
              "14025        1.0                    1.0       0       0       1       0   \n",
              "14026        1.0                    1.0       0       0       1       0   \n",
              "14027        1.0                    1.0       0       0       1       0   \n",
              "14028        1.0                    1.0       0       0       1       0   \n",
              "\n",
              "       TYPE_4  \n",
              "0           0  \n",
              "1           0  \n",
              "2           0  \n",
              "3           0  \n",
              "4           0  \n",
              "...       ...  \n",
              "14024       0  \n",
              "14025       0  \n",
              "14026       0  \n",
              "14027       0  \n",
              "14028       0  \n",
              "\n",
              "[14029 rows x 897 columns]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfpr = pd.get_dummies(dfpr, columns = ['TYPE'])\n",
        "dfpr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAksIVu3NTv6",
        "outputId": "cc125509-3e7a-49d4-c6df-9078118dc31f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(18, 'const/4')           float64\n",
              "(106, 'sput-boolean')     float64\n",
              "(103, 'sput')             float64\n",
              "(34, 'new-instance')      float64\n",
              "(112, 'invoke-direct')    float64\n",
              "                           ...   \n",
              "TYPE_0                      uint8\n",
              "TYPE_1                      uint8\n",
              "TYPE_2                      uint8\n",
              "TYPE_3                      uint8\n",
              "TYPE_4                      uint8\n",
              "Length: 897, dtype: object"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dfpr.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "690Ketv5H_Rw"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import shuffle\n",
        "dfpr = shuffle(dfpr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "xTP_DSl9rY_R",
        "outputId": "f2bbd005-ae90-401d-b94e-5ab1fd3a05b3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-87c980e7-b23f-47a9-8072-38c7921d0f01\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TYPE_0</th>\n",
              "      <th>TYPE_1</th>\n",
              "      <th>TYPE_2</th>\n",
              "      <th>TYPE_3</th>\n",
              "      <th>TYPE_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10922</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10848</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8414</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10030</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5751</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8198</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3504</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4556</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8908</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14029 rows Ã— 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87c980e7-b23f-47a9-8072-38c7921d0f01')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-87c980e7-b23f-47a9-8072-38c7921d0f01 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-87c980e7-b23f-47a9-8072-38c7921d0f01');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       TYPE_0  TYPE_1  TYPE_2  TYPE_3  TYPE_4\n",
              "168         0       0       0       1       0\n",
              "10922       0       0       0       0       1\n",
              "10848       0       0       0       0       1\n",
              "8414        0       0       0       0       1\n",
              "10030       0       0       0       0       1\n",
              "...       ...     ...     ...     ...     ...\n",
              "5751        1       0       0       0       0\n",
              "8198        0       0       0       0       1\n",
              "3504        0       1       0       0       0\n",
              "4556        0       1       0       0       0\n",
              "8908        0       0       0       0       1\n",
              "\n",
              "[14029 rows x 5 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "target = ['TYPE_0','TYPE_1','TYPE_2','TYPE_3','TYPE_4']\n",
        "y = dfpr[target]\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "OgWIoz21hgb6",
        "outputId": "85fafc40-f9d0-4eab-bf4f-c77911c79f80"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-06a65e42-c7c7-4559-ab60-23bf2ae6509a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(18, 'const/4')</th>\n",
              "      <th>(106, 'sput-boolean')</th>\n",
              "      <th>(103, 'sput')</th>\n",
              "      <th>(34, 'new-instance')</th>\n",
              "      <th>(112, 'invoke-direct')</th>\n",
              "      <th>(105, 'sput-object')</th>\n",
              "      <th>(26, 'const-string')</th>\n",
              "      <th>(22, 'const-wide/16')</th>\n",
              "      <th>(104, 'sput-wide')</th>\n",
              "      <th>(14, 'return-void')</th>\n",
              "      <th>...</th>\n",
              "      <th>WATCHDOX_API</th>\n",
              "      <th>CopyToClipboard</th>\n",
              "      <th>KP2aInternalFileBrowsing</th>\n",
              "      <th>BLACKLISTED_USB_DEVICE</th>\n",
              "      <th>READ_NOTE</th>\n",
              "      <th>RECEIVE_DATA</th>\n",
              "      <th>SEND_FEEDBACK</th>\n",
              "      <th>ACCESS_NETWORK</th>\n",
              "      <th>READ_ONLY</th>\n",
              "      <th>ACCESS_NETWORK_CHANGE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10922</th>\n",
              "      <td>152.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10848</th>\n",
              "      <td>102.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>292.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>264.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8414</th>\n",
              "      <td>59.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>201.0</td>\n",
              "      <td>239.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10030</th>\n",
              "      <td>745.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>373.0</td>\n",
              "      <td>757.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5751</th>\n",
              "      <td>1064.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>974.0</td>\n",
              "      <td>1258.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>2067.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>522.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8198</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3504</th>\n",
              "      <td>5928.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2549.0</td>\n",
              "      <td>5199.0</td>\n",
              "      <td>379.0</td>\n",
              "      <td>2682.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5336.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4556</th>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8908</th>\n",
              "      <td>661.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>2119.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14029 rows Ã— 892 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06a65e42-c7c7-4559-ab60-23bf2ae6509a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-06a65e42-c7c7-4559-ab60-23bf2ae6509a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-06a65e42-c7c7-4559-ab60-23bf2ae6509a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       (18, 'const/4')  (106, 'sput-boolean')  (103, 'sput')  \\\n",
              "168               17.0                    0.0            0.0   \n",
              "10922            152.0                    0.0            0.0   \n",
              "10848            102.0                    1.0            1.0   \n",
              "8414              59.0                    0.0            1.0   \n",
              "10030            745.0                   20.0           41.0   \n",
              "...                ...                    ...            ...   \n",
              "5751            1064.0                   57.0           16.0   \n",
              "8198               0.0                    0.0            0.0   \n",
              "3504            5928.0                   30.0           14.0   \n",
              "4556              94.0                    0.0            0.0   \n",
              "8908             661.0                   18.0           47.0   \n",
              "\n",
              "       (34, 'new-instance')  (112, 'invoke-direct')  (105, 'sput-object')  \\\n",
              "168                     9.0                    11.0                   0.0   \n",
              "10922                  82.0                   122.0                   0.0   \n",
              "10848                 262.0                   292.0                   2.0   \n",
              "8414                  201.0                   239.0                   1.0   \n",
              "10030                 111.0                   131.0                 373.0   \n",
              "...                     ...                     ...                   ...   \n",
              "5751                  974.0                  1258.0                 124.0   \n",
              "8198                    0.0                     0.0                   0.0   \n",
              "3504                 2549.0                  5199.0                 379.0   \n",
              "4556                   39.0                    51.0                   2.0   \n",
              "8908                  139.0                   157.0                  58.0   \n",
              "\n",
              "       (26, 'const-string')  (22, 'const-wide/16')  (104, 'sput-wide')  \\\n",
              "168                    36.0                    0.0                 0.0   \n",
              "10922                 171.0                    2.0                 0.0   \n",
              "10848                 264.0                    6.0                 0.0   \n",
              "8414                  137.0                    2.0                 0.0   \n",
              "10030                 757.0                    0.0                 0.0   \n",
              "...                     ...                    ...                 ...   \n",
              "5751                 2067.0                   23.0                 2.0   \n",
              "8198                    0.0                    0.0                 0.0   \n",
              "3504                 2682.0                  125.0                 0.0   \n",
              "4556                  161.0                    1.0                 0.0   \n",
              "8908                 2119.0                    4.0                 0.0   \n",
              "\n",
              "       (14, 'return-void')  ...  WATCHDOX_API  CopyToClipboard  \\\n",
              "168                    8.0  ...           0.0              0.0   \n",
              "10922                 74.0  ...           0.0              0.0   \n",
              "10848                121.0  ...           0.0              0.0   \n",
              "8414                  76.0  ...           0.0              0.0   \n",
              "10030                 77.0  ...           0.0              0.0   \n",
              "...                    ...  ...           ...              ...   \n",
              "5751                 522.0  ...           0.0              0.0   \n",
              "8198                   0.0  ...           0.0              0.0   \n",
              "3504                5336.0  ...           0.0              0.0   \n",
              "4556                  35.0  ...           0.0              0.0   \n",
              "8908                  53.0  ...           0.0              0.0   \n",
              "\n",
              "       KP2aInternalFileBrowsing  BLACKLISTED_USB_DEVICE  READ_NOTE  \\\n",
              "168                         0.0                     0.0        0.0   \n",
              "10922                       0.0                     0.0        0.0   \n",
              "10848                       0.0                     0.0        0.0   \n",
              "8414                        0.0                     0.0        0.0   \n",
              "10030                       0.0                     0.0        0.0   \n",
              "...                         ...                     ...        ...   \n",
              "5751                        0.0                     0.0        0.0   \n",
              "8198                        0.0                     0.0        0.0   \n",
              "3504                        0.0                     0.0        0.0   \n",
              "4556                        0.0                     0.0        0.0   \n",
              "8908                        0.0                     0.0        0.0   \n",
              "\n",
              "       RECEIVE_DATA  SEND_FEEDBACK  ACCESS_NETWORK  READ_ONLY  \\\n",
              "168             0.0            0.0             0.0        0.0   \n",
              "10922           0.0            0.0             0.0        0.0   \n",
              "10848           0.0            0.0             0.0        0.0   \n",
              "8414            0.0            0.0             0.0        0.0   \n",
              "10030           0.0            0.0             0.0        0.0   \n",
              "...             ...            ...             ...        ...   \n",
              "5751            0.0            0.0             0.0        0.0   \n",
              "8198            0.0            0.0             0.0        0.0   \n",
              "3504            0.0            0.0             0.0        0.0   \n",
              "4556            0.0            0.0             0.0        0.0   \n",
              "8908            0.0            0.0             0.0        0.0   \n",
              "\n",
              "       ACCESS_NETWORK_CHANGE  \n",
              "168                      0.0  \n",
              "10922                    0.0  \n",
              "10848                    0.0  \n",
              "8414                     0.0  \n",
              "10030                    0.0  \n",
              "...                      ...  \n",
              "5751                     0.0  \n",
              "8198                     0.0  \n",
              "3504                     0.0  \n",
              "4556                     0.0  \n",
              "8908                     0.0  \n",
              "\n",
              "[14029 rows x 892 columns]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = dfpr.loc[:, dfpr.columns != 'TYPE_0']\n",
        "x = x.loc[:, x.columns != 'TYPE_1']\n",
        "x = x.loc[:, x.columns != 'TYPE_2']\n",
        "x = x.loc[:, x.columns != 'TYPE_3']\n",
        "x = x.loc[:, x.columns != 'TYPE_4']\n",
        "\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "LRMLuy9AyGCy",
        "outputId": "c99a27db-c864-4f64-b656-d79ec9896ebd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2abfe6a7-a6c3-4af3-bd4b-5789c328d5e9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(18, 'const/4')</th>\n",
              "      <th>(106, 'sput-boolean')</th>\n",
              "      <th>(103, 'sput')</th>\n",
              "      <th>(34, 'new-instance')</th>\n",
              "      <th>(112, 'invoke-direct')</th>\n",
              "      <th>(105, 'sput-object')</th>\n",
              "      <th>(26, 'const-string')</th>\n",
              "      <th>(22, 'const-wide/16')</th>\n",
              "      <th>(104, 'sput-wide')</th>\n",
              "      <th>(14, 'return-void')</th>\n",
              "      <th>...</th>\n",
              "      <th>WATCHDOX_API</th>\n",
              "      <th>CopyToClipboard</th>\n",
              "      <th>KP2aInternalFileBrowsing</th>\n",
              "      <th>BLACKLISTED_USB_DEVICE</th>\n",
              "      <th>READ_NOTE</th>\n",
              "      <th>RECEIVE_DATA</th>\n",
              "      <th>SEND_FEEDBACK</th>\n",
              "      <th>ACCESS_NETWORK</th>\n",
              "      <th>READ_ONLY</th>\n",
              "      <th>ACCESS_NETWORK_CHANGE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10922</th>\n",
              "      <td>152.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10848</th>\n",
              "      <td>102.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>292.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>264.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8414</th>\n",
              "      <td>59.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>201.0</td>\n",
              "      <td>239.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10030</th>\n",
              "      <td>745.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>373.0</td>\n",
              "      <td>757.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5751</th>\n",
              "      <td>1064.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>974.0</td>\n",
              "      <td>1258.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>2067.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>522.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8198</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3504</th>\n",
              "      <td>5928.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2549.0</td>\n",
              "      <td>5199.0</td>\n",
              "      <td>379.0</td>\n",
              "      <td>2682.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5336.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4556</th>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8908</th>\n",
              "      <td>661.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>2119.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14029 rows Ã— 891 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2abfe6a7-a6c3-4af3-bd4b-5789c328d5e9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2abfe6a7-a6c3-4af3-bd4b-5789c328d5e9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2abfe6a7-a6c3-4af3-bd4b-5789c328d5e9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       (18, 'const/4')  (106, 'sput-boolean')  (103, 'sput')  \\\n",
              "168               17.0                    0.0            0.0   \n",
              "10922            152.0                    0.0            0.0   \n",
              "10848            102.0                    1.0            1.0   \n",
              "8414              59.0                    0.0            1.0   \n",
              "10030            745.0                   20.0           41.0   \n",
              "...                ...                    ...            ...   \n",
              "5751            1064.0                   57.0           16.0   \n",
              "8198               0.0                    0.0            0.0   \n",
              "3504            5928.0                   30.0           14.0   \n",
              "4556              94.0                    0.0            0.0   \n",
              "8908             661.0                   18.0           47.0   \n",
              "\n",
              "       (34, 'new-instance')  (112, 'invoke-direct')  (105, 'sput-object')  \\\n",
              "168                     9.0                    11.0                   0.0   \n",
              "10922                  82.0                   122.0                   0.0   \n",
              "10848                 262.0                   292.0                   2.0   \n",
              "8414                  201.0                   239.0                   1.0   \n",
              "10030                 111.0                   131.0                 373.0   \n",
              "...                     ...                     ...                   ...   \n",
              "5751                  974.0                  1258.0                 124.0   \n",
              "8198                    0.0                     0.0                   0.0   \n",
              "3504                 2549.0                  5199.0                 379.0   \n",
              "4556                   39.0                    51.0                   2.0   \n",
              "8908                  139.0                   157.0                  58.0   \n",
              "\n",
              "       (26, 'const-string')  (22, 'const-wide/16')  (104, 'sput-wide')  \\\n",
              "168                    36.0                    0.0                 0.0   \n",
              "10922                 171.0                    2.0                 0.0   \n",
              "10848                 264.0                    6.0                 0.0   \n",
              "8414                  137.0                    2.0                 0.0   \n",
              "10030                 757.0                    0.0                 0.0   \n",
              "...                     ...                    ...                 ...   \n",
              "5751                 2067.0                   23.0                 2.0   \n",
              "8198                    0.0                    0.0                 0.0   \n",
              "3504                 2682.0                  125.0                 0.0   \n",
              "4556                  161.0                    1.0                 0.0   \n",
              "8908                 2119.0                    4.0                 0.0   \n",
              "\n",
              "       (14, 'return-void')  ...  WATCHDOX_API  CopyToClipboard  \\\n",
              "168                    8.0  ...           0.0              0.0   \n",
              "10922                 74.0  ...           0.0              0.0   \n",
              "10848                121.0  ...           0.0              0.0   \n",
              "8414                  76.0  ...           0.0              0.0   \n",
              "10030                 77.0  ...           0.0              0.0   \n",
              "...                    ...  ...           ...              ...   \n",
              "5751                 522.0  ...           0.0              0.0   \n",
              "8198                   0.0  ...           0.0              0.0   \n",
              "3504                5336.0  ...           0.0              0.0   \n",
              "4556                  35.0  ...           0.0              0.0   \n",
              "8908                  53.0  ...           0.0              0.0   \n",
              "\n",
              "       KP2aInternalFileBrowsing  BLACKLISTED_USB_DEVICE  READ_NOTE  \\\n",
              "168                         0.0                     0.0        0.0   \n",
              "10922                       0.0                     0.0        0.0   \n",
              "10848                       0.0                     0.0        0.0   \n",
              "8414                        0.0                     0.0        0.0   \n",
              "10030                       0.0                     0.0        0.0   \n",
              "...                         ...                     ...        ...   \n",
              "5751                        0.0                     0.0        0.0   \n",
              "8198                        0.0                     0.0        0.0   \n",
              "3504                        0.0                     0.0        0.0   \n",
              "4556                        0.0                     0.0        0.0   \n",
              "8908                        0.0                     0.0        0.0   \n",
              "\n",
              "       RECEIVE_DATA  SEND_FEEDBACK  ACCESS_NETWORK  READ_ONLY  \\\n",
              "168             0.0            0.0             0.0        0.0   \n",
              "10922           0.0            0.0             0.0        0.0   \n",
              "10848           0.0            0.0             0.0        0.0   \n",
              "8414            0.0            0.0             0.0        0.0   \n",
              "10030           0.0            0.0             0.0        0.0   \n",
              "...             ...            ...             ...        ...   \n",
              "5751            0.0            0.0             0.0        0.0   \n",
              "8198            0.0            0.0             0.0        0.0   \n",
              "3504            0.0            0.0             0.0        0.0   \n",
              "4556            0.0            0.0             0.0        0.0   \n",
              "8908            0.0            0.0             0.0        0.0   \n",
              "\n",
              "       ACCESS_NETWORK_CHANGE  \n",
              "168                      0.0  \n",
              "10922                    0.0  \n",
              "10848                    0.0  \n",
              "8414                     0.0  \n",
              "10030                    0.0  \n",
              "...                      ...  \n",
              "5751                     0.0  \n",
              "8198                     0.0  \n",
              "3504                     0.0  \n",
              "4556                     0.0  \n",
              "8908                     0.0  \n",
              "\n",
              "[14029 rows x 891 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = x.drop('APP', axis=1)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Mn7i5JLL5aqP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "\n",
        "from tensorflow.keras import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "RFWanJ7bsV7j",
        "outputId": "465d008e-d6e2-4a19-ddba-19c99d6c9d38"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-06b6b267-edb7-4f1b-9ff5-f90249ab0cef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(18, 'const/4')</th>\n",
              "      <th>(106, 'sput-boolean')</th>\n",
              "      <th>(103, 'sput')</th>\n",
              "      <th>(34, 'new-instance')</th>\n",
              "      <th>(112, 'invoke-direct')</th>\n",
              "      <th>(105, 'sput-object')</th>\n",
              "      <th>(26, 'const-string')</th>\n",
              "      <th>(22, 'const-wide/16')</th>\n",
              "      <th>(104, 'sput-wide')</th>\n",
              "      <th>(14, 'return-void')</th>\n",
              "      <th>...</th>\n",
              "      <th>WATCHDOX_API</th>\n",
              "      <th>CopyToClipboard</th>\n",
              "      <th>KP2aInternalFileBrowsing</th>\n",
              "      <th>BLACKLISTED_USB_DEVICE</th>\n",
              "      <th>READ_NOTE</th>\n",
              "      <th>RECEIVE_DATA</th>\n",
              "      <th>SEND_FEEDBACK</th>\n",
              "      <th>ACCESS_NETWORK</th>\n",
              "      <th>READ_ONLY</th>\n",
              "      <th>ACCESS_NETWORK_CHANGE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>168</th>\n",
              "      <td>17.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>36.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10922</th>\n",
              "      <td>152.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>171.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>74.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10848</th>\n",
              "      <td>102.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>292.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>264.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8414</th>\n",
              "      <td>59.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>201.0</td>\n",
              "      <td>239.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>137.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10030</th>\n",
              "      <td>745.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>41.0</td>\n",
              "      <td>111.0</td>\n",
              "      <td>131.0</td>\n",
              "      <td>373.0</td>\n",
              "      <td>757.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>77.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5751</th>\n",
              "      <td>1064.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>974.0</td>\n",
              "      <td>1258.0</td>\n",
              "      <td>124.0</td>\n",
              "      <td>2067.0</td>\n",
              "      <td>23.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>522.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8198</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3504</th>\n",
              "      <td>5928.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>2549.0</td>\n",
              "      <td>5199.0</td>\n",
              "      <td>379.0</td>\n",
              "      <td>2682.0</td>\n",
              "      <td>125.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5336.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4556</th>\n",
              "      <td>94.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>161.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8908</th>\n",
              "      <td>661.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>47.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>157.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>2119.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>53.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14029 rows Ã— 891 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06b6b267-edb7-4f1b-9ff5-f90249ab0cef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-06b6b267-edb7-4f1b-9ff5-f90249ab0cef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-06b6b267-edb7-4f1b-9ff5-f90249ab0cef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       (18, 'const/4')  (106, 'sput-boolean')  (103, 'sput')  \\\n",
              "168               17.0                    0.0            0.0   \n",
              "10922            152.0                    0.0            0.0   \n",
              "10848            102.0                    1.0            1.0   \n",
              "8414              59.0                    0.0            1.0   \n",
              "10030            745.0                   20.0           41.0   \n",
              "...                ...                    ...            ...   \n",
              "5751            1064.0                   57.0           16.0   \n",
              "8198               0.0                    0.0            0.0   \n",
              "3504            5928.0                   30.0           14.0   \n",
              "4556              94.0                    0.0            0.0   \n",
              "8908             661.0                   18.0           47.0   \n",
              "\n",
              "       (34, 'new-instance')  (112, 'invoke-direct')  (105, 'sput-object')  \\\n",
              "168                     9.0                    11.0                   0.0   \n",
              "10922                  82.0                   122.0                   0.0   \n",
              "10848                 262.0                   292.0                   2.0   \n",
              "8414                  201.0                   239.0                   1.0   \n",
              "10030                 111.0                   131.0                 373.0   \n",
              "...                     ...                     ...                   ...   \n",
              "5751                  974.0                  1258.0                 124.0   \n",
              "8198                    0.0                     0.0                   0.0   \n",
              "3504                 2549.0                  5199.0                 379.0   \n",
              "4556                   39.0                    51.0                   2.0   \n",
              "8908                  139.0                   157.0                  58.0   \n",
              "\n",
              "       (26, 'const-string')  (22, 'const-wide/16')  (104, 'sput-wide')  \\\n",
              "168                    36.0                    0.0                 0.0   \n",
              "10922                 171.0                    2.0                 0.0   \n",
              "10848                 264.0                    6.0                 0.0   \n",
              "8414                  137.0                    2.0                 0.0   \n",
              "10030                 757.0                    0.0                 0.0   \n",
              "...                     ...                    ...                 ...   \n",
              "5751                 2067.0                   23.0                 2.0   \n",
              "8198                    0.0                    0.0                 0.0   \n",
              "3504                 2682.0                  125.0                 0.0   \n",
              "4556                  161.0                    1.0                 0.0   \n",
              "8908                 2119.0                    4.0                 0.0   \n",
              "\n",
              "       (14, 'return-void')  ...  WATCHDOX_API  CopyToClipboard  \\\n",
              "168                    8.0  ...           0.0              0.0   \n",
              "10922                 74.0  ...           0.0              0.0   \n",
              "10848                121.0  ...           0.0              0.0   \n",
              "8414                  76.0  ...           0.0              0.0   \n",
              "10030                 77.0  ...           0.0              0.0   \n",
              "...                    ...  ...           ...              ...   \n",
              "5751                 522.0  ...           0.0              0.0   \n",
              "8198                   0.0  ...           0.0              0.0   \n",
              "3504                5336.0  ...           0.0              0.0   \n",
              "4556                  35.0  ...           0.0              0.0   \n",
              "8908                  53.0  ...           0.0              0.0   \n",
              "\n",
              "       KP2aInternalFileBrowsing  BLACKLISTED_USB_DEVICE  READ_NOTE  \\\n",
              "168                         0.0                     0.0        0.0   \n",
              "10922                       0.0                     0.0        0.0   \n",
              "10848                       0.0                     0.0        0.0   \n",
              "8414                        0.0                     0.0        0.0   \n",
              "10030                       0.0                     0.0        0.0   \n",
              "...                         ...                     ...        ...   \n",
              "5751                        0.0                     0.0        0.0   \n",
              "8198                        0.0                     0.0        0.0   \n",
              "3504                        0.0                     0.0        0.0   \n",
              "4556                        0.0                     0.0        0.0   \n",
              "8908                        0.0                     0.0        0.0   \n",
              "\n",
              "       RECEIVE_DATA  SEND_FEEDBACK  ACCESS_NETWORK  READ_ONLY  \\\n",
              "168             0.0            0.0             0.0        0.0   \n",
              "10922           0.0            0.0             0.0        0.0   \n",
              "10848           0.0            0.0             0.0        0.0   \n",
              "8414            0.0            0.0             0.0        0.0   \n",
              "10030           0.0            0.0             0.0        0.0   \n",
              "...             ...            ...             ...        ...   \n",
              "5751            0.0            0.0             0.0        0.0   \n",
              "8198            0.0            0.0             0.0        0.0   \n",
              "3504            0.0            0.0             0.0        0.0   \n",
              "4556            0.0            0.0             0.0        0.0   \n",
              "8908            0.0            0.0             0.0        0.0   \n",
              "\n",
              "       ACCESS_NETWORK_CHANGE  \n",
              "168                      0.0  \n",
              "10922                    0.0  \n",
              "10848                    0.0  \n",
              "8414                     0.0  \n",
              "10030                    0.0  \n",
              "...                      ...  \n",
              "5751                     0.0  \n",
              "8198                     0.0  \n",
              "3504                     0.0  \n",
              "4556                     0.0  \n",
              "8908                     0.0  \n",
              "\n",
              "[14029 rows x 891 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "_fh1vj7uk4wa"
      },
      "outputs": [],
      "source": [
        "train_ratio = 0.70\n",
        "validation_ratio = 0.15\n",
        "test_ratio = 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "q63YLgeVkbul"
      },
      "outputs": [],
      "source": [
        "trainX, testX, trainY, testY = train_test_split(x, y, test_size= 1 - train_ratio)\n",
        "\n",
        "valX, testX, valY, testY = train_test_split(testX, testY, test_size=test_ratio/(test_ratio + validation_ratio))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "IJP98bRdpyhM"
      },
      "outputs": [],
      "source": [
        "y_train = np.array(trainY).tolist()\n",
        "y_val = np.array(valY).tolist()\n",
        "y_test = np.array(testY).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pG1AiYnk4Kn",
        "outputId": "d040e9ab-8ccf-4d92-96cf-2f37c72879bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(9820, 891)\n",
            "(2104, 891)\n",
            "(2105, 891)\n"
          ]
        }
      ],
      "source": [
        "print(trainX.shape)\n",
        "print(valX.shape)\n",
        "print(testX.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "H_Lddcic1fiD"
      },
      "outputs": [],
      "source": [
        "#Dependencies\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import initializers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {
        "id": "F7ePZ648QxRv"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[891, ]),\n",
        "    keras.layers.Dense(3200, activation=\"tanh\"),\n",
        "    keras.layers.Dense(1600, activation=\"tanh\"),\n",
        "    keras.layers.Dense(800, activation=\"tanh\"),\n",
        "    keras.layers.Dense(400, activation=\"tanh\"),\n",
        "    keras.layers.Dense(200, activation=\"tanh\"),\n",
        "    keras.layers.Dense(100, activation=\"tanh\"),\n",
        "    keras.layers.Dense(5, activation=\"softmax\")\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        },
        "id": "8iong8IZ24Fi",
        "outputId": "187ff29a-d5fa-499d-fd2e-83880a631ac5"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 191,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiDkczxE2gnp",
        "outputId": "dd256de7-876c-4e2e-e064-1dc8f8c3e05b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_5 (Flatten)         (None, 891)               0         \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 3200)              2854400   \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 1600)              5121600   \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 800)               1280800   \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 400)               320400    \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 200)               80200     \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 100)               20100     \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 5)                 505       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9,678,005\n",
            "Trainable params: 9,678,005\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 192,
      "metadata": {
        "id": "b4HD-mCAZxsY"
      },
      "outputs": [],
      "source": [
        "cp = r\"/content/drive/MyDrive/apk-2020-zip/firstModel/modeltl1.h5\"\n",
        "csvl = r\"/content/drive/MyDrive/apk-2020-zip/firstModel/modeltl1.log\"\n",
        "graph = r\"/content/drive/MyDrive/apk-2020-zip/firstModel/modeltl1.png\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {
        "id": "b2mPqN7grd67"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import  Adam\n",
        "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from keras.callbacks import CSVLogger\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 194,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVbaoX_gsJJ3",
        "outputId": "62cffe39-61d5-478d-d08a-8801be4c8b2f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input Shape: 891\n",
            "Batch Size: 128\n",
            "\n",
            "Steps per Epoch: 76\n",
            "\n",
            "Validation Steps: 16\n",
            "Test Steps: 16\n",
            "\n",
            "Number of Epochs: 100\n"
          ]
        }
      ],
      "source": [
        "input_shape = trainX.shape[1]\n",
        "\n",
        "n_batch_size = 128\n",
        "\n",
        "n_steps_per_epoch = int(trainX.shape[0] / n_batch_size)\n",
        "n_validation_steps = int(valX.shape[0] / n_batch_size)\n",
        "n_test_steps = int(testX.shape[0] / n_batch_size)\n",
        "\n",
        "n_epochs = 100\n",
        "\n",
        "\n",
        "print('Input Shape: ' + str(input_shape))\n",
        "print('Batch Size: ' + str(n_batch_size))\n",
        "print()\n",
        "print('Steps per Epoch: ' + str(n_steps_per_epoch))\n",
        "print()\n",
        "print('Validation Steps: ' + str(n_validation_steps))\n",
        "print('Test Steps: ' + str(n_test_steps))\n",
        "print()\n",
        "print('Number of Epochs: ' + str(n_epochs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "metadata": {
        "id": "9ZGHtHjq1Oak"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "metadata": {
        "id": "CKEC4nxTUPQu"
      },
      "outputs": [],
      "source": [
        "#with reference: https://www.tensorflow.org/guide/keras/custom_callback\n",
        "class ChangeLearningRate(tf.keras.callbacks.Callback):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ChangeLearningRate, self).__init__\n",
        "        self.prev_val_loss = -np.Inf\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))\n",
        "        if (epoch+1) % 10 == 0:\n",
        "            lr = lr - (lr * 0.02)\n",
        "        else:\n",
        "            pass\n",
        "        if logs['val_loss'] < self.prev_val_loss:\n",
        "            lr = lr - (lr*0.025)\n",
        "        else:\n",
        "            pass\n",
        "        print('\\nLearning rate Changed to:{}'.format(round(lr,4)))\n",
        "        self.prev_val_loss = logs['val_loss']\n",
        "        tf.keras.backend.set_value(self.model.optimizer.lr, lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 198,
      "metadata": {
        "id": "WooSDgIxsTvw"
      },
      "outputs": [],
      "source": [
        "lrdecay = ChangeLearningRate()\n",
        "\n",
        "checkpoint = ModelCheckpoint(cp ,\n",
        "                             monitor = \"val_loss\",\n",
        "                             mode = \"min\" ,\n",
        "                            save_best_only = True , verbose = 1)\n",
        "\n",
        "earlystop = EarlyStopping (monitor = \"val_loss\" , min_delta = 0 ,\n",
        "                           patience = 15 , restore_best_weights = True , \n",
        "                           verbose = 1)\n",
        "\n",
        "csv_logger = CSVLogger(csvl, separator=',', append=False)\n",
        "callbacks = [ checkpoint , csv_logger]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "id": "q4n4DWxrmZYf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BTIHAtqLsdIb",
        "outputId": "7f0a58fb-2f33-41bb-dc6e-a65e3f260802"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "opt = SGD(lr=0.01, decay=1e-6,momentum=0.9)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOqUWp9jM3FG",
        "outputId": "545d5005-604e-4703-eeef-ccf3d3316f96"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(18, 'const/4')           float64\n",
              "(106, 'sput-boolean')     float64\n",
              "(103, 'sput')             float64\n",
              "(34, 'new-instance')      float64\n",
              "(112, 'invoke-direct')    float64\n",
              "                           ...   \n",
              "RECEIVE_DATA              float64\n",
              "SEND_FEEDBACK             float64\n",
              "ACCESS_NETWORK            float64\n",
              "READ_ONLY                 float64\n",
              "ACCESS_NETWORK_CHANGE     float64\n",
              "Length: 891, dtype: object"
            ]
          },
          "execution_count": 201,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "SG0_nSCQf6aT",
        "outputId": "447d5afb-b3bb-4be7-b91b-bf8709e18c1b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-142d4d91-66d9-4b2b-8f27-b13288dbf0da\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>(18, 'const/4')</th>\n",
              "      <th>(106, 'sput-boolean')</th>\n",
              "      <th>(103, 'sput')</th>\n",
              "      <th>(34, 'new-instance')</th>\n",
              "      <th>(112, 'invoke-direct')</th>\n",
              "      <th>(105, 'sput-object')</th>\n",
              "      <th>(26, 'const-string')</th>\n",
              "      <th>(22, 'const-wide/16')</th>\n",
              "      <th>(104, 'sput-wide')</th>\n",
              "      <th>(14, 'return-void')</th>\n",
              "      <th>...</th>\n",
              "      <th>WATCHDOX_API</th>\n",
              "      <th>CopyToClipboard</th>\n",
              "      <th>KP2aInternalFileBrowsing</th>\n",
              "      <th>BLACKLISTED_USB_DEVICE</th>\n",
              "      <th>READ_NOTE</th>\n",
              "      <th>RECEIVE_DATA</th>\n",
              "      <th>SEND_FEEDBACK</th>\n",
              "      <th>ACCESS_NETWORK</th>\n",
              "      <th>READ_ONLY</th>\n",
              "      <th>ACCESS_NETWORK_CHANGE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1346</th>\n",
              "      <td>2023.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1250.0</td>\n",
              "      <td>1742.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>1598.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1292.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9148</th>\n",
              "      <td>156.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>82.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>176.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>75.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3331</th>\n",
              "      <td>1333.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>984.0</td>\n",
              "      <td>1537.0</td>\n",
              "      <td>113.0</td>\n",
              "      <td>1213.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1277.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9619</th>\n",
              "      <td>102.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>262.0</td>\n",
              "      <td>292.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>264.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>121.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13080</th>\n",
              "      <td>23091.0</td>\n",
              "      <td>71.0</td>\n",
              "      <td>58.0</td>\n",
              "      <td>13625.0</td>\n",
              "      <td>21324.0</td>\n",
              "      <td>2798.0</td>\n",
              "      <td>19203.0</td>\n",
              "      <td>557.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16415.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8710</th>\n",
              "      <td>159.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>57.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>116.0</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>55.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>534</th>\n",
              "      <td>35.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>14.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12940</th>\n",
              "      <td>32593.0</td>\n",
              "      <td>156.0</td>\n",
              "      <td>164.0</td>\n",
              "      <td>24340.0</td>\n",
              "      <td>39321.0</td>\n",
              "      <td>5279.0</td>\n",
              "      <td>35335.0</td>\n",
              "      <td>1868.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>26623.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8368</th>\n",
              "      <td>100.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>255.0</td>\n",
              "      <td>283.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>263.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>117.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13722</th>\n",
              "      <td>24967.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>16717.0</td>\n",
              "      <td>27630.0</td>\n",
              "      <td>2519.0</td>\n",
              "      <td>24970.0</td>\n",
              "      <td>781.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>19950.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2104 rows Ã— 891 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-142d4d91-66d9-4b2b-8f27-b13288dbf0da')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-142d4d91-66d9-4b2b-8f27-b13288dbf0da button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-142d4d91-66d9-4b2b-8f27-b13288dbf0da');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       (18, 'const/4')  (106, 'sput-boolean')  (103, 'sput')  \\\n",
              "1346            2023.0                   17.0            3.0   \n",
              "9148             156.0                    0.0            0.0   \n",
              "3331            1333.0                    2.0            3.0   \n",
              "9619             102.0                    1.0            1.0   \n",
              "13080          23091.0                   71.0           58.0   \n",
              "...                ...                    ...            ...   \n",
              "8710             159.0                    6.0            8.0   \n",
              "534               35.0                    0.0            0.0   \n",
              "12940          32593.0                  156.0          164.0   \n",
              "8368             100.0                    1.0            1.0   \n",
              "13722          24967.0                  119.0           76.0   \n",
              "\n",
              "       (34, 'new-instance')  (112, 'invoke-direct')  (105, 'sput-object')  \\\n",
              "1346                 1250.0                  1742.0                 140.0   \n",
              "9148                   82.0                   122.0                   0.0   \n",
              "3331                  984.0                  1537.0                 113.0   \n",
              "9619                  262.0                   292.0                   2.0   \n",
              "13080               13625.0                 21324.0                2798.0   \n",
              "...                     ...                     ...                   ...   \n",
              "8710                   46.0                    57.0                  10.0   \n",
              "534                    14.0                    16.0                   2.0   \n",
              "12940               24340.0                 39321.0                5279.0   \n",
              "8368                  255.0                   283.0                   2.0   \n",
              "13722               16717.0                 27630.0                2519.0   \n",
              "\n",
              "       (26, 'const-string')  (22, 'const-wide/16')  (104, 'sput-wide')  \\\n",
              "1346                 1598.0                   58.0                 0.0   \n",
              "9148                  176.0                    2.0                 0.0   \n",
              "3331                 1213.0                   25.0                 0.0   \n",
              "9619                  264.0                    6.0                 0.0   \n",
              "13080               19203.0                  557.0                20.0   \n",
              "...                     ...                    ...                 ...   \n",
              "8710                  116.0                   26.0                 0.0   \n",
              "534                    56.0                    5.0                 0.0   \n",
              "12940               35335.0                 1868.0                15.0   \n",
              "8368                  263.0                    6.0                 0.0   \n",
              "13722               24970.0                  781.0                16.0   \n",
              "\n",
              "       (14, 'return-void')  ...  WATCHDOX_API  CopyToClipboard  \\\n",
              "1346                1292.0  ...           0.0              0.0   \n",
              "9148                  75.0  ...           0.0              0.0   \n",
              "3331                1277.0  ...           0.0              0.0   \n",
              "9619                 121.0  ...           0.0              0.0   \n",
              "13080              16415.0  ...           0.0              0.0   \n",
              "...                    ...  ...           ...              ...   \n",
              "8710                  55.0  ...           0.0              0.0   \n",
              "534                   10.0  ...           0.0              0.0   \n",
              "12940              26623.0  ...           0.0              0.0   \n",
              "8368                 117.0  ...           0.0              0.0   \n",
              "13722              19950.0  ...           0.0              0.0   \n",
              "\n",
              "       KP2aInternalFileBrowsing  BLACKLISTED_USB_DEVICE  READ_NOTE  \\\n",
              "1346                        0.0                     0.0        0.0   \n",
              "9148                        0.0                     0.0        0.0   \n",
              "3331                        0.0                     0.0        0.0   \n",
              "9619                        0.0                     0.0        0.0   \n",
              "13080                       0.0                     0.0        0.0   \n",
              "...                         ...                     ...        ...   \n",
              "8710                        0.0                     0.0        0.0   \n",
              "534                         0.0                     0.0        0.0   \n",
              "12940                       0.0                     0.0        0.0   \n",
              "8368                        0.0                     0.0        0.0   \n",
              "13722                       0.0                     0.0        0.0   \n",
              "\n",
              "       RECEIVE_DATA  SEND_FEEDBACK  ACCESS_NETWORK  READ_ONLY  \\\n",
              "1346            0.0            0.0             0.0        0.0   \n",
              "9148            0.0            0.0             0.0        0.0   \n",
              "3331            0.0            0.0             0.0        0.0   \n",
              "9619            0.0            0.0             0.0        0.0   \n",
              "13080           0.0            0.0             0.0        0.0   \n",
              "...             ...            ...             ...        ...   \n",
              "8710            0.0            0.0             0.0        0.0   \n",
              "534             0.0            0.0             0.0        0.0   \n",
              "12940           0.0            0.0             0.0        0.0   \n",
              "8368            0.0            0.0             0.0        0.0   \n",
              "13722           0.0            0.0             0.0        0.0   \n",
              "\n",
              "       ACCESS_NETWORK_CHANGE  \n",
              "1346                     0.0  \n",
              "9148                     0.0  \n",
              "3331                     0.0  \n",
              "9619                     0.0  \n",
              "13080                    0.0  \n",
              "...                      ...  \n",
              "8710                     0.0  \n",
              "534                      0.0  \n",
              "12940                    0.0  \n",
              "8368                     0.0  \n",
              "13722                    0.0  \n",
              "\n",
              "[2104 rows x 891 columns]"
            ]
          },
          "execution_count": 202,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "gNWn-LQPKId0",
        "outputId": "069a26f6-f1a2-44d1-8428-0fe493b28614"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-74cd7745-38cf-469f-86af-fad1a7fa5763\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TYPE_0</th>\n",
              "      <th>TYPE_1</th>\n",
              "      <th>TYPE_2</th>\n",
              "      <th>TYPE_3</th>\n",
              "      <th>TYPE_4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1346</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9148</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3331</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9619</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13080</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8710</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>534</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12940</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8368</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13722</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2104 rows Ã— 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-74cd7745-38cf-469f-86af-fad1a7fa5763')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-74cd7745-38cf-469f-86af-fad1a7fa5763 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-74cd7745-38cf-469f-86af-fad1a7fa5763');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       TYPE_0  TYPE_1  TYPE_2  TYPE_3  TYPE_4\n",
              "1346        0       0       0       1       0\n",
              "9148        0       0       0       0       1\n",
              "3331        0       1       0       0       0\n",
              "9619        0       0       0       0       1\n",
              "13080       0       0       1       0       0\n",
              "...       ...     ...     ...     ...     ...\n",
              "8710        0       0       0       0       1\n",
              "534         0       0       0       1       0\n",
              "12940       0       0       1       0       0\n",
              "8368        0       0       0       0       1\n",
              "13722       0       0       1       0       0\n",
              "\n",
              "[2104 rows x 5 columns]"
            ]
          },
          "execution_count": 203,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "valY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56-3QGg8G4Gv",
        "outputId": "a1b9c049-c8bb-4baa-f3a7-b2c896cd935a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(None, 891) <dtype: 'float32'>\n",
            "(None, 5) <dtype: 'float32'>\n",
            "flatten_5 (None, 891) float32\n",
            "dense_23 (None, 891) float32\n",
            "dense_24 (None, 3200) float32\n",
            "dense_25 (None, 1600) float32\n",
            "dense_26 (None, 800) float32\n",
            "dense_27 (None, 400) float32\n",
            "dense_28 (None, 200) float32\n",
            "dense_29 (None, 100) float32\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[None, None, None, None, None, None, None, None]"
            ]
          },
          "execution_count": 204,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "[print(i.shape, i.dtype) for i in model.inputs]\n",
        "[print(o.shape, o.dtype) for o in model.outputs]\n",
        "[print(l.name, l.input_shape, l.dtype) for l in model.layers]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DayGS4mfZywo",
        "outputId": "6191bd72-1653-4bf2-d1d9-5e03e8966e28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.3726 - accuracy: 0.4206\n",
            "Epoch 1: val_loss improved from inf to 1.64349, saving model to /content/drive/MyDrive/apk-2020-zip/firstModel/modeltl1.h5\n",
            "76/76 [==============================] - 2s 23ms/step - loss: 1.3805 - accuracy: 0.4180 - val_loss: 1.6435 - val_accuracy: 0.4209\n",
            "Epoch 2/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.3612 - accuracy: 0.4387\n",
            "Epoch 2: val_loss improved from 1.64349 to 1.28975, saving model to /content/drive/MyDrive/apk-2020-zip/firstModel/modeltl1.h5\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 1.3551 - accuracy: 0.4437 - val_loss: 1.2897 - val_accuracy: 0.5630\n",
            "Epoch 3/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4860 - accuracy: 0.3958\n",
            "Epoch 3: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4791 - accuracy: 0.3969 - val_loss: 1.3867 - val_accuracy: 0.4404\n",
            "Epoch 4/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5037 - accuracy: 0.3759\n",
            "Epoch 4: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4989 - accuracy: 0.3773 - val_loss: 1.3718 - val_accuracy: 0.4336\n",
            "Epoch 5/600\n",
            "75/76 [============================>.] - ETA: 0s - loss: 1.3799 - accuracy: 0.4189\n",
            "Epoch 5: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.3792 - accuracy: 0.4190 - val_loss: 1.3192 - val_accuracy: 0.4360\n",
            "Epoch 6/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4573 - accuracy: 0.3724\n",
            "Epoch 6: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4579 - accuracy: 0.3746 - val_loss: 1.4813 - val_accuracy: 0.3535\n",
            "Epoch 7/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.4657 - accuracy: 0.3831\n",
            "Epoch 7: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4640 - accuracy: 0.3836 - val_loss: 1.5364 - val_accuracy: 0.3604\n",
            "Epoch 8/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4807 - accuracy: 0.3875\n",
            "Epoch 8: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4876 - accuracy: 0.3844 - val_loss: 1.5796 - val_accuracy: 0.3496\n",
            "Epoch 9/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5341 - accuracy: 0.3175\n",
            "Epoch 9: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5321 - accuracy: 0.3186 - val_loss: 1.5038 - val_accuracy: 0.3535\n",
            "Epoch 10/600\n",
            "75/76 [============================>.] - ETA: 0s - loss: 1.4302 - accuracy: 0.4110\n",
            "Epoch 10: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4310 - accuracy: 0.4115 - val_loss: 1.3540 - val_accuracy: 0.4131\n",
            "Epoch 11/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.4563 - accuracy: 0.3767\n",
            "Epoch 11: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4579 - accuracy: 0.3758 - val_loss: 1.5075 - val_accuracy: 0.3511\n",
            "Epoch 12/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5065 - accuracy: 0.3694\n",
            "Epoch 12: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5061 - accuracy: 0.3686 - val_loss: 1.5168 - val_accuracy: 0.3535\n",
            "Epoch 13/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.4722 - accuracy: 0.4008\n",
            "Epoch 13: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4744 - accuracy: 0.3969 - val_loss: 1.4929 - val_accuracy: 0.3594\n",
            "Epoch 14/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4222 - accuracy: 0.3989\n",
            "Epoch 14: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4248 - accuracy: 0.3967 - val_loss: 1.4761 - val_accuracy: 0.4414\n",
            "Epoch 15/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4384 - accuracy: 0.3931\n",
            "Epoch 15: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4342 - accuracy: 0.3947 - val_loss: 1.4140 - val_accuracy: 0.3267\n",
            "Epoch 16/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5086 - accuracy: 0.3335\n",
            "Epoch 16: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5100 - accuracy: 0.3330 - val_loss: 1.5253 - val_accuracy: 0.3608\n",
            "Epoch 17/600\n",
            "64/76 [========================>.....] - ETA: 0s - loss: 1.5209 - accuracy: 0.3397\n",
            "Epoch 17: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5207 - accuracy: 0.3379 - val_loss: 1.5395 - val_accuracy: 0.2422\n",
            "Epoch 18/600\n",
            "76/76 [==============================] - ETA: 0s - loss: 1.4484 - accuracy: 0.3900\n",
            "Epoch 18: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4484 - accuracy: 0.3900 - val_loss: 1.3278 - val_accuracy: 0.4399\n",
            "Epoch 19/600\n",
            "75/76 [============================>.] - ETA: 0s - loss: 1.3605 - accuracy: 0.4027\n",
            "Epoch 19: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.3639 - accuracy: 0.4019 - val_loss: 1.4465 - val_accuracy: 0.3608\n",
            "Epoch 20/600\n",
            "75/76 [============================>.] - ETA: 0s - loss: 1.5322 - accuracy: 0.3243\n",
            "Epoch 20: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5312 - accuracy: 0.3251 - val_loss: 1.5026 - val_accuracy: 0.3535\n",
            "Epoch 21/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5170 - accuracy: 0.3737\n",
            "Epoch 21: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5185 - accuracy: 0.3717 - val_loss: 1.5139 - val_accuracy: 0.3809\n",
            "Epoch 22/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4527 - accuracy: 0.3780\n",
            "Epoch 22: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4677 - accuracy: 0.3755 - val_loss: 1.7420 - val_accuracy: 0.2329\n",
            "Epoch 23/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5087 - accuracy: 0.3374\n",
            "Epoch 23: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5059 - accuracy: 0.3417 - val_loss: 1.3379 - val_accuracy: 0.4448\n",
            "Epoch 24/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5689 - accuracy: 0.3242\n",
            "Epoch 24: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5674 - accuracy: 0.3250 - val_loss: 1.5242 - val_accuracy: 0.3535\n",
            "Epoch 25/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5126 - accuracy: 0.3405\n",
            "Epoch 25: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5145 - accuracy: 0.3382 - val_loss: 1.5231 - val_accuracy: 0.2422\n",
            "Epoch 26/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5202 - accuracy: 0.3371\n",
            "Epoch 26: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5158 - accuracy: 0.3431 - val_loss: 1.4786 - val_accuracy: 0.3560\n",
            "Epoch 27/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4508 - accuracy: 0.4012\n",
            "Epoch 27: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4536 - accuracy: 0.3986 - val_loss: 1.4755 - val_accuracy: 0.3604\n",
            "Epoch 28/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.4109 - accuracy: 0.4142\n",
            "Epoch 28: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4281 - accuracy: 0.4063 - val_loss: 1.5071 - val_accuracy: 0.3535\n",
            "Epoch 29/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4845 - accuracy: 0.3618\n",
            "Epoch 29: val_loss did not improve from 1.28975\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4806 - accuracy: 0.3635 - val_loss: 1.4222 - val_accuracy: 0.3608\n",
            "Epoch 30/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.4465 - accuracy: 0.3717\n",
            "Epoch 30: val_loss improved from 1.28975 to 1.27960, saving model to /content/drive/MyDrive/apk-2020-zip/firstModel/modeltl1.h5\n",
            "76/76 [==============================] - 1s 8ms/step - loss: 1.4440 - accuracy: 0.3742 - val_loss: 1.2796 - val_accuracy: 0.4600\n",
            "Epoch 31/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.4417 - accuracy: 0.3877\n",
            "Epoch 31: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4407 - accuracy: 0.3885 - val_loss: 1.3666 - val_accuracy: 0.3618\n",
            "Epoch 32/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5644 - accuracy: 0.3487\n",
            "Epoch 32: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5631 - accuracy: 0.3474 - val_loss: 1.5098 - val_accuracy: 0.3535\n",
            "Epoch 33/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5180 - accuracy: 0.3515\n",
            "Epoch 33: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5173 - accuracy: 0.3521 - val_loss: 1.5209 - val_accuracy: 0.3535\n",
            "Epoch 34/600\n",
            "76/76 [==============================] - ETA: 0s - loss: 1.4259 - accuracy: 0.4132\n",
            "Epoch 34: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4259 - accuracy: 0.4132 - val_loss: 1.5449 - val_accuracy: 0.4419\n",
            "Epoch 35/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5635 - accuracy: 0.3233\n",
            "Epoch 35: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5615 - accuracy: 0.3247 - val_loss: 1.5114 - val_accuracy: 0.3535\n",
            "Epoch 36/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5245 - accuracy: 0.3424\n",
            "Epoch 36: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5237 - accuracy: 0.3430 - val_loss: 1.4825 - val_accuracy: 0.3657\n",
            "Epoch 37/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4710 - accuracy: 0.3894\n",
            "Epoch 37: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4693 - accuracy: 0.3905 - val_loss: 1.3601 - val_accuracy: 0.4468\n",
            "Epoch 38/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5458 - accuracy: 0.3586\n",
            "Epoch 38: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5452 - accuracy: 0.3561 - val_loss: 1.5415 - val_accuracy: 0.2329\n",
            "Epoch 39/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5272 - accuracy: 0.3255\n",
            "Epoch 39: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5279 - accuracy: 0.3248 - val_loss: 1.5150 - val_accuracy: 0.3521\n",
            "Epoch 40/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5212 - accuracy: 0.3334\n",
            "Epoch 40: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5209 - accuracy: 0.3340 - val_loss: 1.5188 - val_accuracy: 0.3535\n",
            "Epoch 41/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5198 - accuracy: 0.3414\n",
            "Epoch 41: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5202 - accuracy: 0.3410 - val_loss: 1.5187 - val_accuracy: 0.3535\n",
            "Epoch 42/600\n",
            "64/76 [========================>.....] - ETA: 0s - loss: 1.5218 - accuracy: 0.3339\n",
            "Epoch 42: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5220 - accuracy: 0.3307 - val_loss: 1.5244 - val_accuracy: 0.3535\n",
            "Epoch 43/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5192 - accuracy: 0.3269\n",
            "Epoch 43: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5198 - accuracy: 0.3269 - val_loss: 1.5169 - val_accuracy: 0.3535\n",
            "Epoch 44/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5210 - accuracy: 0.3300\n",
            "Epoch 44: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5216 - accuracy: 0.3302 - val_loss: 1.5189 - val_accuracy: 0.3535\n",
            "Epoch 45/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5212 - accuracy: 0.3334\n",
            "Epoch 45: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5213 - accuracy: 0.3334 - val_loss: 1.5264 - val_accuracy: 0.3608\n",
            "Epoch 46/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.4890 - accuracy: 0.3427\n",
            "Epoch 46: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4879 - accuracy: 0.3430 - val_loss: 1.4044 - val_accuracy: 0.4399\n",
            "Epoch 47/600\n",
            "75/76 [============================>.] - ETA: 0s - loss: 1.5241 - accuracy: 0.3403\n",
            "Epoch 47: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5254 - accuracy: 0.3392 - val_loss: 1.5175 - val_accuracy: 0.3535\n",
            "Epoch 48/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5197 - accuracy: 0.3279\n",
            "Epoch 48: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5195 - accuracy: 0.3283 - val_loss: 1.5127 - val_accuracy: 0.3535\n",
            "Epoch 49/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5161 - accuracy: 0.3381\n",
            "Epoch 49: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5170 - accuracy: 0.3373 - val_loss: 1.5219 - val_accuracy: 0.3535\n",
            "Epoch 50/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5130 - accuracy: 0.3368\n",
            "Epoch 50: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5147 - accuracy: 0.3355 - val_loss: 1.5144 - val_accuracy: 0.3535\n",
            "Epoch 51/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5237 - accuracy: 0.3337\n",
            "Epoch 51: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5226 - accuracy: 0.3350 - val_loss: 1.5169 - val_accuracy: 0.3535\n",
            "Epoch 52/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5119 - accuracy: 0.3414\n",
            "Epoch 52: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5143 - accuracy: 0.3391 - val_loss: 1.5243 - val_accuracy: 0.3535\n",
            "Epoch 53/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5213 - accuracy: 0.3365\n",
            "Epoch 53: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5206 - accuracy: 0.3369 - val_loss: 1.5132 - val_accuracy: 0.3535\n",
            "Epoch 54/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5160 - accuracy: 0.3395\n",
            "Epoch 54: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5158 - accuracy: 0.3398 - val_loss: 1.5131 - val_accuracy: 0.3535\n",
            "Epoch 55/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5195 - accuracy: 0.3327\n",
            "Epoch 55: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5206 - accuracy: 0.3281 - val_loss: 1.5323 - val_accuracy: 0.2334\n",
            "Epoch 56/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5183 - accuracy: 0.3414\n",
            "Epoch 56: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5194 - accuracy: 0.3406 - val_loss: 1.5137 - val_accuracy: 0.3535\n",
            "Epoch 57/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5191 - accuracy: 0.3234\n",
            "Epoch 57: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5184 - accuracy: 0.3249 - val_loss: 1.5145 - val_accuracy: 0.3535\n",
            "Epoch 58/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5185 - accuracy: 0.3427\n",
            "Epoch 58: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5178 - accuracy: 0.3433 - val_loss: 1.5186 - val_accuracy: 0.3535\n",
            "Epoch 59/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5035 - accuracy: 0.3516\n",
            "Epoch 59: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5006 - accuracy: 0.3557 - val_loss: 1.4681 - val_accuracy: 0.3594\n",
            "Epoch 60/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4713 - accuracy: 0.3756\n",
            "Epoch 60: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4680 - accuracy: 0.3784 - val_loss: 1.3771 - val_accuracy: 0.4438\n",
            "Epoch 61/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4323 - accuracy: 0.4033\n",
            "Epoch 61: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4427 - accuracy: 0.3968 - val_loss: 1.6997 - val_accuracy: 0.3535\n",
            "Epoch 62/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5120 - accuracy: 0.3492\n",
            "Epoch 62: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5114 - accuracy: 0.3491 - val_loss: 1.4007 - val_accuracy: 0.3628\n",
            "Epoch 63/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.4961 - accuracy: 0.3576\n",
            "Epoch 63: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4909 - accuracy: 0.3598 - val_loss: 1.3436 - val_accuracy: 0.4443\n",
            "Epoch 64/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5552 - accuracy: 0.3244\n",
            "Epoch 64: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5545 - accuracy: 0.3243 - val_loss: 1.5095 - val_accuracy: 0.3535\n",
            "Epoch 65/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5129 - accuracy: 0.3391\n",
            "Epoch 65: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5107 - accuracy: 0.3405 - val_loss: 1.5109 - val_accuracy: 0.3535\n",
            "Epoch 66/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5196 - accuracy: 0.3390\n",
            "Epoch 66: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5188 - accuracy: 0.3395 - val_loss: 1.5034 - val_accuracy: 0.3535\n",
            "Epoch 67/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4596 - accuracy: 0.3861\n",
            "Epoch 67: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4574 - accuracy: 0.3890 - val_loss: 1.4926 - val_accuracy: 0.4312\n",
            "Epoch 68/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5207 - accuracy: 0.3481\n",
            "Epoch 68: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5159 - accuracy: 0.3520 - val_loss: 1.4068 - val_accuracy: 0.4434\n",
            "Epoch 69/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4054 - accuracy: 0.4121\n",
            "Epoch 69: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4041 - accuracy: 0.4122 - val_loss: 1.3401 - val_accuracy: 0.4175\n",
            "Epoch 70/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4279 - accuracy: 0.3752\n",
            "Epoch 70: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4323 - accuracy: 0.3720 - val_loss: 1.5299 - val_accuracy: 0.3535\n",
            "Epoch 71/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5268 - accuracy: 0.3312\n",
            "Epoch 71: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5263 - accuracy: 0.3299 - val_loss: 1.5271 - val_accuracy: 0.3535\n",
            "Epoch 72/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5199 - accuracy: 0.3351\n",
            "Epoch 72: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5208 - accuracy: 0.3327 - val_loss: 1.5364 - val_accuracy: 0.2334\n",
            "Epoch 73/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5185 - accuracy: 0.3339\n",
            "Epoch 73: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5196 - accuracy: 0.3306 - val_loss: 1.5151 - val_accuracy: 0.3535\n",
            "Epoch 74/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5179 - accuracy: 0.3341\n",
            "Epoch 74: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5161 - accuracy: 0.3352 - val_loss: 1.5113 - val_accuracy: 0.3535\n",
            "Epoch 75/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5173 - accuracy: 0.3403\n",
            "Epoch 75: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5172 - accuracy: 0.3380 - val_loss: 1.5354 - val_accuracy: 0.2334\n",
            "Epoch 76/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5132 - accuracy: 0.3426\n",
            "Epoch 76: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5120 - accuracy: 0.3429 - val_loss: 1.5002 - val_accuracy: 0.3535\n",
            "Epoch 77/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4610 - accuracy: 0.3968\n",
            "Epoch 77: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4623 - accuracy: 0.3957 - val_loss: 1.4572 - val_accuracy: 0.3535\n",
            "Epoch 78/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4749 - accuracy: 0.3556\n",
            "Epoch 78: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4768 - accuracy: 0.3523 - val_loss: 1.5316 - val_accuracy: 0.2334\n",
            "Epoch 79/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5217 - accuracy: 0.3253\n",
            "Epoch 79: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5224 - accuracy: 0.3252 - val_loss: 1.5137 - val_accuracy: 0.3535\n",
            "Epoch 80/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5190 - accuracy: 0.3327\n",
            "Epoch 80: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5185 - accuracy: 0.3320 - val_loss: 1.5257 - val_accuracy: 0.3535\n",
            "Epoch 81/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5260 - accuracy: 0.3330\n",
            "Epoch 81: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5263 - accuracy: 0.3330 - val_loss: 1.5135 - val_accuracy: 0.3535\n",
            "Epoch 82/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5208 - accuracy: 0.3193\n",
            "Epoch 82: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5223 - accuracy: 0.3156 - val_loss: 1.5181 - val_accuracy: 0.3535\n",
            "Epoch 83/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5211 - accuracy: 0.3241\n",
            "Epoch 83: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5215 - accuracy: 0.3243 - val_loss: 1.5114 - val_accuracy: 0.3535\n",
            "Epoch 84/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5144 - accuracy: 0.3307\n",
            "Epoch 84: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5156 - accuracy: 0.3308 - val_loss: 1.5087 - val_accuracy: 0.3535\n",
            "Epoch 85/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5118 - accuracy: 0.3452\n",
            "Epoch 85: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5121 - accuracy: 0.3440 - val_loss: 1.4942 - val_accuracy: 0.3599\n",
            "Epoch 86/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4947 - accuracy: 0.3757\n",
            "Epoch 86: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4964 - accuracy: 0.3715 - val_loss: 1.5434 - val_accuracy: 0.3535\n",
            "Epoch 87/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5233 - accuracy: 0.3413\n",
            "Epoch 87: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5231 - accuracy: 0.3408 - val_loss: 1.5091 - val_accuracy: 0.3535\n",
            "Epoch 88/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5201 - accuracy: 0.3304\n",
            "Epoch 88: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5204 - accuracy: 0.3318 - val_loss: 1.5206 - val_accuracy: 0.3535\n",
            "Epoch 89/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5156 - accuracy: 0.3406\n",
            "Epoch 89: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5154 - accuracy: 0.3413 - val_loss: 1.5138 - val_accuracy: 0.3535\n",
            "Epoch 90/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5219 - accuracy: 0.3279\n",
            "Epoch 90: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5199 - accuracy: 0.3293 - val_loss: 1.5193 - val_accuracy: 0.3535\n",
            "Epoch 91/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5144 - accuracy: 0.3266\n",
            "Epoch 91: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5143 - accuracy: 0.3272 - val_loss: 1.5119 - val_accuracy: 0.3535\n",
            "Epoch 92/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5195 - accuracy: 0.3359\n",
            "Epoch 92: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5203 - accuracy: 0.3365 - val_loss: 1.5132 - val_accuracy: 0.3535\n",
            "Epoch 93/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5220 - accuracy: 0.3245\n",
            "Epoch 93: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5215 - accuracy: 0.3245 - val_loss: 1.5166 - val_accuracy: 0.3535\n",
            "Epoch 94/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5168 - accuracy: 0.3322\n",
            "Epoch 94: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5170 - accuracy: 0.3320 - val_loss: 1.5250 - val_accuracy: 0.3535\n",
            "Epoch 95/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5180 - accuracy: 0.3320\n",
            "Epoch 95: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5169 - accuracy: 0.3336 - val_loss: 1.5118 - val_accuracy: 0.3535\n",
            "Epoch 96/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5162 - accuracy: 0.3410\n",
            "Epoch 96: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5162 - accuracy: 0.3402 - val_loss: 1.5308 - val_accuracy: 0.2334\n",
            "Epoch 97/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5124 - accuracy: 0.3351\n",
            "Epoch 97: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5115 - accuracy: 0.3361 - val_loss: 1.5024 - val_accuracy: 0.3535\n",
            "Epoch 98/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4873 - accuracy: 0.3457\n",
            "Epoch 98: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4808 - accuracy: 0.3512 - val_loss: 1.4109 - val_accuracy: 0.3647\n",
            "Epoch 99/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5368 - accuracy: 0.3193\n",
            "Epoch 99: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5367 - accuracy: 0.3176 - val_loss: 1.5502 - val_accuracy: 0.2334\n",
            "Epoch 100/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5189 - accuracy: 0.3271\n",
            "Epoch 100: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5198 - accuracy: 0.3267 - val_loss: 1.5255 - val_accuracy: 0.2334\n",
            "Epoch 101/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5134 - accuracy: 0.3406\n",
            "Epoch 101: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5137 - accuracy: 0.3401 - val_loss: 1.5095 - val_accuracy: 0.3535\n",
            "Epoch 102/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5196 - accuracy: 0.3374\n",
            "Epoch 102: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3336 - val_loss: 1.5222 - val_accuracy: 0.3535\n",
            "Epoch 103/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5200 - accuracy: 0.3340\n",
            "Epoch 103: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5182 - accuracy: 0.3354 - val_loss: 1.5202 - val_accuracy: 0.3535\n",
            "Epoch 104/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5090 - accuracy: 0.3478\n",
            "Epoch 104: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5094 - accuracy: 0.3461 - val_loss: 1.5319 - val_accuracy: 0.2334\n",
            "Epoch 105/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5242 - accuracy: 0.3242\n",
            "Epoch 105: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5246 - accuracy: 0.3242 - val_loss: 1.5136 - val_accuracy: 0.3535\n",
            "Epoch 106/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5155 - accuracy: 0.3394\n",
            "Epoch 106: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5167 - accuracy: 0.3380 - val_loss: 1.5143 - val_accuracy: 0.3535\n",
            "Epoch 107/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5174 - accuracy: 0.3383\n",
            "Epoch 107: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5173 - accuracy: 0.3381 - val_loss: 1.5199 - val_accuracy: 0.3535\n",
            "Epoch 108/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5152 - accuracy: 0.3420\n",
            "Epoch 108: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5157 - accuracy: 0.3417 - val_loss: 1.5156 - val_accuracy: 0.3535\n",
            "Epoch 109/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5127 - accuracy: 0.3422\n",
            "Epoch 109: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5120 - accuracy: 0.3427 - val_loss: 1.5150 - val_accuracy: 0.3535\n",
            "Epoch 110/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5193 - accuracy: 0.3302\n",
            "Epoch 110: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3310 - val_loss: 1.5145 - val_accuracy: 0.3535\n",
            "Epoch 111/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5174 - accuracy: 0.3257\n",
            "Epoch 111: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5178 - accuracy: 0.3271 - val_loss: 1.5118 - val_accuracy: 0.3535\n",
            "Epoch 112/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5223 - accuracy: 0.3285\n",
            "Epoch 112: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5214 - accuracy: 0.3296 - val_loss: 1.5100 - val_accuracy: 0.3535\n",
            "Epoch 113/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5173 - accuracy: 0.3383\n",
            "Epoch 113: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5181 - accuracy: 0.3378 - val_loss: 1.5161 - val_accuracy: 0.3535\n",
            "Epoch 114/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5116 - accuracy: 0.3433\n",
            "Epoch 114: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5123 - accuracy: 0.3400 - val_loss: 1.5150 - val_accuracy: 0.3535\n",
            "Epoch 115/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5084 - accuracy: 0.3467\n",
            "Epoch 115: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5090 - accuracy: 0.3462 - val_loss: 1.5151 - val_accuracy: 0.3535\n",
            "Epoch 116/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5232 - accuracy: 0.3346\n",
            "Epoch 116: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5234 - accuracy: 0.3348 - val_loss: 1.5110 - val_accuracy: 0.3535\n",
            "Epoch 117/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5145 - accuracy: 0.3314\n",
            "Epoch 117: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5147 - accuracy: 0.3315 - val_loss: 1.5111 - val_accuracy: 0.3535\n",
            "Epoch 118/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5165 - accuracy: 0.3382\n",
            "Epoch 118: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5166 - accuracy: 0.3369 - val_loss: 1.5169 - val_accuracy: 0.3535\n",
            "Epoch 119/600\n",
            "76/76 [==============================] - ETA: 0s - loss: 1.5163 - accuracy: 0.3386\n",
            "Epoch 119: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5163 - accuracy: 0.3386 - val_loss: 1.5288 - val_accuracy: 0.2334\n",
            "Epoch 120/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5166 - accuracy: 0.3276\n",
            "Epoch 120: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5165 - accuracy: 0.3279 - val_loss: 1.5118 - val_accuracy: 0.3535\n",
            "Epoch 121/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5127 - accuracy: 0.3454\n",
            "Epoch 121: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5138 - accuracy: 0.3445 - val_loss: 1.5131 - val_accuracy: 0.3535\n",
            "Epoch 122/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5186 - accuracy: 0.3282\n",
            "Epoch 122: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5183 - accuracy: 0.3290 - val_loss: 1.5215 - val_accuracy: 0.3535\n",
            "Epoch 123/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5162 - accuracy: 0.3357\n",
            "Epoch 123: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5162 - accuracy: 0.3367 - val_loss: 1.5136 - val_accuracy: 0.3535\n",
            "Epoch 124/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5179 - accuracy: 0.3345\n",
            "Epoch 124: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5181 - accuracy: 0.3354 - val_loss: 1.5151 - val_accuracy: 0.3535\n",
            "Epoch 125/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5139 - accuracy: 0.3345\n",
            "Epoch 125: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5144 - accuracy: 0.3339 - val_loss: 1.5125 - val_accuracy: 0.3535\n",
            "Epoch 126/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5187 - accuracy: 0.3334\n",
            "Epoch 126: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5171 - accuracy: 0.3327 - val_loss: 1.5114 - val_accuracy: 0.3535\n",
            "Epoch 127/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5212 - accuracy: 0.3237\n",
            "Epoch 127: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5214 - accuracy: 0.3240 - val_loss: 1.5146 - val_accuracy: 0.3535\n",
            "Epoch 128/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5096 - accuracy: 0.3464\n",
            "Epoch 128: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5088 - accuracy: 0.3465 - val_loss: 1.5228 - val_accuracy: 0.3535\n",
            "Epoch 129/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5163 - accuracy: 0.3336\n",
            "Epoch 129: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5159 - accuracy: 0.3349 - val_loss: 1.5096 - val_accuracy: 0.3535\n",
            "Epoch 130/600\n",
            "76/76 [==============================] - ETA: 0s - loss: 1.4947 - accuracy: 0.3505\n",
            "Epoch 130: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4947 - accuracy: 0.3505 - val_loss: 1.4196 - val_accuracy: 0.3633\n",
            "Epoch 131/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5501 - accuracy: 0.3366\n",
            "Epoch 131: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5485 - accuracy: 0.3339 - val_loss: 1.5166 - val_accuracy: 0.3535\n",
            "Epoch 132/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5200 - accuracy: 0.3248\n",
            "Epoch 132: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5194 - accuracy: 0.3262 - val_loss: 1.5115 - val_accuracy: 0.3535\n",
            "Epoch 133/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5174 - accuracy: 0.3286\n",
            "Epoch 133: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5175 - accuracy: 0.3305 - val_loss: 1.5111 - val_accuracy: 0.3535\n",
            "Epoch 134/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5164 - accuracy: 0.3314\n",
            "Epoch 134: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5158 - accuracy: 0.3315 - val_loss: 1.5096 - val_accuracy: 0.3535\n",
            "Epoch 135/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5122 - accuracy: 0.3384\n",
            "Epoch 135: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5142 - accuracy: 0.3373 - val_loss: 1.5128 - val_accuracy: 0.3535\n",
            "Epoch 136/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5220 - accuracy: 0.3364\n",
            "Epoch 136: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5214 - accuracy: 0.3368 - val_loss: 1.5116 - val_accuracy: 0.3535\n",
            "Epoch 137/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5164 - accuracy: 0.3274\n",
            "Epoch 137: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5160 - accuracy: 0.3276 - val_loss: 1.5268 - val_accuracy: 0.3535\n",
            "Epoch 138/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5210 - accuracy: 0.3346\n",
            "Epoch 138: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5203 - accuracy: 0.3356 - val_loss: 1.5146 - val_accuracy: 0.3535\n",
            "Epoch 139/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5160 - accuracy: 0.3372\n",
            "Epoch 139: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5171 - accuracy: 0.3362 - val_loss: 1.5190 - val_accuracy: 0.3535\n",
            "Epoch 140/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5188 - accuracy: 0.3429\n",
            "Epoch 140: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5173 - accuracy: 0.3439 - val_loss: 1.5249 - val_accuracy: 0.3535\n",
            "Epoch 141/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5199 - accuracy: 0.3359\n",
            "Epoch 141: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5199 - accuracy: 0.3368 - val_loss: 1.5216 - val_accuracy: 0.3535\n",
            "Epoch 142/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5239 - accuracy: 0.3281\n",
            "Epoch 142: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5237 - accuracy: 0.3291 - val_loss: 1.5129 - val_accuracy: 0.3535\n",
            "Epoch 143/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5157 - accuracy: 0.3366\n",
            "Epoch 143: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5164 - accuracy: 0.3362 - val_loss: 1.5126 - val_accuracy: 0.3535\n",
            "Epoch 144/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5133 - accuracy: 0.3366\n",
            "Epoch 144: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5140 - accuracy: 0.3375 - val_loss: 1.5128 - val_accuracy: 0.3535\n",
            "Epoch 145/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5150 - accuracy: 0.3402\n",
            "Epoch 145: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5163 - accuracy: 0.3402 - val_loss: 1.5102 - val_accuracy: 0.3535\n",
            "Epoch 146/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5160 - accuracy: 0.3370\n",
            "Epoch 146: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5165 - accuracy: 0.3374 - val_loss: 1.5115 - val_accuracy: 0.3535\n",
            "Epoch 147/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5137 - accuracy: 0.3375\n",
            "Epoch 147: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5132 - accuracy: 0.3350 - val_loss: 1.5167 - val_accuracy: 0.3535\n",
            "Epoch 148/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5199 - accuracy: 0.3330\n",
            "Epoch 148: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 1.5207 - accuracy: 0.3318 - val_loss: 1.5132 - val_accuracy: 0.3535\n",
            "Epoch 149/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5144 - accuracy: 0.3428\n",
            "Epoch 149: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 1.5155 - accuracy: 0.3396 - val_loss: 1.5226 - val_accuracy: 0.3535\n",
            "Epoch 150/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5131 - accuracy: 0.3386\n",
            "Epoch 150: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5161 - accuracy: 0.3359 - val_loss: 1.5100 - val_accuracy: 0.3535\n",
            "Epoch 151/600\n",
            "66/76 [=========================>....] - ETA: 0s - loss: 1.5141 - accuracy: 0.3447\n",
            "Epoch 151: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 1.5133 - accuracy: 0.3437 - val_loss: 1.5251 - val_accuracy: 0.3535\n",
            "Epoch 152/600\n",
            "67/76 [=========================>....] - ETA: 0s - loss: 1.5179 - accuracy: 0.3359\n",
            "Epoch 152: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 1.5175 - accuracy: 0.3335 - val_loss: 1.5154 - val_accuracy: 0.3535\n",
            "Epoch 153/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5205 - accuracy: 0.3332\n",
            "Epoch 153: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5214 - accuracy: 0.3331 - val_loss: 1.5136 - val_accuracy: 0.3535\n",
            "Epoch 154/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5131 - accuracy: 0.3398\n",
            "Epoch 154: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5138 - accuracy: 0.3398 - val_loss: 1.5139 - val_accuracy: 0.3535\n",
            "Epoch 155/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5164 - accuracy: 0.3366\n",
            "Epoch 155: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5154 - accuracy: 0.3373 - val_loss: 1.5119 - val_accuracy: 0.3535\n",
            "Epoch 156/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5147 - accuracy: 0.3399\n",
            "Epoch 156: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5153 - accuracy: 0.3395 - val_loss: 1.5161 - val_accuracy: 0.3535\n",
            "Epoch 157/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5143 - accuracy: 0.3405\n",
            "Epoch 157: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5149 - accuracy: 0.3399 - val_loss: 1.5115 - val_accuracy: 0.3535\n",
            "Epoch 158/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5151 - accuracy: 0.3390\n",
            "Epoch 158: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5153 - accuracy: 0.3390 - val_loss: 1.5116 - val_accuracy: 0.3535\n",
            "Epoch 159/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5141 - accuracy: 0.3391\n",
            "Epoch 159: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5145 - accuracy: 0.3390 - val_loss: 1.5209 - val_accuracy: 0.3535\n",
            "Epoch 160/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5145 - accuracy: 0.3431\n",
            "Epoch 160: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5152 - accuracy: 0.3421 - val_loss: 1.5104 - val_accuracy: 0.3535\n",
            "Epoch 161/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5171 - accuracy: 0.3378\n",
            "Epoch 161: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5164 - accuracy: 0.3394 - val_loss: 1.5152 - val_accuracy: 0.3535\n",
            "Epoch 162/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5142 - accuracy: 0.3355\n",
            "Epoch 162: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5139 - accuracy: 0.3353 - val_loss: 1.5192 - val_accuracy: 0.3535\n",
            "Epoch 163/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5131 - accuracy: 0.3363\n",
            "Epoch 163: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5131 - accuracy: 0.3370 - val_loss: 1.5133 - val_accuracy: 0.3535\n",
            "Epoch 164/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5190 - accuracy: 0.3332\n",
            "Epoch 164: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5167 - accuracy: 0.3352 - val_loss: 1.5159 - val_accuracy: 0.3535\n",
            "Epoch 165/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5147 - accuracy: 0.3390\n",
            "Epoch 165: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5132 - accuracy: 0.3415 - val_loss: 1.5121 - val_accuracy: 0.3535\n",
            "Epoch 166/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5121 - accuracy: 0.3399\n",
            "Epoch 166: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5129 - accuracy: 0.3396 - val_loss: 1.5110 - val_accuracy: 0.3535\n",
            "Epoch 167/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5192 - accuracy: 0.3326\n",
            "Epoch 167: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5186 - accuracy: 0.3343 - val_loss: 1.5108 - val_accuracy: 0.3535\n",
            "Epoch 168/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5131 - accuracy: 0.3383\n",
            "Epoch 168: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5132 - accuracy: 0.3387 - val_loss: 1.5139 - val_accuracy: 0.3535\n",
            "Epoch 169/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5136 - accuracy: 0.3388\n",
            "Epoch 169: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5135 - accuracy: 0.3391 - val_loss: 1.5144 - val_accuracy: 0.3535\n",
            "Epoch 170/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5175 - accuracy: 0.3399\n",
            "Epoch 170: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5160 - accuracy: 0.3408 - val_loss: 1.5143 - val_accuracy: 0.3535\n",
            "Epoch 171/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5148 - accuracy: 0.3349\n",
            "Epoch 171: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5148 - accuracy: 0.3349 - val_loss: 1.5174 - val_accuracy: 0.3535\n",
            "Epoch 172/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5120 - accuracy: 0.3392\n",
            "Epoch 172: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5111 - accuracy: 0.3392 - val_loss: 1.5175 - val_accuracy: 0.3535\n",
            "Epoch 173/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5141 - accuracy: 0.3401\n",
            "Epoch 173: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5143 - accuracy: 0.3406 - val_loss: 1.5092 - val_accuracy: 0.3535\n",
            "Epoch 174/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5143 - accuracy: 0.3382\n",
            "Epoch 174: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5144 - accuracy: 0.3389 - val_loss: 1.5108 - val_accuracy: 0.3535\n",
            "Epoch 175/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5151 - accuracy: 0.3301\n",
            "Epoch 175: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5150 - accuracy: 0.3307 - val_loss: 1.5169 - val_accuracy: 0.3535\n",
            "Epoch 176/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5178 - accuracy: 0.3361\n",
            "Epoch 176: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5185 - accuracy: 0.3356 - val_loss: 1.5090 - val_accuracy: 0.3535\n",
            "Epoch 177/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5131 - accuracy: 0.3465\n",
            "Epoch 177: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5125 - accuracy: 0.3467 - val_loss: 1.5112 - val_accuracy: 0.3535\n",
            "Epoch 178/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5163 - accuracy: 0.3333\n",
            "Epoch 178: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5162 - accuracy: 0.3336 - val_loss: 1.5183 - val_accuracy: 0.3535\n",
            "Epoch 179/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5101 - accuracy: 0.3456\n",
            "Epoch 179: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5096 - accuracy: 0.3459 - val_loss: 1.5075 - val_accuracy: 0.3535\n",
            "Epoch 180/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.4823 - accuracy: 0.3720\n",
            "Epoch 180: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4805 - accuracy: 0.3727 - val_loss: 1.3932 - val_accuracy: 0.4351\n",
            "Epoch 181/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4978 - accuracy: 0.3643\n",
            "Epoch 181: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4923 - accuracy: 0.3661 - val_loss: 1.4710 - val_accuracy: 0.3599\n",
            "Epoch 182/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5023 - accuracy: 0.3473\n",
            "Epoch 182: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5026 - accuracy: 0.3466 - val_loss: 1.3945 - val_accuracy: 0.3623\n",
            "Epoch 183/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4576 - accuracy: 0.3626\n",
            "Epoch 183: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4621 - accuracy: 0.3607 - val_loss: 1.5093 - val_accuracy: 0.3535\n",
            "Epoch 184/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5100 - accuracy: 0.3430\n",
            "Epoch 184: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5104 - accuracy: 0.3424 - val_loss: 1.4842 - val_accuracy: 0.3535\n",
            "Epoch 185/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.4333 - accuracy: 0.4018\n",
            "Epoch 185: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 1.4348 - accuracy: 0.4004 - val_loss: 1.4324 - val_accuracy: 0.3599\n",
            "Epoch 186/600\n",
            "68/76 [=========================>....] - ETA: 0s - loss: 1.4202 - accuracy: 0.3995\n",
            "Epoch 186: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 1.4198 - accuracy: 0.3964 - val_loss: 1.4440 - val_accuracy: 0.3599\n",
            "Epoch 187/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5436 - accuracy: 0.3295\n",
            "Epoch 187: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5430 - accuracy: 0.3282 - val_loss: 1.5122 - val_accuracy: 0.3535\n",
            "Epoch 188/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5127 - accuracy: 0.3398\n",
            "Epoch 188: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5117 - accuracy: 0.3408 - val_loss: 1.5076 - val_accuracy: 0.3535\n",
            "Epoch 189/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4757 - accuracy: 0.3759\n",
            "Epoch 189: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4752 - accuracy: 0.3773 - val_loss: 1.5701 - val_accuracy: 0.4312\n",
            "Epoch 190/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.4625 - accuracy: 0.3806\n",
            "Epoch 190: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4660 - accuracy: 0.3784 - val_loss: 1.5295 - val_accuracy: 0.3535\n",
            "Epoch 191/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5026 - accuracy: 0.3595\n",
            "Epoch 191: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5066 - accuracy: 0.3600 - val_loss: 1.4215 - val_accuracy: 0.3623\n",
            "Epoch 192/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5436 - accuracy: 0.3270\n",
            "Epoch 192: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5433 - accuracy: 0.3265 - val_loss: 1.5166 - val_accuracy: 0.3535\n",
            "Epoch 193/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5159 - accuracy: 0.3386\n",
            "Epoch 193: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5163 - accuracy: 0.3394 - val_loss: 1.5162 - val_accuracy: 0.3535\n",
            "Epoch 194/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4933 - accuracy: 0.3690\n",
            "Epoch 194: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4899 - accuracy: 0.3719 - val_loss: 1.4305 - val_accuracy: 0.3662\n",
            "Epoch 195/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4683 - accuracy: 0.3690\n",
            "Epoch 195: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4695 - accuracy: 0.3689 - val_loss: 1.5146 - val_accuracy: 0.3535\n",
            "Epoch 196/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5127 - accuracy: 0.3437\n",
            "Epoch 196: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5117 - accuracy: 0.3444 - val_loss: 1.5230 - val_accuracy: 0.3535\n",
            "Epoch 197/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5219 - accuracy: 0.3316\n",
            "Epoch 197: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5219 - accuracy: 0.3316 - val_loss: 1.5196 - val_accuracy: 0.3535\n",
            "Epoch 198/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5192 - accuracy: 0.3329\n",
            "Epoch 198: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5194 - accuracy: 0.3318 - val_loss: 1.5202 - val_accuracy: 0.3535\n",
            "Epoch 199/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5218 - accuracy: 0.3354\n",
            "Epoch 199: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5227 - accuracy: 0.3347 - val_loss: 1.5096 - val_accuracy: 0.3535\n",
            "Epoch 200/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5159 - accuracy: 0.3419\n",
            "Epoch 200: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5147 - accuracy: 0.3417 - val_loss: 1.5146 - val_accuracy: 0.3535\n",
            "Epoch 201/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5139 - accuracy: 0.3373\n",
            "Epoch 201: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5138 - accuracy: 0.3379 - val_loss: 1.5182 - val_accuracy: 0.3535\n",
            "Epoch 202/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5188 - accuracy: 0.3338\n",
            "Epoch 202: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5180 - accuracy: 0.3353 - val_loss: 1.5098 - val_accuracy: 0.3535\n",
            "Epoch 203/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5107 - accuracy: 0.3369\n",
            "Epoch 203: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5102 - accuracy: 0.3370 - val_loss: 1.5120 - val_accuracy: 0.3535\n",
            "Epoch 204/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5167 - accuracy: 0.3393\n",
            "Epoch 204: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5161 - accuracy: 0.3401 - val_loss: 1.5245 - val_accuracy: 0.3545\n",
            "Epoch 205/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5183 - accuracy: 0.3343\n",
            "Epoch 205: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5179 - accuracy: 0.3347 - val_loss: 1.5095 - val_accuracy: 0.3535\n",
            "Epoch 206/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4754 - accuracy: 0.3789\n",
            "Epoch 206: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4817 - accuracy: 0.3784 - val_loss: 1.6960 - val_accuracy: 0.2837\n",
            "Epoch 207/600\n",
            "76/76 [==============================] - ETA: 0s - loss: 1.5291 - accuracy: 0.3255\n",
            "Epoch 207: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 1s 7ms/step - loss: 1.5291 - accuracy: 0.3255 - val_loss: 1.5071 - val_accuracy: 0.3604\n",
            "Epoch 208/600\n",
            "66/76 [=========================>....] - ETA: 0s - loss: 1.4644 - accuracy: 0.3771\n",
            "Epoch 208: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4692 - accuracy: 0.3714 - val_loss: 1.4882 - val_accuracy: 0.3633\n",
            "Epoch 209/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4957 - accuracy: 0.3614\n",
            "Epoch 209: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4964 - accuracy: 0.3608 - val_loss: 1.5146 - val_accuracy: 0.3535\n",
            "Epoch 210/600\n",
            "75/76 [============================>.] - ETA: 0s - loss: 1.4687 - accuracy: 0.3699\n",
            "Epoch 210: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4703 - accuracy: 0.3684 - val_loss: 1.5542 - val_accuracy: 0.3491\n",
            "Epoch 211/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5283 - accuracy: 0.3333\n",
            "Epoch 211: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5266 - accuracy: 0.3300 - val_loss: 1.5221 - val_accuracy: 0.3535\n",
            "Epoch 212/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5265 - accuracy: 0.3352\n",
            "Epoch 212: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5266 - accuracy: 0.3349 - val_loss: 1.5165 - val_accuracy: 0.3535\n",
            "Epoch 213/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5192 - accuracy: 0.3337\n",
            "Epoch 213: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5195 - accuracy: 0.3326 - val_loss: 1.5291 - val_accuracy: 0.2334\n",
            "Epoch 214/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5167 - accuracy: 0.3303\n",
            "Epoch 214: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5151 - accuracy: 0.3318 - val_loss: 1.4938 - val_accuracy: 0.3535\n",
            "Epoch 215/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5194 - accuracy: 0.3348\n",
            "Epoch 215: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5182 - accuracy: 0.3365 - val_loss: 1.5221 - val_accuracy: 0.3491\n",
            "Epoch 216/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5207 - accuracy: 0.3318\n",
            "Epoch 216: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5204 - accuracy: 0.3318 - val_loss: 1.5112 - val_accuracy: 0.3535\n",
            "Epoch 217/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5189 - accuracy: 0.3412\n",
            "Epoch 217: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5194 - accuracy: 0.3395 - val_loss: 1.5251 - val_accuracy: 0.2563\n",
            "Epoch 218/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5247 - accuracy: 0.3246\n",
            "Epoch 218: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5247 - accuracy: 0.3259 - val_loss: 1.5385 - val_accuracy: 0.3608\n",
            "Epoch 219/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.4932 - accuracy: 0.3707\n",
            "Epoch 219: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4851 - accuracy: 0.3724 - val_loss: 1.3683 - val_accuracy: 0.4463\n",
            "Epoch 220/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4705 - accuracy: 0.3836\n",
            "Epoch 220: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4796 - accuracy: 0.3799 - val_loss: 1.8008 - val_accuracy: 0.1768\n",
            "Epoch 221/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5482 - accuracy: 0.3224\n",
            "Epoch 221: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5469 - accuracy: 0.3228 - val_loss: 1.5141 - val_accuracy: 0.3535\n",
            "Epoch 222/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5146 - accuracy: 0.3383\n",
            "Epoch 222: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5130 - accuracy: 0.3395 - val_loss: 1.5085 - val_accuracy: 0.3535\n",
            "Epoch 223/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5165 - accuracy: 0.3365\n",
            "Epoch 223: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5161 - accuracy: 0.3378 - val_loss: 1.5153 - val_accuracy: 0.3535\n",
            "Epoch 224/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4940 - accuracy: 0.3625\n",
            "Epoch 224: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4902 - accuracy: 0.3669 - val_loss: 1.3757 - val_accuracy: 0.3657\n",
            "Epoch 225/600\n",
            "68/76 [=========================>....] - ETA: 0s - loss: 1.5182 - accuracy: 0.3406\n",
            "Epoch 225: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5209 - accuracy: 0.3461 - val_loss: 1.5898 - val_accuracy: 0.2563\n",
            "Epoch 226/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4230 - accuracy: 0.3935\n",
            "Epoch 226: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4226 - accuracy: 0.3939 - val_loss: 1.5680 - val_accuracy: 0.3599\n",
            "Epoch 227/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.4967 - accuracy: 0.3449\n",
            "Epoch 227: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4983 - accuracy: 0.3449 - val_loss: 1.5312 - val_accuracy: 0.3491\n",
            "Epoch 228/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5264 - accuracy: 0.3278\n",
            "Epoch 228: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5257 - accuracy: 0.3286 - val_loss: 1.5167 - val_accuracy: 0.3491\n",
            "Epoch 229/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5205 - accuracy: 0.3341\n",
            "Epoch 229: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5203 - accuracy: 0.3346 - val_loss: 1.5223 - val_accuracy: 0.3491\n",
            "Epoch 230/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5197 - accuracy: 0.3347\n",
            "Epoch 230: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5203 - accuracy: 0.3344 - val_loss: 1.5215 - val_accuracy: 0.3491\n",
            "Epoch 231/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5182 - accuracy: 0.3378\n",
            "Epoch 231: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5198 - accuracy: 0.3371 - val_loss: 1.5173 - val_accuracy: 0.3491\n",
            "Epoch 232/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5190 - accuracy: 0.3388\n",
            "Epoch 232: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5184 - accuracy: 0.3375 - val_loss: 1.5258 - val_accuracy: 0.3633\n",
            "Epoch 233/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.4983 - accuracy: 0.3521\n",
            "Epoch 233: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4988 - accuracy: 0.3491 - val_loss: 1.4550 - val_accuracy: 0.3638\n",
            "Epoch 234/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5031 - accuracy: 0.3468\n",
            "Epoch 234: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5033 - accuracy: 0.3475 - val_loss: 1.5196 - val_accuracy: 0.3491\n",
            "Epoch 235/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5095 - accuracy: 0.3426\n",
            "Epoch 235: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5044 - accuracy: 0.3459 - val_loss: 1.4734 - val_accuracy: 0.3604\n",
            "Epoch 236/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4999 - accuracy: 0.3327\n",
            "Epoch 236: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5000 - accuracy: 0.3348 - val_loss: 1.5169 - val_accuracy: 0.3491\n",
            "Epoch 237/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5205 - accuracy: 0.3339\n",
            "Epoch 237: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5222 - accuracy: 0.3327 - val_loss: 1.5306 - val_accuracy: 0.3535\n",
            "Epoch 238/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5250 - accuracy: 0.3264\n",
            "Epoch 238: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5243 - accuracy: 0.3269 - val_loss: 1.5166 - val_accuracy: 0.3491\n",
            "Epoch 239/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5206 - accuracy: 0.3374\n",
            "Epoch 239: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5208 - accuracy: 0.3351 - val_loss: 1.5213 - val_accuracy: 0.3530\n",
            "Epoch 240/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5214 - accuracy: 0.3313\n",
            "Epoch 240: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5210 - accuracy: 0.3320 - val_loss: 1.5194 - val_accuracy: 0.3496\n",
            "Epoch 241/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5178 - accuracy: 0.3325\n",
            "Epoch 241: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5184 - accuracy: 0.3300 - val_loss: 1.5156 - val_accuracy: 0.3535\n",
            "Epoch 242/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5254 - accuracy: 0.3259\n",
            "Epoch 242: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5254 - accuracy: 0.3271 - val_loss: 1.5115 - val_accuracy: 0.3535\n",
            "Epoch 243/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5219 - accuracy: 0.3311\n",
            "Epoch 243: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5199 - accuracy: 0.3339 - val_loss: 1.5251 - val_accuracy: 0.3535\n",
            "Epoch 244/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5151 - accuracy: 0.3454\n",
            "Epoch 244: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5160 - accuracy: 0.3434 - val_loss: 1.5262 - val_accuracy: 0.3594\n",
            "Epoch 245/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5185 - accuracy: 0.3349\n",
            "Epoch 245: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5188 - accuracy: 0.3351 - val_loss: 1.5222 - val_accuracy: 0.3535\n",
            "Epoch 246/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5194 - accuracy: 0.3380\n",
            "Epoch 246: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5190 - accuracy: 0.3359 - val_loss: 1.5195 - val_accuracy: 0.2891\n",
            "Epoch 247/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5260 - accuracy: 0.3471\n",
            "Epoch 247: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5262 - accuracy: 0.3464 - val_loss: 1.5130 - val_accuracy: 0.3535\n",
            "Epoch 248/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.4908 - accuracy: 0.3574\n",
            "Epoch 248: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4861 - accuracy: 0.3607 - val_loss: 1.3929 - val_accuracy: 0.3652\n",
            "Epoch 249/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5193 - accuracy: 0.3287\n",
            "Epoch 249: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5177 - accuracy: 0.3309 - val_loss: 1.5180 - val_accuracy: 0.3491\n",
            "Epoch 250/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5202 - accuracy: 0.3345\n",
            "Epoch 250: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5200 - accuracy: 0.3350 - val_loss: 1.5151 - val_accuracy: 0.3491\n",
            "Epoch 251/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5190 - accuracy: 0.3331\n",
            "Epoch 251: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5172 - accuracy: 0.3311 - val_loss: 1.5197 - val_accuracy: 0.3491\n",
            "Epoch 252/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5266 - accuracy: 0.3304\n",
            "Epoch 252: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5277 - accuracy: 0.3303 - val_loss: 1.5210 - val_accuracy: 0.3535\n",
            "Epoch 253/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5229 - accuracy: 0.3394\n",
            "Epoch 253: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5221 - accuracy: 0.3413 - val_loss: 1.5137 - val_accuracy: 0.3491\n",
            "Epoch 254/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5240 - accuracy: 0.3201\n",
            "Epoch 254: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5240 - accuracy: 0.3204 - val_loss: 1.5168 - val_accuracy: 0.3535\n",
            "Epoch 255/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5152 - accuracy: 0.3344\n",
            "Epoch 255: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5161 - accuracy: 0.3303 - val_loss: 1.5325 - val_accuracy: 0.2334\n",
            "Epoch 256/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5232 - accuracy: 0.3380\n",
            "Epoch 256: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5227 - accuracy: 0.3377 - val_loss: 1.5121 - val_accuracy: 0.3535\n",
            "Epoch 257/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5244 - accuracy: 0.3320\n",
            "Epoch 257: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5235 - accuracy: 0.3334 - val_loss: 1.5143 - val_accuracy: 0.3535\n",
            "Epoch 258/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5156 - accuracy: 0.3325\n",
            "Epoch 258: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5158 - accuracy: 0.3319 - val_loss: 1.5134 - val_accuracy: 0.3535\n",
            "Epoch 259/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5187 - accuracy: 0.3365\n",
            "Epoch 259: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5201 - accuracy: 0.3348 - val_loss: 1.5185 - val_accuracy: 0.3599\n",
            "Epoch 260/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5170 - accuracy: 0.3495\n",
            "Epoch 260: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5170 - accuracy: 0.3494 - val_loss: 1.5218 - val_accuracy: 0.3491\n",
            "Epoch 261/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5172 - accuracy: 0.3341\n",
            "Epoch 261: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3322 - val_loss: 1.5318 - val_accuracy: 0.2334\n",
            "Epoch 262/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5251 - accuracy: 0.3310\n",
            "Epoch 262: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5240 - accuracy: 0.3315 - val_loss: 1.5210 - val_accuracy: 0.3491\n",
            "Epoch 263/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5235 - accuracy: 0.3285\n",
            "Epoch 263: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5221 - accuracy: 0.3298 - val_loss: 1.5188 - val_accuracy: 0.3491\n",
            "Epoch 264/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5221 - accuracy: 0.3254\n",
            "Epoch 264: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5228 - accuracy: 0.3269 - val_loss: 1.5278 - val_accuracy: 0.3491\n",
            "Epoch 265/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5235 - accuracy: 0.3310\n",
            "Epoch 265: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5231 - accuracy: 0.3311 - val_loss: 1.5113 - val_accuracy: 0.3491\n",
            "Epoch 266/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4814 - accuracy: 0.3638\n",
            "Epoch 266: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4760 - accuracy: 0.3684 - val_loss: 1.3136 - val_accuracy: 0.4595\n",
            "Epoch 267/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5174 - accuracy: 0.3242\n",
            "Epoch 267: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5180 - accuracy: 0.3255 - val_loss: 1.5185 - val_accuracy: 0.3491\n",
            "Epoch 268/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5273 - accuracy: 0.3254\n",
            "Epoch 268: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5286 - accuracy: 0.3232 - val_loss: 1.5107 - val_accuracy: 0.3491\n",
            "Epoch 269/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.4652 - accuracy: 0.3806\n",
            "Epoch 269: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4672 - accuracy: 0.3786 - val_loss: 1.4309 - val_accuracy: 0.3491\n",
            "Epoch 270/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5486 - accuracy: 0.3197\n",
            "Epoch 270: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5460 - accuracy: 0.3217 - val_loss: 1.5184 - val_accuracy: 0.3491\n",
            "Epoch 271/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5212 - accuracy: 0.3403\n",
            "Epoch 271: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5200 - accuracy: 0.3403 - val_loss: 1.5227 - val_accuracy: 0.3491\n",
            "Epoch 272/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5237 - accuracy: 0.3316\n",
            "Epoch 272: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5238 - accuracy: 0.3325 - val_loss: 1.5187 - val_accuracy: 0.3491\n",
            "Epoch 273/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5173 - accuracy: 0.3341\n",
            "Epoch 273: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5169 - accuracy: 0.3349 - val_loss: 1.5179 - val_accuracy: 0.3491\n",
            "Epoch 274/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4583 - accuracy: 0.3558\n",
            "Epoch 274: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4617 - accuracy: 0.3552 - val_loss: 1.5243 - val_accuracy: 0.3491\n",
            "Epoch 275/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5235 - accuracy: 0.3244\n",
            "Epoch 275: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5238 - accuracy: 0.3240 - val_loss: 1.5168 - val_accuracy: 0.3491\n",
            "Epoch 276/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5197 - accuracy: 0.3292\n",
            "Epoch 276: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5183 - accuracy: 0.3305 - val_loss: 1.5206 - val_accuracy: 0.3491\n",
            "Epoch 277/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5176 - accuracy: 0.3336\n",
            "Epoch 277: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5188 - accuracy: 0.3326 - val_loss: 1.5185 - val_accuracy: 0.3491\n",
            "Epoch 278/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5237 - accuracy: 0.3338\n",
            "Epoch 278: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5245 - accuracy: 0.3342 - val_loss: 1.5272 - val_accuracy: 0.3491\n",
            "Epoch 279/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5192 - accuracy: 0.3357\n",
            "Epoch 279: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5206 - accuracy: 0.3331 - val_loss: 1.5352 - val_accuracy: 0.2334\n",
            "Epoch 280/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5258 - accuracy: 0.3187\n",
            "Epoch 280: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5268 - accuracy: 0.3189 - val_loss: 1.5191 - val_accuracy: 0.3491\n",
            "Epoch 281/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5146 - accuracy: 0.3430\n",
            "Epoch 281: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5148 - accuracy: 0.3432 - val_loss: 1.5208 - val_accuracy: 0.3491\n",
            "Epoch 282/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5223 - accuracy: 0.3230\n",
            "Epoch 282: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5234 - accuracy: 0.3226 - val_loss: 1.5157 - val_accuracy: 0.3491\n",
            "Epoch 283/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5179 - accuracy: 0.3407\n",
            "Epoch 283: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5160 - accuracy: 0.3422 - val_loss: 1.5188 - val_accuracy: 0.3491\n",
            "Epoch 284/600\n",
            "68/76 [=========================>....] - ETA: 0s - loss: 1.5222 - accuracy: 0.3319\n",
            "Epoch 284: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5209 - accuracy: 0.3338 - val_loss: 1.5204 - val_accuracy: 0.3491\n",
            "Epoch 285/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5215 - accuracy: 0.3341\n",
            "Epoch 285: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5211 - accuracy: 0.3336 - val_loss: 1.5155 - val_accuracy: 0.3491\n",
            "Epoch 286/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5185 - accuracy: 0.3324\n",
            "Epoch 286: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5184 - accuracy: 0.3332 - val_loss: 1.5147 - val_accuracy: 0.3491\n",
            "Epoch 287/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5179 - accuracy: 0.3396\n",
            "Epoch 287: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5187 - accuracy: 0.3390 - val_loss: 1.5138 - val_accuracy: 0.3491\n",
            "Epoch 288/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5165 - accuracy: 0.3325\n",
            "Epoch 288: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5154 - accuracy: 0.3330 - val_loss: 1.5144 - val_accuracy: 0.3491\n",
            "Epoch 289/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5267 - accuracy: 0.3261\n",
            "Epoch 289: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5254 - accuracy: 0.3265 - val_loss: 1.5258 - val_accuracy: 0.3491\n",
            "Epoch 290/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5230 - accuracy: 0.3236\n",
            "Epoch 290: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5216 - accuracy: 0.3248 - val_loss: 1.5154 - val_accuracy: 0.3491\n",
            "Epoch 291/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5211 - accuracy: 0.3368\n",
            "Epoch 291: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5214 - accuracy: 0.3363 - val_loss: 1.5165 - val_accuracy: 0.3491\n",
            "Epoch 292/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5225 - accuracy: 0.3373\n",
            "Epoch 292: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5211 - accuracy: 0.3379 - val_loss: 1.5144 - val_accuracy: 0.3491\n",
            "Epoch 293/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5153 - accuracy: 0.3340\n",
            "Epoch 293: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5150 - accuracy: 0.3338 - val_loss: 1.5098 - val_accuracy: 0.3491\n",
            "Epoch 294/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.4913 - accuracy: 0.3508\n",
            "Epoch 294: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4940 - accuracy: 0.3495 - val_loss: 1.4463 - val_accuracy: 0.3501\n",
            "Epoch 295/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.4716 - accuracy: 0.3574\n",
            "Epoch 295: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4653 - accuracy: 0.3592 - val_loss: 1.3677 - val_accuracy: 0.3623\n",
            "Epoch 296/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5053 - accuracy: 0.3446\n",
            "Epoch 296: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5066 - accuracy: 0.3428 - val_loss: 1.5169 - val_accuracy: 0.3491\n",
            "Epoch 297/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5186 - accuracy: 0.3353\n",
            "Epoch 297: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5173 - accuracy: 0.3358 - val_loss: 1.5266 - val_accuracy: 0.3506\n",
            "Epoch 298/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5215 - accuracy: 0.3304\n",
            "Epoch 298: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5210 - accuracy: 0.3310 - val_loss: 1.5179 - val_accuracy: 0.3491\n",
            "Epoch 299/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5197 - accuracy: 0.3366\n",
            "Epoch 299: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5174 - accuracy: 0.3388 - val_loss: 1.5226 - val_accuracy: 0.3491\n",
            "Epoch 300/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5276 - accuracy: 0.3242\n",
            "Epoch 300: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5257 - accuracy: 0.3256 - val_loss: 1.5138 - val_accuracy: 0.3491\n",
            "Epoch 301/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5209 - accuracy: 0.3307\n",
            "Epoch 301: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5191 - accuracy: 0.3327 - val_loss: 1.5199 - val_accuracy: 0.3491\n",
            "Epoch 302/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5196 - accuracy: 0.3341\n",
            "Epoch 302: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5205 - accuracy: 0.3351 - val_loss: 1.5205 - val_accuracy: 0.3491\n",
            "Epoch 303/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5219 - accuracy: 0.3343\n",
            "Epoch 303: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5232 - accuracy: 0.3336 - val_loss: 1.5283 - val_accuracy: 0.3491\n",
            "Epoch 304/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5180 - accuracy: 0.3385\n",
            "Epoch 304: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5183 - accuracy: 0.3387 - val_loss: 1.5204 - val_accuracy: 0.3491\n",
            "Epoch 305/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5176 - accuracy: 0.3359\n",
            "Epoch 305: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5184 - accuracy: 0.3348 - val_loss: 1.5148 - val_accuracy: 0.3491\n",
            "Epoch 306/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5155 - accuracy: 0.3379\n",
            "Epoch 306: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5139 - accuracy: 0.3386 - val_loss: 1.5015 - val_accuracy: 0.3535\n",
            "Epoch 307/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.4890 - accuracy: 0.3554\n",
            "Epoch 307: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4904 - accuracy: 0.3550 - val_loss: 1.5172 - val_accuracy: 0.3491\n",
            "Epoch 308/600\n",
            "68/76 [=========================>....] - ETA: 0s - loss: 1.5201 - accuracy: 0.3341\n",
            "Epoch 308: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5196 - accuracy: 0.3344 - val_loss: 1.5170 - val_accuracy: 0.3491\n",
            "Epoch 309/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5208 - accuracy: 0.3312\n",
            "Epoch 309: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5201 - accuracy: 0.3319 - val_loss: 1.5142 - val_accuracy: 0.3491\n",
            "Epoch 310/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5232 - accuracy: 0.3344\n",
            "Epoch 310: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5215 - accuracy: 0.3358 - val_loss: 1.5197 - val_accuracy: 0.3491\n",
            "Epoch 311/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5199 - accuracy: 0.3317\n",
            "Epoch 311: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5192 - accuracy: 0.3325 - val_loss: 1.5203 - val_accuracy: 0.3491\n",
            "Epoch 312/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5163 - accuracy: 0.3375\n",
            "Epoch 312: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5160 - accuracy: 0.3382 - val_loss: 1.5164 - val_accuracy: 0.3491\n",
            "Epoch 313/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5135 - accuracy: 0.3356\n",
            "Epoch 313: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5121 - accuracy: 0.3361 - val_loss: 1.4779 - val_accuracy: 0.3535\n",
            "Epoch 314/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.4802 - accuracy: 0.3512\n",
            "Epoch 314: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4828 - accuracy: 0.3498 - val_loss: 1.5147 - val_accuracy: 0.3491\n",
            "Epoch 315/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5247 - accuracy: 0.3291\n",
            "Epoch 315: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5238 - accuracy: 0.3305 - val_loss: 1.5223 - val_accuracy: 0.3491\n",
            "Epoch 316/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5231 - accuracy: 0.3293\n",
            "Epoch 316: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5227 - accuracy: 0.3298 - val_loss: 1.5156 - val_accuracy: 0.3491\n",
            "Epoch 317/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5230 - accuracy: 0.3323\n",
            "Epoch 317: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5225 - accuracy: 0.3317 - val_loss: 1.5509 - val_accuracy: 0.2334\n",
            "Epoch 318/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5204 - accuracy: 0.3318\n",
            "Epoch 318: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5199 - accuracy: 0.3318 - val_loss: 1.5213 - val_accuracy: 0.3491\n",
            "Epoch 319/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5228 - accuracy: 0.3344\n",
            "Epoch 319: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5237 - accuracy: 0.3343 - val_loss: 1.5154 - val_accuracy: 0.3491\n",
            "Epoch 320/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5206 - accuracy: 0.3290\n",
            "Epoch 320: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5201 - accuracy: 0.3309 - val_loss: 1.5165 - val_accuracy: 0.3491\n",
            "Epoch 321/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5201 - accuracy: 0.3374\n",
            "Epoch 321: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5204 - accuracy: 0.3379 - val_loss: 1.5225 - val_accuracy: 0.3491\n",
            "Epoch 322/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5228 - accuracy: 0.3351\n",
            "Epoch 322: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5225 - accuracy: 0.3346 - val_loss: 1.5206 - val_accuracy: 0.3491\n",
            "Epoch 323/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5186 - accuracy: 0.3354\n",
            "Epoch 323: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5175 - accuracy: 0.3351 - val_loss: 1.5190 - val_accuracy: 0.3491\n",
            "Epoch 324/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5203 - accuracy: 0.3326\n",
            "Epoch 324: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5204 - accuracy: 0.3325 - val_loss: 1.5152 - val_accuracy: 0.3491\n",
            "Epoch 325/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5226 - accuracy: 0.3206\n",
            "Epoch 325: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5221 - accuracy: 0.3216 - val_loss: 1.5181 - val_accuracy: 0.3491\n",
            "Epoch 326/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5226 - accuracy: 0.3304\n",
            "Epoch 326: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5220 - accuracy: 0.3311 - val_loss: 1.5150 - val_accuracy: 0.3491\n",
            "Epoch 327/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5119 - accuracy: 0.3409\n",
            "Epoch 327: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5116 - accuracy: 0.3410 - val_loss: 1.5012 - val_accuracy: 0.3491\n",
            "Epoch 328/600\n",
            "75/76 [============================>.] - ETA: 0s - loss: 1.4916 - accuracy: 0.3536\n",
            "Epoch 328: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4902 - accuracy: 0.3539 - val_loss: 1.4039 - val_accuracy: 0.3608\n",
            "Epoch 329/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5108 - accuracy: 0.3414\n",
            "Epoch 329: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5136 - accuracy: 0.3380 - val_loss: 1.5342 - val_accuracy: 0.2334\n",
            "Epoch 330/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5252 - accuracy: 0.3187\n",
            "Epoch 330: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5232 - accuracy: 0.3214 - val_loss: 1.5216 - val_accuracy: 0.3491\n",
            "Epoch 331/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5172 - accuracy: 0.3418\n",
            "Epoch 331: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5169 - accuracy: 0.3420 - val_loss: 1.5205 - val_accuracy: 0.3491\n",
            "Epoch 332/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5219 - accuracy: 0.3313\n",
            "Epoch 332: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5205 - accuracy: 0.3330 - val_loss: 1.5185 - val_accuracy: 0.3491\n",
            "Epoch 333/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5207 - accuracy: 0.3382\n",
            "Epoch 333: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5198 - accuracy: 0.3386 - val_loss: 1.5161 - val_accuracy: 0.3491\n",
            "Epoch 334/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5157 - accuracy: 0.3376\n",
            "Epoch 334: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5154 - accuracy: 0.3383 - val_loss: 1.5222 - val_accuracy: 0.3491\n",
            "Epoch 335/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5268 - accuracy: 0.3272\n",
            "Epoch 335: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5256 - accuracy: 0.3290 - val_loss: 1.5188 - val_accuracy: 0.3491\n",
            "Epoch 336/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5210 - accuracy: 0.3293\n",
            "Epoch 336: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5210 - accuracy: 0.3305 - val_loss: 1.5214 - val_accuracy: 0.3491\n",
            "Epoch 337/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5188 - accuracy: 0.3363\n",
            "Epoch 337: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5196 - accuracy: 0.3358 - val_loss: 1.5189 - val_accuracy: 0.3491\n",
            "Epoch 338/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5207 - accuracy: 0.3396\n",
            "Epoch 338: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5197 - accuracy: 0.3401 - val_loss: 1.5163 - val_accuracy: 0.3491\n",
            "Epoch 339/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5217 - accuracy: 0.3334\n",
            "Epoch 339: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5213 - accuracy: 0.3326 - val_loss: 1.5256 - val_accuracy: 0.3491\n",
            "Epoch 340/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5254 - accuracy: 0.3302\n",
            "Epoch 340: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5263 - accuracy: 0.3304 - val_loss: 1.5162 - val_accuracy: 0.3491\n",
            "Epoch 341/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5171 - accuracy: 0.3393\n",
            "Epoch 341: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5171 - accuracy: 0.3399 - val_loss: 1.5182 - val_accuracy: 0.3491\n",
            "Epoch 342/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5161 - accuracy: 0.3350\n",
            "Epoch 342: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5176 - accuracy: 0.3340 - val_loss: 1.5213 - val_accuracy: 0.3491\n",
            "Epoch 343/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5168 - accuracy: 0.3357\n",
            "Epoch 343: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5175 - accuracy: 0.3356 - val_loss: 1.5165 - val_accuracy: 0.3491\n",
            "Epoch 344/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5211 - accuracy: 0.3352\n",
            "Epoch 344: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5196 - accuracy: 0.3363 - val_loss: 1.5144 - val_accuracy: 0.3491\n",
            "Epoch 345/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5197 - accuracy: 0.3319\n",
            "Epoch 345: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5187 - accuracy: 0.3336 - val_loss: 1.5141 - val_accuracy: 0.3491\n",
            "Epoch 346/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5195 - accuracy: 0.3347\n",
            "Epoch 346: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3354 - val_loss: 1.5160 - val_accuracy: 0.3491\n",
            "Epoch 347/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5245 - accuracy: 0.3241\n",
            "Epoch 347: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5240 - accuracy: 0.3259 - val_loss: 1.5178 - val_accuracy: 0.3491\n",
            "Epoch 348/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5152 - accuracy: 0.3372\n",
            "Epoch 348: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5152 - accuracy: 0.3374 - val_loss: 1.5185 - val_accuracy: 0.3491\n",
            "Epoch 349/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5295 - accuracy: 0.3217\n",
            "Epoch 349: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5283 - accuracy: 0.3223 - val_loss: 1.5181 - val_accuracy: 0.3491\n",
            "Epoch 350/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5234 - accuracy: 0.3317\n",
            "Epoch 350: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5217 - accuracy: 0.3334 - val_loss: 1.5183 - val_accuracy: 0.3491\n",
            "Epoch 351/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5171 - accuracy: 0.3384\n",
            "Epoch 351: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5171 - accuracy: 0.3387 - val_loss: 1.5153 - val_accuracy: 0.3491\n",
            "Epoch 352/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5183 - accuracy: 0.3338\n",
            "Epoch 352: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5176 - accuracy: 0.3345 - val_loss: 1.5201 - val_accuracy: 0.3491\n",
            "Epoch 353/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5244 - accuracy: 0.3338\n",
            "Epoch 353: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5245 - accuracy: 0.3322 - val_loss: 1.5218 - val_accuracy: 0.3491\n",
            "Epoch 354/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5153 - accuracy: 0.3359\n",
            "Epoch 354: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5161 - accuracy: 0.3356 - val_loss: 1.5196 - val_accuracy: 0.3491\n",
            "Epoch 355/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5207 - accuracy: 0.3340\n",
            "Epoch 355: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5199 - accuracy: 0.3349 - val_loss: 1.5220 - val_accuracy: 0.3491\n",
            "Epoch 356/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5182 - accuracy: 0.3375\n",
            "Epoch 356: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5196 - accuracy: 0.3366 - val_loss: 1.5195 - val_accuracy: 0.3491\n",
            "Epoch 357/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5222 - accuracy: 0.3273\n",
            "Epoch 357: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5238 - accuracy: 0.3264 - val_loss: 1.5164 - val_accuracy: 0.3491\n",
            "Epoch 358/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5183 - accuracy: 0.3381\n",
            "Epoch 358: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5179 - accuracy: 0.3372 - val_loss: 1.5281 - val_accuracy: 0.3491\n",
            "Epoch 359/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5217 - accuracy: 0.3284\n",
            "Epoch 359: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5208 - accuracy: 0.3303 - val_loss: 1.5187 - val_accuracy: 0.3491\n",
            "Epoch 360/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5210 - accuracy: 0.3351\n",
            "Epoch 360: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5203 - accuracy: 0.3355 - val_loss: 1.5154 - val_accuracy: 0.3491\n",
            "Epoch 361/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5195 - accuracy: 0.3412\n",
            "Epoch 361: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5191 - accuracy: 0.3410 - val_loss: 1.5169 - val_accuracy: 0.3491\n",
            "Epoch 362/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5201 - accuracy: 0.3325\n",
            "Epoch 362: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5217 - accuracy: 0.3322 - val_loss: 1.5152 - val_accuracy: 0.3491\n",
            "Epoch 363/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5138 - accuracy: 0.3387\n",
            "Epoch 363: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5137 - accuracy: 0.3391 - val_loss: 1.5166 - val_accuracy: 0.3491\n",
            "Epoch 364/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5217 - accuracy: 0.3355\n",
            "Epoch 364: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5218 - accuracy: 0.3366 - val_loss: 1.5167 - val_accuracy: 0.3491\n",
            "Epoch 365/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5152 - accuracy: 0.3348\n",
            "Epoch 365: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5158 - accuracy: 0.3347 - val_loss: 1.5167 - val_accuracy: 0.3491\n",
            "Epoch 366/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5187 - accuracy: 0.3354\n",
            "Epoch 366: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5188 - accuracy: 0.3355 - val_loss: 1.5180 - val_accuracy: 0.3491\n",
            "Epoch 367/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5226 - accuracy: 0.3270\n",
            "Epoch 367: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5241 - accuracy: 0.3286 - val_loss: 1.5145 - val_accuracy: 0.3491\n",
            "Epoch 368/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5201 - accuracy: 0.3330\n",
            "Epoch 368: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5194 - accuracy: 0.3294 - val_loss: 1.5280 - val_accuracy: 0.2334\n",
            "Epoch 369/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5168 - accuracy: 0.3403\n",
            "Epoch 369: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5171 - accuracy: 0.3398 - val_loss: 1.5156 - val_accuracy: 0.3491\n",
            "Epoch 370/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5170 - accuracy: 0.3339\n",
            "Epoch 370: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5163 - accuracy: 0.3352 - val_loss: 1.5213 - val_accuracy: 0.3491\n",
            "Epoch 371/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5188 - accuracy: 0.3310\n",
            "Epoch 371: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5195 - accuracy: 0.3285 - val_loss: 1.5296 - val_accuracy: 0.2334\n",
            "Epoch 372/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5240 - accuracy: 0.3293\n",
            "Epoch 372: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5231 - accuracy: 0.3307 - val_loss: 1.5174 - val_accuracy: 0.3491\n",
            "Epoch 373/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5221 - accuracy: 0.3315\n",
            "Epoch 373: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5222 - accuracy: 0.3323 - val_loss: 1.5142 - val_accuracy: 0.3491\n",
            "Epoch 374/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5179 - accuracy: 0.3308\n",
            "Epoch 374: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5183 - accuracy: 0.3299 - val_loss: 1.5166 - val_accuracy: 0.3491\n",
            "Epoch 375/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5207 - accuracy: 0.3379\n",
            "Epoch 375: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5210 - accuracy: 0.3388 - val_loss: 1.5170 - val_accuracy: 0.3491\n",
            "Epoch 376/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5195 - accuracy: 0.3355\n",
            "Epoch 376: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5175 - accuracy: 0.3381 - val_loss: 1.5153 - val_accuracy: 0.3491\n",
            "Epoch 377/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5209 - accuracy: 0.3344\n",
            "Epoch 377: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5214 - accuracy: 0.3342 - val_loss: 1.5159 - val_accuracy: 0.3491\n",
            "Epoch 378/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5170 - accuracy: 0.3376\n",
            "Epoch 378: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5182 - accuracy: 0.3359 - val_loss: 1.5134 - val_accuracy: 0.3491\n",
            "Epoch 379/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5173 - accuracy: 0.3377\n",
            "Epoch 379: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5167 - accuracy: 0.3373 - val_loss: 1.5232 - val_accuracy: 0.3491\n",
            "Epoch 380/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5196 - accuracy: 0.3361\n",
            "Epoch 380: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5183 - accuracy: 0.3379 - val_loss: 1.5141 - val_accuracy: 0.3491\n",
            "Epoch 381/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5187 - accuracy: 0.3344\n",
            "Epoch 381: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5197 - accuracy: 0.3341 - val_loss: 1.5195 - val_accuracy: 0.3491\n",
            "Epoch 382/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5176 - accuracy: 0.3381\n",
            "Epoch 382: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5169 - accuracy: 0.3378 - val_loss: 1.5193 - val_accuracy: 0.3491\n",
            "Epoch 383/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5198 - accuracy: 0.3320\n",
            "Epoch 383: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5198 - accuracy: 0.3312 - val_loss: 1.5236 - val_accuracy: 0.3491\n",
            "Epoch 384/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5191 - accuracy: 0.3370\n",
            "Epoch 384: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3371 - val_loss: 1.5173 - val_accuracy: 0.3491\n",
            "Epoch 385/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5186 - accuracy: 0.3315\n",
            "Epoch 385: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5192 - accuracy: 0.3314 - val_loss: 1.5175 - val_accuracy: 0.3491\n",
            "Epoch 386/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5193 - accuracy: 0.3385\n",
            "Epoch 386: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5188 - accuracy: 0.3362 - val_loss: 1.5344 - val_accuracy: 0.2334\n",
            "Epoch 387/600\n",
            "68/76 [=========================>....] - ETA: 0s - loss: 1.5169 - accuracy: 0.3346\n",
            "Epoch 387: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5188 - accuracy: 0.3327 - val_loss: 1.5163 - val_accuracy: 0.3491\n",
            "Epoch 388/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5202 - accuracy: 0.3327\n",
            "Epoch 388: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5208 - accuracy: 0.3329 - val_loss: 1.5190 - val_accuracy: 0.3491\n",
            "Epoch 389/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5183 - accuracy: 0.3400\n",
            "Epoch 389: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3408 - val_loss: 1.5148 - val_accuracy: 0.3491\n",
            "Epoch 390/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.4991 - accuracy: 0.3457\n",
            "Epoch 390: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4969 - accuracy: 0.3480 - val_loss: 1.4944 - val_accuracy: 0.3604\n",
            "Epoch 391/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.4845 - accuracy: 0.3542\n",
            "Epoch 391: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4886 - accuracy: 0.3507 - val_loss: 1.5333 - val_accuracy: 0.3491\n",
            "Epoch 392/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5213 - accuracy: 0.3269\n",
            "Epoch 392: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5220 - accuracy: 0.3266 - val_loss: 1.5166 - val_accuracy: 0.3491\n",
            "Epoch 393/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5235 - accuracy: 0.3188\n",
            "Epoch 393: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5223 - accuracy: 0.3194 - val_loss: 1.5236 - val_accuracy: 0.3491\n",
            "Epoch 394/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5223 - accuracy: 0.3362\n",
            "Epoch 394: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5232 - accuracy: 0.3362 - val_loss: 1.5181 - val_accuracy: 0.3491\n",
            "Epoch 395/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5209 - accuracy: 0.3378\n",
            "Epoch 395: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5211 - accuracy: 0.3385 - val_loss: 1.5150 - val_accuracy: 0.3491\n",
            "Epoch 396/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5185 - accuracy: 0.3356\n",
            "Epoch 396: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5192 - accuracy: 0.3351 - val_loss: 1.5162 - val_accuracy: 0.3491\n",
            "Epoch 397/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5175 - accuracy: 0.3337\n",
            "Epoch 397: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5204 - accuracy: 0.3321 - val_loss: 1.5171 - val_accuracy: 0.3491\n",
            "Epoch 398/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5170 - accuracy: 0.3384\n",
            "Epoch 398: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5193 - accuracy: 0.3378 - val_loss: 1.5230 - val_accuracy: 0.3491\n",
            "Epoch 399/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5158 - accuracy: 0.3364\n",
            "Epoch 399: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5157 - accuracy: 0.3363 - val_loss: 1.5154 - val_accuracy: 0.3491\n",
            "Epoch 400/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5238 - accuracy: 0.3356\n",
            "Epoch 400: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5237 - accuracy: 0.3346 - val_loss: 1.5187 - val_accuracy: 0.3491\n",
            "Epoch 401/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5165 - accuracy: 0.3400\n",
            "Epoch 401: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5167 - accuracy: 0.3391 - val_loss: 1.5169 - val_accuracy: 0.3491\n",
            "Epoch 402/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5198 - accuracy: 0.3333\n",
            "Epoch 402: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5197 - accuracy: 0.3345 - val_loss: 1.5233 - val_accuracy: 0.3491\n",
            "Epoch 403/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5222 - accuracy: 0.3395\n",
            "Epoch 403: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5212 - accuracy: 0.3391 - val_loss: 1.5196 - val_accuracy: 0.3491\n",
            "Epoch 404/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5185 - accuracy: 0.3313\n",
            "Epoch 404: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5200 - accuracy: 0.3274 - val_loss: 1.5135 - val_accuracy: 0.3491\n",
            "Epoch 405/600\n",
            "68/76 [=========================>....] - ETA: 0s - loss: 1.5202 - accuracy: 0.3340\n",
            "Epoch 405: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 1.5203 - accuracy: 0.3340 - val_loss: 1.5178 - val_accuracy: 0.3491\n",
            "Epoch 406/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5197 - accuracy: 0.3383\n",
            "Epoch 406: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5213 - accuracy: 0.3354 - val_loss: 1.5313 - val_accuracy: 0.3491\n",
            "Epoch 407/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5177 - accuracy: 0.3392\n",
            "Epoch 407: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5201 - accuracy: 0.3375 - val_loss: 1.5151 - val_accuracy: 0.3491\n",
            "Epoch 408/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5135 - accuracy: 0.3428\n",
            "Epoch 408: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5138 - accuracy: 0.3424 - val_loss: 1.5142 - val_accuracy: 0.3491\n",
            "Epoch 409/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5216 - accuracy: 0.3335\n",
            "Epoch 409: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5221 - accuracy: 0.3287 - val_loss: 1.5248 - val_accuracy: 0.3491\n",
            "Epoch 410/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5202 - accuracy: 0.3326\n",
            "Epoch 410: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5207 - accuracy: 0.3319 - val_loss: 1.5207 - val_accuracy: 0.3491\n",
            "Epoch 411/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5249 - accuracy: 0.3234\n",
            "Epoch 411: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5235 - accuracy: 0.3262 - val_loss: 1.5193 - val_accuracy: 0.3491\n",
            "Epoch 412/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5172 - accuracy: 0.3329\n",
            "Epoch 412: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5187 - accuracy: 0.3315 - val_loss: 1.5213 - val_accuracy: 0.3491\n",
            "Epoch 413/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5185 - accuracy: 0.3414\n",
            "Epoch 413: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5190 - accuracy: 0.3420 - val_loss: 1.5216 - val_accuracy: 0.3491\n",
            "Epoch 414/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5257 - accuracy: 0.3293\n",
            "Epoch 414: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5240 - accuracy: 0.3307 - val_loss: 1.5187 - val_accuracy: 0.3491\n",
            "Epoch 415/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5165 - accuracy: 0.3363\n",
            "Epoch 415: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5159 - accuracy: 0.3371 - val_loss: 1.5152 - val_accuracy: 0.3491\n",
            "Epoch 416/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5092 - accuracy: 0.3409\n",
            "Epoch 416: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5111 - accuracy: 0.3365 - val_loss: 1.4807 - val_accuracy: 0.3535\n",
            "Epoch 417/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.4796 - accuracy: 0.3541\n",
            "Epoch 417: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4828 - accuracy: 0.3521 - val_loss: 1.5311 - val_accuracy: 0.3491\n",
            "Epoch 418/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5259 - accuracy: 0.3269\n",
            "Epoch 418: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5260 - accuracy: 0.3258 - val_loss: 1.5251 - val_accuracy: 0.3491\n",
            "Epoch 419/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5281 - accuracy: 0.3239\n",
            "Epoch 419: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5273 - accuracy: 0.3244 - val_loss: 1.5376 - val_accuracy: 0.3535\n",
            "Epoch 420/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5170 - accuracy: 0.3337\n",
            "Epoch 420: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5168 - accuracy: 0.3336 - val_loss: 1.5197 - val_accuracy: 0.3491\n",
            "Epoch 421/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5222 - accuracy: 0.3344\n",
            "Epoch 421: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5230 - accuracy: 0.3346 - val_loss: 1.5149 - val_accuracy: 0.3491\n",
            "Epoch 422/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5252 - accuracy: 0.3310\n",
            "Epoch 422: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5248 - accuracy: 0.3316 - val_loss: 1.5158 - val_accuracy: 0.3491\n",
            "Epoch 423/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5150 - accuracy: 0.3407\n",
            "Epoch 423: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5153 - accuracy: 0.3392 - val_loss: 1.5210 - val_accuracy: 0.3491\n",
            "Epoch 424/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5214 - accuracy: 0.3307\n",
            "Epoch 424: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5216 - accuracy: 0.3260 - val_loss: 1.5337 - val_accuracy: 0.2334\n",
            "Epoch 425/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5200 - accuracy: 0.3375\n",
            "Epoch 425: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5190 - accuracy: 0.3390 - val_loss: 1.5189 - val_accuracy: 0.3491\n",
            "Epoch 426/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5182 - accuracy: 0.3269\n",
            "Epoch 426: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5177 - accuracy: 0.3300 - val_loss: 1.5167 - val_accuracy: 0.3491\n",
            "Epoch 427/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5226 - accuracy: 0.3296\n",
            "Epoch 427: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5233 - accuracy: 0.3276 - val_loss: 1.5177 - val_accuracy: 0.3491\n",
            "Epoch 428/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5179 - accuracy: 0.3272\n",
            "Epoch 428: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5194 - accuracy: 0.3273 - val_loss: 1.5183 - val_accuracy: 0.3491\n",
            "Epoch 429/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5173 - accuracy: 0.3406\n",
            "Epoch 429: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5163 - accuracy: 0.3412 - val_loss: 1.5202 - val_accuracy: 0.3491\n",
            "Epoch 430/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5201 - accuracy: 0.3365\n",
            "Epoch 430: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5205 - accuracy: 0.3352 - val_loss: 1.5197 - val_accuracy: 0.3491\n",
            "Epoch 431/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5270 - accuracy: 0.3251\n",
            "Epoch 431: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5248 - accuracy: 0.3245 - val_loss: 1.5163 - val_accuracy: 0.3491\n",
            "Epoch 432/600\n",
            "68/76 [=========================>....] - ETA: 0s - loss: 1.5160 - accuracy: 0.3407\n",
            "Epoch 432: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5169 - accuracy: 0.3408 - val_loss: 1.5176 - val_accuracy: 0.3491\n",
            "Epoch 433/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5209 - accuracy: 0.3321\n",
            "Epoch 433: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5218 - accuracy: 0.3327 - val_loss: 1.5161 - val_accuracy: 0.3491\n",
            "Epoch 434/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5178 - accuracy: 0.3411\n",
            "Epoch 434: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5187 - accuracy: 0.3415 - val_loss: 1.5166 - val_accuracy: 0.3491\n",
            "Epoch 435/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5194 - accuracy: 0.3292\n",
            "Epoch 435: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5197 - accuracy: 0.3283 - val_loss: 1.5220 - val_accuracy: 0.3491\n",
            "Epoch 436/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5197 - accuracy: 0.3316\n",
            "Epoch 436: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5191 - accuracy: 0.3327 - val_loss: 1.5136 - val_accuracy: 0.3491\n",
            "Epoch 437/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5156 - accuracy: 0.3432\n",
            "Epoch 437: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5172 - accuracy: 0.3414 - val_loss: 1.5162 - val_accuracy: 0.3491\n",
            "Epoch 438/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5160 - accuracy: 0.3331\n",
            "Epoch 438: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5169 - accuracy: 0.3329 - val_loss: 1.5205 - val_accuracy: 0.3535\n",
            "Epoch 439/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5160 - accuracy: 0.3381\n",
            "Epoch 439: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5176 - accuracy: 0.3340 - val_loss: 1.5298 - val_accuracy: 0.2334\n",
            "Epoch 440/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5061 - accuracy: 0.3408\n",
            "Epoch 440: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5055 - accuracy: 0.3402 - val_loss: 1.4832 - val_accuracy: 0.4468\n",
            "Epoch 441/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.4720 - accuracy: 0.3601\n",
            "Epoch 441: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4703 - accuracy: 0.3601 - val_loss: 1.3819 - val_accuracy: 0.3901\n",
            "Epoch 442/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5179 - accuracy: 0.3349\n",
            "Epoch 442: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5168 - accuracy: 0.3346 - val_loss: 1.5276 - val_accuracy: 0.3491\n",
            "Epoch 443/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5210 - accuracy: 0.3383\n",
            "Epoch 443: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5218 - accuracy: 0.3372 - val_loss: 1.5209 - val_accuracy: 0.3491\n",
            "Epoch 444/600\n",
            "68/76 [=========================>....] - ETA: 0s - loss: 1.5211 - accuracy: 0.3302\n",
            "Epoch 444: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5231 - accuracy: 0.3275 - val_loss: 1.5398 - val_accuracy: 0.2334\n",
            "Epoch 445/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5179 - accuracy: 0.3355\n",
            "Epoch 445: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3346 - val_loss: 1.5227 - val_accuracy: 0.3491\n",
            "Epoch 446/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5194 - accuracy: 0.3328\n",
            "Epoch 446: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5187 - accuracy: 0.3348 - val_loss: 1.5150 - val_accuracy: 0.3491\n",
            "Epoch 447/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5164 - accuracy: 0.3401\n",
            "Epoch 447: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5164 - accuracy: 0.3398 - val_loss: 1.5170 - val_accuracy: 0.3491\n",
            "Epoch 448/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5241 - accuracy: 0.3214\n",
            "Epoch 448: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5242 - accuracy: 0.3204 - val_loss: 1.5203 - val_accuracy: 0.3491\n",
            "Epoch 449/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5210 - accuracy: 0.3257\n",
            "Epoch 449: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5216 - accuracy: 0.3262 - val_loss: 1.5178 - val_accuracy: 0.3491\n",
            "Epoch 450/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5213 - accuracy: 0.3373\n",
            "Epoch 450: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5212 - accuracy: 0.3377 - val_loss: 1.5142 - val_accuracy: 0.3491\n",
            "Epoch 451/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5167 - accuracy: 0.3364\n",
            "Epoch 451: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5155 - accuracy: 0.3390 - val_loss: 1.5204 - val_accuracy: 0.3491\n",
            "Epoch 452/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5200 - accuracy: 0.3339\n",
            "Epoch 452: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5203 - accuracy: 0.3340 - val_loss: 1.5245 - val_accuracy: 0.3491\n",
            "Epoch 453/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5230 - accuracy: 0.3338\n",
            "Epoch 453: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5215 - accuracy: 0.3343 - val_loss: 1.5229 - val_accuracy: 0.3491\n",
            "Epoch 454/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5208 - accuracy: 0.3373\n",
            "Epoch 454: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5208 - accuracy: 0.3370 - val_loss: 1.5152 - val_accuracy: 0.3491\n",
            "Epoch 455/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5168 - accuracy: 0.3374\n",
            "Epoch 455: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5173 - accuracy: 0.3383 - val_loss: 1.5159 - val_accuracy: 0.3491\n",
            "Epoch 456/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5209 - accuracy: 0.3275\n",
            "Epoch 456: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5231 - accuracy: 0.3264 - val_loss: 1.5165 - val_accuracy: 0.3491\n",
            "Epoch 457/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5199 - accuracy: 0.3290\n",
            "Epoch 457: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3299 - val_loss: 1.5164 - val_accuracy: 0.3491\n",
            "Epoch 458/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5214 - accuracy: 0.3297\n",
            "Epoch 458: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5205 - accuracy: 0.3316 - val_loss: 1.5167 - val_accuracy: 0.3491\n",
            "Epoch 459/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5200 - accuracy: 0.3360\n",
            "Epoch 459: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5201 - accuracy: 0.3367 - val_loss: 1.5145 - val_accuracy: 0.3491\n",
            "Epoch 460/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5196 - accuracy: 0.3320\n",
            "Epoch 460: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5197 - accuracy: 0.3320 - val_loss: 1.5171 - val_accuracy: 0.3491\n",
            "Epoch 461/600\n",
            "68/76 [=========================>....] - ETA: 0s - loss: 1.5191 - accuracy: 0.3352\n",
            "Epoch 461: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5196 - accuracy: 0.3359 - val_loss: 1.5175 - val_accuracy: 0.3491\n",
            "Epoch 462/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5190 - accuracy: 0.3340\n",
            "Epoch 462: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5193 - accuracy: 0.3341 - val_loss: 1.5166 - val_accuracy: 0.3491\n",
            "Epoch 463/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5161 - accuracy: 0.3384\n",
            "Epoch 463: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5173 - accuracy: 0.3375 - val_loss: 1.5221 - val_accuracy: 0.3491\n",
            "Epoch 464/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5196 - accuracy: 0.3365\n",
            "Epoch 464: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5199 - accuracy: 0.3362 - val_loss: 1.5144 - val_accuracy: 0.3491\n",
            "Epoch 465/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5195 - accuracy: 0.3363\n",
            "Epoch 465: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5195 - accuracy: 0.3350 - val_loss: 1.5173 - val_accuracy: 0.3491\n",
            "Epoch 466/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5197 - accuracy: 0.3364\n",
            "Epoch 466: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5199 - accuracy: 0.3358 - val_loss: 1.5195 - val_accuracy: 0.3491\n",
            "Epoch 467/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5195 - accuracy: 0.3382\n",
            "Epoch 467: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5203 - accuracy: 0.3378 - val_loss: 1.5153 - val_accuracy: 0.3491\n",
            "Epoch 468/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5189 - accuracy: 0.3265\n",
            "Epoch 468: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5196 - accuracy: 0.3271 - val_loss: 1.5148 - val_accuracy: 0.3491\n",
            "Epoch 469/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5144 - accuracy: 0.3415\n",
            "Epoch 469: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5155 - accuracy: 0.3378 - val_loss: 1.5228 - val_accuracy: 0.3491\n",
            "Epoch 470/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5213 - accuracy: 0.3343\n",
            "Epoch 470: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3341 - val_loss: 1.5228 - val_accuracy: 0.3491\n",
            "Epoch 471/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5202 - accuracy: 0.3309\n",
            "Epoch 471: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5203 - accuracy: 0.3308 - val_loss: 1.5154 - val_accuracy: 0.3491\n",
            "Epoch 472/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5215 - accuracy: 0.3378\n",
            "Epoch 472: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5205 - accuracy: 0.3374 - val_loss: 1.5259 - val_accuracy: 0.3491\n",
            "Epoch 473/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5186 - accuracy: 0.3352\n",
            "Epoch 473: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5173 - accuracy: 0.3364 - val_loss: 1.5193 - val_accuracy: 0.3491\n",
            "Epoch 474/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5211 - accuracy: 0.3350\n",
            "Epoch 474: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5193 - accuracy: 0.3380 - val_loss: 1.5195 - val_accuracy: 0.3491\n",
            "Epoch 475/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5291 - accuracy: 0.3289\n",
            "Epoch 475: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5270 - accuracy: 0.3314 - val_loss: 1.5154 - val_accuracy: 0.3491\n",
            "Epoch 476/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5136 - accuracy: 0.3404\n",
            "Epoch 476: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5167 - accuracy: 0.3398 - val_loss: 1.5168 - val_accuracy: 0.3491\n",
            "Epoch 477/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5213 - accuracy: 0.3394\n",
            "Epoch 477: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5201 - accuracy: 0.3379 - val_loss: 1.5219 - val_accuracy: 0.3491\n",
            "Epoch 478/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5202 - accuracy: 0.3256\n",
            "Epoch 478: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5197 - accuracy: 0.3276 - val_loss: 1.5138 - val_accuracy: 0.3491\n",
            "Epoch 479/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5205 - accuracy: 0.3297\n",
            "Epoch 479: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5204 - accuracy: 0.3310 - val_loss: 1.5172 - val_accuracy: 0.3491\n",
            "Epoch 480/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5188 - accuracy: 0.3374\n",
            "Epoch 480: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5169 - accuracy: 0.3375 - val_loss: 1.5227 - val_accuracy: 0.3491\n",
            "Epoch 481/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5201 - accuracy: 0.3299\n",
            "Epoch 481: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5186 - accuracy: 0.3306 - val_loss: 1.5188 - val_accuracy: 0.3491\n",
            "Epoch 482/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5237 - accuracy: 0.3351\n",
            "Epoch 482: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5223 - accuracy: 0.3354 - val_loss: 1.5184 - val_accuracy: 0.3491\n",
            "Epoch 483/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5153 - accuracy: 0.3385\n",
            "Epoch 483: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5159 - accuracy: 0.3377 - val_loss: 1.5146 - val_accuracy: 0.3491\n",
            "Epoch 484/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5223 - accuracy: 0.3330\n",
            "Epoch 484: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5206 - accuracy: 0.3350 - val_loss: 1.5183 - val_accuracy: 0.3491\n",
            "Epoch 485/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5222 - accuracy: 0.3351\n",
            "Epoch 485: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5225 - accuracy: 0.3349 - val_loss: 1.5150 - val_accuracy: 0.3491\n",
            "Epoch 486/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5164 - accuracy: 0.3387\n",
            "Epoch 486: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5172 - accuracy: 0.3385 - val_loss: 1.5148 - val_accuracy: 0.3491\n",
            "Epoch 487/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5197 - accuracy: 0.3314\n",
            "Epoch 487: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5194 - accuracy: 0.3314 - val_loss: 1.5188 - val_accuracy: 0.3491\n",
            "Epoch 488/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5224 - accuracy: 0.3273\n",
            "Epoch 488: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5214 - accuracy: 0.3294 - val_loss: 1.5158 - val_accuracy: 0.3491\n",
            "Epoch 489/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5191 - accuracy: 0.3312\n",
            "Epoch 489: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5204 - accuracy: 0.3306 - val_loss: 1.5199 - val_accuracy: 0.3491\n",
            "Epoch 490/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5180 - accuracy: 0.3339\n",
            "Epoch 490: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3334 - val_loss: 1.5205 - val_accuracy: 0.3491\n",
            "Epoch 491/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5194 - accuracy: 0.3316\n",
            "Epoch 491: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3323 - val_loss: 1.5236 - val_accuracy: 0.3491\n",
            "Epoch 492/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5207 - accuracy: 0.3359\n",
            "Epoch 492: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5217 - accuracy: 0.3352 - val_loss: 1.5208 - val_accuracy: 0.3491\n",
            "Epoch 493/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5194 - accuracy: 0.3341\n",
            "Epoch 493: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5173 - accuracy: 0.3365 - val_loss: 1.5193 - val_accuracy: 0.3491\n",
            "Epoch 494/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5250 - accuracy: 0.3301\n",
            "Epoch 494: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5242 - accuracy: 0.3309 - val_loss: 1.5138 - val_accuracy: 0.3491\n",
            "Epoch 495/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5172 - accuracy: 0.3386\n",
            "Epoch 495: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5182 - accuracy: 0.3378 - val_loss: 1.5212 - val_accuracy: 0.3491\n",
            "Epoch 496/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5174 - accuracy: 0.3375\n",
            "Epoch 496: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5193 - accuracy: 0.3369 - val_loss: 1.5150 - val_accuracy: 0.3491\n",
            "Epoch 497/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5211 - accuracy: 0.3365\n",
            "Epoch 497: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5213 - accuracy: 0.3374 - val_loss: 1.5178 - val_accuracy: 0.3491\n",
            "Epoch 498/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5175 - accuracy: 0.3319\n",
            "Epoch 498: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5188 - accuracy: 0.3308 - val_loss: 1.5194 - val_accuracy: 0.3491\n",
            "Epoch 499/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5192 - accuracy: 0.3325\n",
            "Epoch 499: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5217 - accuracy: 0.3310 - val_loss: 1.5145 - val_accuracy: 0.3491\n",
            "Epoch 500/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5209 - accuracy: 0.3286\n",
            "Epoch 500: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5196 - accuracy: 0.3290 - val_loss: 1.5175 - val_accuracy: 0.3491\n",
            "Epoch 501/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5209 - accuracy: 0.3324\n",
            "Epoch 501: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5183 - accuracy: 0.3353 - val_loss: 1.5149 - val_accuracy: 0.3491\n",
            "Epoch 502/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5181 - accuracy: 0.3353\n",
            "Epoch 502: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5200 - accuracy: 0.3334 - val_loss: 1.5245 - val_accuracy: 0.3491\n",
            "Epoch 503/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5186 - accuracy: 0.3377\n",
            "Epoch 503: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5181 - accuracy: 0.3366 - val_loss: 1.5155 - val_accuracy: 0.3491\n",
            "Epoch 504/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5195 - accuracy: 0.3378\n",
            "Epoch 504: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3380 - val_loss: 1.5145 - val_accuracy: 0.3491\n",
            "Epoch 505/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5196 - accuracy: 0.3336\n",
            "Epoch 505: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5208 - accuracy: 0.3321 - val_loss: 1.5171 - val_accuracy: 0.3491\n",
            "Epoch 506/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5212 - accuracy: 0.3374\n",
            "Epoch 506: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5210 - accuracy: 0.3368 - val_loss: 1.5258 - val_accuracy: 0.3491\n",
            "Epoch 507/600\n",
            "67/76 [=========================>....] - ETA: 0s - loss: 1.5174 - accuracy: 0.3352\n",
            "Epoch 507: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 1.5184 - accuracy: 0.3332 - val_loss: 1.5165 - val_accuracy: 0.3491\n",
            "Epoch 508/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5193 - accuracy: 0.3343\n",
            "Epoch 508: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5194 - accuracy: 0.3337 - val_loss: 1.5189 - val_accuracy: 0.3491\n",
            "Epoch 509/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5136 - accuracy: 0.3437\n",
            "Epoch 509: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5136 - accuracy: 0.3442 - val_loss: 1.5147 - val_accuracy: 0.3491\n",
            "Epoch 510/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5216 - accuracy: 0.3292\n",
            "Epoch 510: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5198 - accuracy: 0.3310 - val_loss: 1.5197 - val_accuracy: 0.3491\n",
            "Epoch 511/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5225 - accuracy: 0.3252\n",
            "Epoch 511: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5222 - accuracy: 0.3260 - val_loss: 1.5161 - val_accuracy: 0.3491\n",
            "Epoch 512/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5170 - accuracy: 0.3346\n",
            "Epoch 512: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5188 - accuracy: 0.3335 - val_loss: 1.5157 - val_accuracy: 0.3491\n",
            "Epoch 513/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5171 - accuracy: 0.3358\n",
            "Epoch 513: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5154 - accuracy: 0.3383 - val_loss: 1.5153 - val_accuracy: 0.3491\n",
            "Epoch 514/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5239 - accuracy: 0.3268\n",
            "Epoch 514: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5228 - accuracy: 0.3283 - val_loss: 1.5160 - val_accuracy: 0.3491\n",
            "Epoch 515/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5155 - accuracy: 0.3407\n",
            "Epoch 515: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5173 - accuracy: 0.3397 - val_loss: 1.5149 - val_accuracy: 0.3491\n",
            "Epoch 516/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5233 - accuracy: 0.3327\n",
            "Epoch 516: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5238 - accuracy: 0.3326 - val_loss: 1.5237 - val_accuracy: 0.3491\n",
            "Epoch 517/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5165 - accuracy: 0.3372\n",
            "Epoch 517: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5172 - accuracy: 0.3376 - val_loss: 1.5188 - val_accuracy: 0.3491\n",
            "Epoch 518/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5163 - accuracy: 0.3407\n",
            "Epoch 518: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5157 - accuracy: 0.3403 - val_loss: 1.5192 - val_accuracy: 0.3491\n",
            "Epoch 519/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5218 - accuracy: 0.3309\n",
            "Epoch 519: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5219 - accuracy: 0.3316 - val_loss: 1.5173 - val_accuracy: 0.3491\n",
            "Epoch 520/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5169 - accuracy: 0.3403\n",
            "Epoch 520: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5163 - accuracy: 0.3405 - val_loss: 1.5200 - val_accuracy: 0.3491\n",
            "Epoch 521/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5201 - accuracy: 0.3330\n",
            "Epoch 521: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5208 - accuracy: 0.3315 - val_loss: 1.5147 - val_accuracy: 0.3491\n",
            "Epoch 522/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5179 - accuracy: 0.3414\n",
            "Epoch 522: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5184 - accuracy: 0.3361 - val_loss: 1.5460 - val_accuracy: 0.2334\n",
            "Epoch 523/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5221 - accuracy: 0.3321\n",
            "Epoch 523: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5217 - accuracy: 0.3324 - val_loss: 1.5172 - val_accuracy: 0.3491\n",
            "Epoch 524/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5190 - accuracy: 0.3358\n",
            "Epoch 524: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5197 - accuracy: 0.3352 - val_loss: 1.5177 - val_accuracy: 0.3491\n",
            "Epoch 525/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5203 - accuracy: 0.3358\n",
            "Epoch 525: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5177 - accuracy: 0.3381 - val_loss: 1.5141 - val_accuracy: 0.3491\n",
            "Epoch 526/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5176 - accuracy: 0.3367\n",
            "Epoch 526: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5179 - accuracy: 0.3361 - val_loss: 1.5203 - val_accuracy: 0.3491\n",
            "Epoch 527/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5213 - accuracy: 0.3363\n",
            "Epoch 527: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5219 - accuracy: 0.3354 - val_loss: 1.5139 - val_accuracy: 0.3491\n",
            "Epoch 528/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5133 - accuracy: 0.3392\n",
            "Epoch 528: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5143 - accuracy: 0.3375 - val_loss: 1.5184 - val_accuracy: 0.3491\n",
            "Epoch 529/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5213 - accuracy: 0.3290\n",
            "Epoch 529: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5198 - accuracy: 0.3298 - val_loss: 1.5168 - val_accuracy: 0.3491\n",
            "Epoch 530/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5219 - accuracy: 0.3352\n",
            "Epoch 530: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5226 - accuracy: 0.3352 - val_loss: 1.5133 - val_accuracy: 0.3491\n",
            "Epoch 531/600\n",
            "68/76 [=========================>....] - ETA: 0s - loss: 1.5170 - accuracy: 0.3361\n",
            "Epoch 531: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5191 - accuracy: 0.3348 - val_loss: 1.5154 - val_accuracy: 0.3491\n",
            "Epoch 532/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5180 - accuracy: 0.3368\n",
            "Epoch 532: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5174 - accuracy: 0.3370 - val_loss: 1.5222 - val_accuracy: 0.3491\n",
            "Epoch 533/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5189 - accuracy: 0.3366\n",
            "Epoch 533: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5168 - accuracy: 0.3372 - val_loss: 1.5231 - val_accuracy: 0.3491\n",
            "Epoch 534/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5201 - accuracy: 0.3372\n",
            "Epoch 534: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5202 - accuracy: 0.3365 - val_loss: 1.5149 - val_accuracy: 0.3491\n",
            "Epoch 535/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5175 - accuracy: 0.3336\n",
            "Epoch 535: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5190 - accuracy: 0.3320 - val_loss: 1.5155 - val_accuracy: 0.3491\n",
            "Epoch 536/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5161 - accuracy: 0.3389\n",
            "Epoch 536: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5160 - accuracy: 0.3389 - val_loss: 1.5166 - val_accuracy: 0.3491\n",
            "Epoch 537/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5190 - accuracy: 0.3357\n",
            "Epoch 537: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5197 - accuracy: 0.3351 - val_loss: 1.5180 - val_accuracy: 0.3491\n",
            "Epoch 538/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5169 - accuracy: 0.3374\n",
            "Epoch 538: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5181 - accuracy: 0.3358 - val_loss: 1.5150 - val_accuracy: 0.3491\n",
            "Epoch 539/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5186 - accuracy: 0.3305\n",
            "Epoch 539: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5190 - accuracy: 0.3274 - val_loss: 1.5194 - val_accuracy: 0.3491\n",
            "Epoch 540/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5208 - accuracy: 0.3342\n",
            "Epoch 540: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5192 - accuracy: 0.3361 - val_loss: 1.5147 - val_accuracy: 0.3491\n",
            "Epoch 541/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5202 - accuracy: 0.3299\n",
            "Epoch 541: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5201 - accuracy: 0.3317 - val_loss: 1.5145 - val_accuracy: 0.3491\n",
            "Epoch 542/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5178 - accuracy: 0.3364\n",
            "Epoch 542: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5174 - accuracy: 0.3386 - val_loss: 1.5241 - val_accuracy: 0.3491\n",
            "Epoch 543/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5217 - accuracy: 0.3260\n",
            "Epoch 543: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5219 - accuracy: 0.3265 - val_loss: 1.5162 - val_accuracy: 0.3491\n",
            "Epoch 544/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5185 - accuracy: 0.3272\n",
            "Epoch 544: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5182 - accuracy: 0.3289 - val_loss: 1.5159 - val_accuracy: 0.3491\n",
            "Epoch 545/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5155 - accuracy: 0.3393\n",
            "Epoch 545: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5164 - accuracy: 0.3388 - val_loss: 1.5133 - val_accuracy: 0.3491\n",
            "Epoch 546/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5178 - accuracy: 0.3281\n",
            "Epoch 546: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3284 - val_loss: 1.5193 - val_accuracy: 0.3491\n",
            "Epoch 547/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5194 - accuracy: 0.3366\n",
            "Epoch 547: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5199 - accuracy: 0.3354 - val_loss: 1.5253 - val_accuracy: 0.3491\n",
            "Epoch 548/600\n",
            "73/76 [===========================>..] - ETA: 0s - loss: 1.5181 - accuracy: 0.3381\n",
            "Epoch 548: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5185 - accuracy: 0.3376 - val_loss: 1.5184 - val_accuracy: 0.3491\n",
            "Epoch 549/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5194 - accuracy: 0.3362\n",
            "Epoch 549: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5181 - accuracy: 0.3373 - val_loss: 1.5158 - val_accuracy: 0.3491\n",
            "Epoch 550/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5189 - accuracy: 0.3378\n",
            "Epoch 550: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5202 - accuracy: 0.3326 - val_loss: 1.5235 - val_accuracy: 0.3491\n",
            "Epoch 551/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5212 - accuracy: 0.3301\n",
            "Epoch 551: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5194 - accuracy: 0.3324 - val_loss: 1.5145 - val_accuracy: 0.3491\n",
            "Epoch 552/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5161 - accuracy: 0.3357\n",
            "Epoch 552: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5165 - accuracy: 0.3364 - val_loss: 1.5155 - val_accuracy: 0.3491\n",
            "Epoch 553/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5182 - accuracy: 0.3386\n",
            "Epoch 553: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5183 - accuracy: 0.3388 - val_loss: 1.5212 - val_accuracy: 0.3491\n",
            "Epoch 554/600\n",
            "67/76 [=========================>....] - ETA: 0s - loss: 1.5204 - accuracy: 0.3317\n",
            "Epoch 554: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 1.5185 - accuracy: 0.3336 - val_loss: 1.5132 - val_accuracy: 0.3491\n",
            "Epoch 555/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5218 - accuracy: 0.3296\n",
            "Epoch 555: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5214 - accuracy: 0.3303 - val_loss: 1.5194 - val_accuracy: 0.3491\n",
            "Epoch 556/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5180 - accuracy: 0.3338\n",
            "Epoch 556: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5166 - accuracy: 0.3355 - val_loss: 1.5109 - val_accuracy: 0.3491\n",
            "Epoch 557/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5030 - accuracy: 0.3401\n",
            "Epoch 557: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5041 - accuracy: 0.3395 - val_loss: 1.4908 - val_accuracy: 0.3535\n",
            "Epoch 558/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.4889 - accuracy: 0.3506\n",
            "Epoch 558: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.4842 - accuracy: 0.3528 - val_loss: 1.4188 - val_accuracy: 0.3491\n",
            "Epoch 559/600\n",
            "68/76 [=========================>....] - ETA: 0s - loss: 1.5019 - accuracy: 0.3323\n",
            "Epoch 559: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5037 - accuracy: 0.3344 - val_loss: 1.5228 - val_accuracy: 0.3491\n",
            "Epoch 560/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5207 - accuracy: 0.3326\n",
            "Epoch 560: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5209 - accuracy: 0.3330 - val_loss: 1.5142 - val_accuracy: 0.3491\n",
            "Epoch 561/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5171 - accuracy: 0.3386\n",
            "Epoch 561: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5172 - accuracy: 0.3375 - val_loss: 1.5204 - val_accuracy: 0.3491\n",
            "Epoch 562/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5152 - accuracy: 0.3396\n",
            "Epoch 562: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5165 - accuracy: 0.3339 - val_loss: 1.5179 - val_accuracy: 0.3491\n",
            "Epoch 563/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5243 - accuracy: 0.3316\n",
            "Epoch 563: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5232 - accuracy: 0.3337 - val_loss: 1.5272 - val_accuracy: 0.3491\n",
            "Epoch 564/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5201 - accuracy: 0.3285\n",
            "Epoch 564: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5197 - accuracy: 0.3294 - val_loss: 1.5139 - val_accuracy: 0.3491\n",
            "Epoch 565/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5152 - accuracy: 0.3426\n",
            "Epoch 565: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5152 - accuracy: 0.3416 - val_loss: 1.5205 - val_accuracy: 0.3491\n",
            "Epoch 566/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5211 - accuracy: 0.3300\n",
            "Epoch 566: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5198 - accuracy: 0.3315 - val_loss: 1.5215 - val_accuracy: 0.3491\n",
            "Epoch 567/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5200 - accuracy: 0.3377\n",
            "Epoch 567: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5180 - accuracy: 0.3398 - val_loss: 1.5200 - val_accuracy: 0.3491\n",
            "Epoch 568/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5247 - accuracy: 0.3305\n",
            "Epoch 568: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5245 - accuracy: 0.3319 - val_loss: 1.5143 - val_accuracy: 0.3491\n",
            "Epoch 569/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5147 - accuracy: 0.3413\n",
            "Epoch 569: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5170 - accuracy: 0.3384 - val_loss: 1.5168 - val_accuracy: 0.3491\n",
            "Epoch 570/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5176 - accuracy: 0.3342\n",
            "Epoch 570: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5167 - accuracy: 0.3355 - val_loss: 1.5189 - val_accuracy: 0.3491\n",
            "Epoch 571/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5206 - accuracy: 0.3365\n",
            "Epoch 571: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5200 - accuracy: 0.3377 - val_loss: 1.5163 - val_accuracy: 0.3491\n",
            "Epoch 572/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5208 - accuracy: 0.3375\n",
            "Epoch 572: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5220 - accuracy: 0.3370 - val_loss: 1.5238 - val_accuracy: 0.3491\n",
            "Epoch 573/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5219 - accuracy: 0.3299\n",
            "Epoch 573: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5209 - accuracy: 0.3304 - val_loss: 1.5139 - val_accuracy: 0.3491\n",
            "Epoch 574/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5166 - accuracy: 0.3334\n",
            "Epoch 574: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5160 - accuracy: 0.3340 - val_loss: 1.5173 - val_accuracy: 0.3491\n",
            "Epoch 575/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5264 - accuracy: 0.3197\n",
            "Epoch 575: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5266 - accuracy: 0.3201 - val_loss: 1.5312 - val_accuracy: 0.3491\n",
            "Epoch 576/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5197 - accuracy: 0.3379\n",
            "Epoch 576: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5194 - accuracy: 0.3383 - val_loss: 1.5138 - val_accuracy: 0.3491\n",
            "Epoch 577/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5144 - accuracy: 0.3383\n",
            "Epoch 577: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5138 - accuracy: 0.3404 - val_loss: 1.5184 - val_accuracy: 0.3491\n",
            "Epoch 578/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5221 - accuracy: 0.3302\n",
            "Epoch 578: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5217 - accuracy: 0.3313 - val_loss: 1.5159 - val_accuracy: 0.3491\n",
            "Epoch 579/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5197 - accuracy: 0.3349\n",
            "Epoch 579: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5186 - accuracy: 0.3371 - val_loss: 1.5142 - val_accuracy: 0.3491\n",
            "Epoch 580/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5190 - accuracy: 0.3325\n",
            "Epoch 580: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5184 - accuracy: 0.3315 - val_loss: 1.5248 - val_accuracy: 0.3491\n",
            "Epoch 581/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5213 - accuracy: 0.3281\n",
            "Epoch 581: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5180 - accuracy: 0.3310 - val_loss: 1.5171 - val_accuracy: 0.3491\n",
            "Epoch 582/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5176 - accuracy: 0.3410\n",
            "Epoch 582: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5168 - accuracy: 0.3416 - val_loss: 1.5152 - val_accuracy: 0.3491\n",
            "Epoch 583/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5179 - accuracy: 0.3317\n",
            "Epoch 583: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5201 - accuracy: 0.3308 - val_loss: 1.5241 - val_accuracy: 0.3491\n",
            "Epoch 584/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5188 - accuracy: 0.3319\n",
            "Epoch 584: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5204 - accuracy: 0.3289 - val_loss: 1.5192 - val_accuracy: 0.3491\n",
            "Epoch 585/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5168 - accuracy: 0.3381\n",
            "Epoch 585: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5178 - accuracy: 0.3372 - val_loss: 1.5160 - val_accuracy: 0.3491\n",
            "Epoch 586/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5139 - accuracy: 0.3410\n",
            "Epoch 586: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5146 - accuracy: 0.3400 - val_loss: 1.5203 - val_accuracy: 0.3491\n",
            "Epoch 587/600\n",
            "67/76 [=========================>....] - ETA: 0s - loss: 1.5207 - accuracy: 0.3308\n",
            "Epoch 587: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5225 - accuracy: 0.3277 - val_loss: 1.5305 - val_accuracy: 0.2334\n",
            "Epoch 588/600\n",
            "68/76 [=========================>....] - ETA: 0s - loss: 1.5218 - accuracy: 0.3272\n",
            "Epoch 588: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5220 - accuracy: 0.3272 - val_loss: 1.5145 - val_accuracy: 0.3491\n",
            "Epoch 589/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5126 - accuracy: 0.3451\n",
            "Epoch 589: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5128 - accuracy: 0.3439 - val_loss: 1.5160 - val_accuracy: 0.3491\n",
            "Epoch 590/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5194 - accuracy: 0.3346\n",
            "Epoch 590: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5206 - accuracy: 0.3340 - val_loss: 1.5175 - val_accuracy: 0.3491\n",
            "Epoch 591/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5170 - accuracy: 0.3407\n",
            "Epoch 591: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5190 - accuracy: 0.3382 - val_loss: 1.5174 - val_accuracy: 0.3491\n",
            "Epoch 592/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5150 - accuracy: 0.3403\n",
            "Epoch 592: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5162 - accuracy: 0.3396 - val_loss: 1.5135 - val_accuracy: 0.3491\n",
            "Epoch 593/600\n",
            "70/76 [==========================>...] - ETA: 0s - loss: 1.5186 - accuracy: 0.3306\n",
            "Epoch 593: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5178 - accuracy: 0.3306 - val_loss: 1.5195 - val_accuracy: 0.3491\n",
            "Epoch 594/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5265 - accuracy: 0.3344\n",
            "Epoch 594: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5256 - accuracy: 0.3362 - val_loss: 1.5153 - val_accuracy: 0.3491\n",
            "Epoch 595/600\n",
            "74/76 [============================>.] - ETA: 0s - loss: 1.5132 - accuracy: 0.3390\n",
            "Epoch 595: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5128 - accuracy: 0.3394 - val_loss: 1.5144 - val_accuracy: 0.3491\n",
            "Epoch 596/600\n",
            "69/76 [==========================>...] - ETA: 0s - loss: 1.5198 - accuracy: 0.3334\n",
            "Epoch 596: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 1.5201 - accuracy: 0.3336 - val_loss: 1.5211 - val_accuracy: 0.3491\n",
            "Epoch 597/600\n",
            "71/76 [===========================>..] - ETA: 0s - loss: 1.5202 - accuracy: 0.3306\n",
            "Epoch 597: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5189 - accuracy: 0.3316 - val_loss: 1.5188 - val_accuracy: 0.3491\n",
            "Epoch 598/600\n",
            "67/76 [=========================>....] - ETA: 0s - loss: 1.5228 - accuracy: 0.3378\n",
            "Epoch 598: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 6ms/step - loss: 1.5212 - accuracy: 0.3378 - val_loss: 1.5230 - val_accuracy: 0.3491\n",
            "Epoch 599/600\n",
            "72/76 [===========================>..] - ETA: 0s - loss: 1.5171 - accuracy: 0.3365\n",
            "Epoch 599: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5171 - accuracy: 0.3361 - val_loss: 1.5169 - val_accuracy: 0.3491\n",
            "Epoch 600/600\n",
            "68/76 [=========================>....] - ETA: 0s - loss: 1.5202 - accuracy: 0.3293\n",
            "Epoch 600: val_loss did not improve from 1.27960\n",
            "76/76 [==============================] - 0s 5ms/step - loss: 1.5191 - accuracy: 0.3310 - val_loss: 1.5155 - val_accuracy: 0.3491\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(trainX,\n",
        "                    trainY, \n",
        "                    steps_per_epoch=n_steps_per_epoch,\n",
        "                    epochs=600,\n",
        "                    batch_size=n_batch_size,\n",
        "                    validation_data=(valX, valY),\n",
        "                    validation_steps=n_validation_steps,\n",
        "                    callbacks = callbacks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "ahOlnxVVshQ6",
        "outputId": "e659855f-c878-4ecd-e129-591caaf7ec66"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x432 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "epoch = 600\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "epochs_range = range(epoch)\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "plt.plot(epochs_range, acc[:epoch], label=\"accuracy\")\n",
        "plt.plot(epochs_range, val_acc[:epoch], label=\"validataion accuracy\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "plt.plot(epochs_range, loss[:epoch], label=\"train loss\")\n",
        "plt.plot(epochs_range, val_loss[:epoch], label=\"validataion loss\")\n",
        "plt.title(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "\n",
        "fig.tight_layout()\n",
        "plt.show()\n",
        "plt.savefig(graph)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "-YXWTl8QG3Vr"
      },
      "outputs": [],
      "source": [
        "#model.load_weights(\"/content/drive/MyDrive/NewCsv/firstModel/modeltl1.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "kX5BVDq3HPIV"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(testX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpjy_qRYNJAW",
        "outputId": "83e88503-9a27-4ede-d1b1-4f61e6bd74a2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([4, 4, 4, ..., 4, 4, 4])"
            ]
          },
          "execution_count": 209,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "preds_classes = np.argmax(predictions, axis=-1)\n",
        "\n",
        "preds_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T35OpR-qHkGK",
        "outputId": "bd7aa2e9-2d73-4d2a-829a-463b2dfb5e01"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0, 0],\n",
              "       [1, 0, 0, 0, 0],\n",
              "       [0, 0, 1, 0, 0],\n",
              "       ...,\n",
              "       [0, 0, 0, 0, 1],\n",
              "       [0, 0, 0, 0, 1],\n",
              "       [0, 1, 0, 0, 0]], dtype=uint8)"
            ]
          },
          "execution_count": 210,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "expected = testY\n",
        "expected = expected.to_numpy()\n",
        "expected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "a_mNLN-wdMJ0"
      },
      "outputs": [],
      "source": [
        "expected = np.argmax(expected, axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "d0tMV5CDN7GP"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score,recall_score,accuracy_score\n",
        "from sklearn.metrics import multilabel_confusion_matrix, classification_report, confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvk0JNKIuXSe",
        "outputId": "483bf222-f9fb-4cdb-8a89-01b1c928f18d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "[[[1873    0]\n",
            "  [ 232    0]]\n",
            "\n",
            " [[1789    0]\n",
            "  [ 309    7]]\n",
            "\n",
            " [[1781    0]\n",
            "  [ 324    0]]\n",
            "\n",
            " [[1620    0]\n",
            "  [ 485    0]]\n",
            "\n",
            " [[   7 1350]\n",
            "  [   0  748]]]\n",
            "Classification Report\n"
          ]
        }
      ],
      "source": [
        "print('Confusion Matrix')\n",
        "print(multilabel_confusion_matrix(expected, preds_classes))\n",
        "print('Classification Report')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8qPor7p72bt",
        "outputId": "ab02fc07-b653-4a95-f5b4-bd7441cda234"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 35.87%\n"
          ]
        }
      ],
      "source": [
        "accuracy = accuracy_score(expected ,  preds_classes)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovzFpn5sNRHb",
        "outputId": "5ba774e0-ab31-453c-b9b8-e1f19b234741"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F1 score: 0.193294\n"
          ]
        }
      ],
      "source": [
        "f1 = f1_score(expected, preds_classes,average='weighted')\n",
        "print('F1 score: %f' % f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VL03ZhgOTLJ",
        "outputId": "1c815442-d04e-46ae-c693-111506c99652"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision Score :  0.27680972037615287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "print(\"Precision Score : \",precision_score(expected, preds_classes,average='weighted'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFFh1QRqOUK4",
        "outputId": "1e79b345-9e37-4102-a44d-0e278eb67b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Recall Score :  0.3586698337292161\n"
          ]
        }
      ],
      "source": [
        "print(\"Recall Score : \",recall_score(expected, preds_classes,average='weighted'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y5JqbQQeAjs",
        "outputId": "da56930f-c82b-46ad-f1d3-4a9228ce0ecb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0, 232],\n",
              "       [  0,   7,   0,   0, 309],\n",
              "       [  0,   0,   0,   0, 324],\n",
              "       [  0,   0,   0,   0, 485],\n",
              "       [  0,   0,   0,   0, 748]])"
            ]
          },
          "execution_count": 218,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix as conm\n",
        "cf_matrix = conm(expected , preds_classes)\n",
        "cf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "iRQGC7voegKB",
        "outputId": "4c0f0913-bb45-4162-9646-ec842911e854"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc1a971d6d0>"
            ]
          },
          "execution_count": 219,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
        "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
        "                cf_matrix.flatten()]\n",
        "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
        "                     cf_matrix.flatten()/np.sum(cf_matrix)]\n",
        "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, fmt=\".2%\", cmap='Blues' )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "ANASTASIA Model",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
